{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DataCamp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIFguyPNvC5V"
      },
      "source": [
        "#pip install h5py"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kpXKYCXvLY0",
        "outputId": "3850a1bb-4216-4c15-a324-8cd369eee86e"
      },
      "source": [
        "import h5py\r\n",
        "import matplotlib.pyplot as plt  \r\n",
        "from google.colab import drive,files\r\n",
        "drive.mount('/content/drive')\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgFisEpovPMr"
      },
      "source": [
        "f = h5py.File('/content/drive/My Drive/DataCamp/X_train_new.h5', 'r')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDyXZUOyjyl0"
      },
      "source": [
        "y_train_src=pd.read_csv('/content/drive/My Drive/DataCamp/y_train_AvCsavx.csv',index_col=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGZDJNsmj0JD"
      },
      "source": [
        "ytt=y_train_src['label']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMh5I1REj39y",
        "outputId": "0db978e1-26a6-4b50-f50c-95714dd08413"
      },
      "source": [
        "ytt.value_counts()/946"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.77907\n",
              "1    0.22093\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzziB5xCwutZ",
        "outputId": "a6bd514d-865b-4531-befc-15a30e93a499"
      },
      "source": [
        "list(f.keys())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffPOffz7w56V"
      },
      "source": [
        "dset = f['features']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZzQYayTw_QO",
        "outputId": "5ee90ef8-7020-4d2f-fa07-4434503df4e4"
      },
      "source": [
        "dset.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(946, 40, 7, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6n6ZDViiv60",
        "outputId": "e348e947-635f-4826-daeb-3413ae037d12"
      },
      "source": [
        "import torch\r\n",
        "x = torch.tensor(dset[1][0][0], dtype=torch.float32)\r\n",
        "y = x.view(250,2)\r\n",
        "y.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([250, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDWynhF2mc_S"
      },
      "source": [
        "![131901954_382406862863168_552539021667655993_n.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "CyJpLXgti5-0",
        "outputId": "a5949be6-3072-4d17-f106-67003f2d11fe"
      },
      "source": [
        "plt.figure(figsize=(6,12))\r\n",
        "plt.imshow(y,origin='lower')\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8c62739dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "cnBXGeYVBmft",
        "outputId": "786ee0a1-7d5f-434f-a553-bb44915e9263"
      },
      "source": [
        "x1 = torch.tensor(dset[0][0][0], dtype=torch.float32)\r\n",
        "\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure(figsize=(6,6))\r\n",
        "plt.plot(x,color='r',label='female')\r\n",
        "plt.plot(x1,color='b',label='male')\r\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\r\n",
        "\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0BCaSmNxB0i",
        "outputId": "ea1f47dd-2e01-4cbb-f741-60d71556e43c"
      },
      "source": [
        "dset.dtype"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('<f4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PECvVSVnxqeI",
        "outputId": "646c5713-02ed-477e-b713-8278c9ee892f"
      },
      "source": [
        "dset[0].shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 7, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMCxMNgA04cn",
        "outputId": "004aec46-c293-4cb4-d4e4-5fb00b8fb3f3"
      },
      "source": [
        "dset[0][0][0].shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5VkM7Xviyyx"
      },
      "source": [
        "import torch\r\n",
        "#l=dset[1][0][0].tolist()\r\n",
        "#for i in range(1,40):\r\n",
        "#  l+=[dset[1][i][0].tolist()]\r\n",
        "#l = torch.tensor(l, dtype=torch.float32)\r\n",
        "\r\n",
        "x = torch.tensor(dset[1][0][0], dtype=torch.float32)\r\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFa5NYgPo3DI"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "d=torch.tensor(dset).to(device) #, dtype=torch.float32"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a21IebQipD67"
      },
      "source": [
        "dd = d.view(946,40*7,500)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RdEyQ7sd7u",
        "outputId": "6b2169bc-54a2-4e15-ba7d-c9bbd2cd933d"
      },
      "source": [
        "d.shape,dd.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([946, 40, 7, 500]), torch.Size([946, 280, 500]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbztZhcZu02w"
      },
      "source": [
        "x_train=dd\r\n",
        "y_train_src=pd.read_csv('/content/drive/My Drive/DataCamp/y_train_AvCsavx.csv',index_col=False)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAolNMMIvEWD"
      },
      "source": [
        "y_train=y_train_src['label']"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NlLlNZhvVFh"
      },
      "source": [
        "y_train=y_train.tolist()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaaTlZPUvg9J"
      },
      "source": [
        "y=[]\r\n",
        "for i in range(len(y_train)):\r\n",
        "    for j in range(40):\r\n",
        "        y+=[y_train[i]]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGb9gtEHv-Mn"
      },
      "source": [
        "yt=torch.tensor(y).to(device) #, dtype=torch.float32"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HvDFw1axHki"
      },
      "source": [
        "y_train=torch.tensor(y_train).to(device) #, dtype=torch.float32"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mFmF-pfwGin",
        "outputId": "c26c8a00-44d9-4d3a-f35e-72ec7b6e7615"
      },
      "source": [
        "yt.shape,y_train.shape,x_train.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([37840]), torch.Size([946]), torch.Size([946, 280, 500]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wNILCfl1lbT"
      },
      "source": [
        "x_train=x_train.view(946,40*7,250,2) #.float()\r\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUWiUOXtzckP",
        "outputId": "edb8308c-e159-4461-85c5-28c6d4b1ae7c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=0.8)\r\n",
        "print(x_train.shape)\r\n",
        "print(x_valid.shape)\r\n",
        "print(y_train.shape)\r\n",
        "print(y_valid.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([756, 280, 250, 2])\n",
            "torch.Size([190, 280, 250, 2])\n",
            "torch.Size([756])\n",
            "torch.Size([190])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFk_vCzq0vZl",
        "outputId": "726801e0-aad8-496a-97ff-4cd300f85cb7"
      },
      "source": [
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "train_ds = TensorDataset(x_train, y_train)\r\n",
        "print(\"number of training samples\", len(train_ds))\r\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) # we usually shuffle the training data.\r\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\r\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\r\n",
        "print(\"number of training batches\", len(train_dl))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples 756\n",
            "number of training batches 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0KNHa308LY"
      },
      "source": [
        "# Loss Function.\r\n",
        "import torch.nn.functional as F\r\n",
        "loss_func = F.cross_entropy # Remember: the function F.cross_entropy combines the softmax function and the negative log-likelihood."
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YSelKCE1EBX"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "class deer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv2d(280,100,(3,1))\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.pool = nn.MaxPool2d((3,1), stride=1)\r\n",
        "        self.conv2 = nn.Conv2d(100,100,(3,1))\r\n",
        "        self.conv3 = nn.Conv2d(100,300,(3,1))\r\n",
        "        #self.pool = nn.MaxPool2d(2, stride=1)\r\n",
        "        self.fc1 = nn.Linear(300*238*2, 2)\r\n",
        "        self.dp=nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "    def forward(self, xb):\r\n",
        "        xb=self.conv1(xb)\r\n",
        "        xb = self.relu(xb)\r\n",
        "        xb=self.pool(xb)\r\n",
        "        xb = self.dp(xb)\r\n",
        "        #xb=self.pool(xb)\r\n",
        "#        xb = xb.view(xb.size(0),-1).float()\r\n",
        "        xb=self.conv2(xb)\r\n",
        "        xb = self.relu(xb)\r\n",
        "        xb=self.pool(xb)\r\n",
        "        xb = self.dp(xb)\r\n",
        "        xb=self.conv3(xb)\r\n",
        "        xb = self.relu(xb)\r\n",
        "        xb=self.pool(xb)\r\n",
        "        xb = self.dp(xb)\r\n",
        "        xb = xb.view(xb.size(0),-1)\r\n",
        "        xb = self.fc1(xb)       \r\n",
        "        return xb\r\n",
        "\r\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYnRO_CEJyMS",
        "outputId": "2de2a49b-2e92-4632-c03e-d1ea11c9b272"
      },
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([756, 280, 250, 2]), torch.Size([756]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D_-Yg2018-n"
      },
      "source": [
        "from torch import optim\r\n",
        "def get_model(lr, device=device):\r\n",
        "    model = deer()\r\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\r\n",
        "    model = model.to(device) # transferring the model to the GPU if available. \r\n",
        "    return model, optimizer\r\n",
        "\r\n",
        "model, opt = get_model(lr=lr)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2cLM6Ku05wO"
      },
      "source": [
        "lr = 0.1\r\n",
        "epochs = 10000\r\n",
        "bs = 32"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyxEN1Kv2BVd",
        "outputId": "1e8b3fe2-239d-4a60-86f9-09244dfcf9bc"
      },
      "source": [
        "def fit(model, epochs, opt, loss_func, train_dl, valid_dl):\r\n",
        "  fit.train_losss=[]\r\n",
        "  fit.val_losss=[]\r\n",
        "  for epoch in range(epochs):\r\n",
        "      model.train() # useful for dropout. \r\n",
        "      train_loss = 0.\r\n",
        "      for xb, yb in train_dl:\r\n",
        "          # transfering data to device if GPU is available. \r\n",
        "        \r\n",
        "          xb = xb.to(device)\r\n",
        "          yb = yb.flatten().to(device)\r\n",
        "          pred = model(xb)\r\n",
        "          loss = loss_func(pred, yb)\r\n",
        "          train_loss += loss\r\n",
        "          loss.backward()\r\n",
        "          opt.step()\r\n",
        "          opt.zero_grad()\r\n",
        "      # pass forward on the validation dataset. \r\n",
        "      model.eval() # useful for dropout. \r\n",
        "      with torch.no_grad():\r\n",
        "          loss_valid = []\r\n",
        "          for xb, yb in valid_dl:\r\n",
        "              xb = xb.to(device)\r\n",
        "              yb = yb.flatten().to(device)\r\n",
        "              loss_valid.append(loss_func(model(xb), yb).cpu().item()) # .cpu() transfer back to the CPU. \r\n",
        "          valid_loss = sum(loss_valid) / len(loss_valid)\r\n",
        "          #print(type(xb))\r\n",
        "      print(\"Epoch: {}\".format(epoch), \"Train Loss: {:5.5f}\".format(train_loss.cpu().item()/len(train_dl)), \"Validation Loss: {:5.5f}\".format(valid_loss))\r\n",
        "      fit.train_losss+=[train_loss.cpu().item()/len(train_dl)]\r\n",
        "      fit.val_losss+=[valid_loss]\r\n",
        "fit(model, epochs, opt, loss_func, train_dl, valid_dl)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "Epoch: 5000 Train Loss: 0.52300 Validation Loss: 0.54876\n",
            "Epoch: 5001 Train Loss: 0.51985 Validation Loss: 0.54829\n",
            "Epoch: 5002 Train Loss: 0.52197 Validation Loss: 0.54832\n",
            "Epoch: 5003 Train Loss: 0.52821 Validation Loss: 0.55042\n",
            "Epoch: 5004 Train Loss: 0.52292 Validation Loss: 0.54785\n",
            "Epoch: 5005 Train Loss: 0.52265 Validation Loss: 0.54896\n",
            "Epoch: 5006 Train Loss: 0.52187 Validation Loss: 0.54882\n",
            "Epoch: 5007 Train Loss: 0.52314 Validation Loss: 0.54904\n",
            "Epoch: 5008 Train Loss: 0.52492 Validation Loss: 0.54919\n",
            "Epoch: 5009 Train Loss: 0.52375 Validation Loss: 0.54848\n",
            "Epoch: 5010 Train Loss: 0.52389 Validation Loss: 0.54807\n",
            "Epoch: 5011 Train Loss: 0.52197 Validation Loss: 0.54886\n",
            "Epoch: 5012 Train Loss: 0.52281 Validation Loss: 0.54847\n",
            "Epoch: 5013 Train Loss: 0.52082 Validation Loss: 0.54864\n",
            "Epoch: 5014 Train Loss: 0.52421 Validation Loss: 0.54907\n",
            "Epoch: 5015 Train Loss: 0.52176 Validation Loss: 0.54830\n",
            "Epoch: 5016 Train Loss: 0.52388 Validation Loss: 0.54850\n",
            "Epoch: 5017 Train Loss: 0.52497 Validation Loss: 0.54868\n",
            "Epoch: 5018 Train Loss: 0.52822 Validation Loss: 0.54854\n",
            "Epoch: 5019 Train Loss: 0.52107 Validation Loss: 0.54809\n",
            "Epoch: 5020 Train Loss: 0.52201 Validation Loss: 0.54904\n",
            "Epoch: 5021 Train Loss: 0.52393 Validation Loss: 0.54869\n",
            "Epoch: 5022 Train Loss: 0.52277 Validation Loss: 0.54820\n",
            "Epoch: 5023 Train Loss: 0.52532 Validation Loss: 0.54816\n",
            "Epoch: 5024 Train Loss: 0.52730 Validation Loss: 0.54968\n",
            "Epoch: 5025 Train Loss: 0.52132 Validation Loss: 0.54805\n",
            "Epoch: 5026 Train Loss: 0.52800 Validation Loss: 0.54923\n",
            "Epoch: 5027 Train Loss: 0.52392 Validation Loss: 0.54859\n",
            "Epoch: 5028 Train Loss: 0.52587 Validation Loss: 0.54866\n",
            "Epoch: 5029 Train Loss: 0.52579 Validation Loss: 0.54851\n",
            "Epoch: 5030 Train Loss: 0.52206 Validation Loss: 0.54804\n",
            "Epoch: 5031 Train Loss: 0.52201 Validation Loss: 0.54851\n",
            "Epoch: 5032 Train Loss: 0.52176 Validation Loss: 0.54939\n",
            "Epoch: 5033 Train Loss: 0.52502 Validation Loss: 0.54937\n",
            "Epoch: 5034 Train Loss: 0.52300 Validation Loss: 0.54813\n",
            "Epoch: 5035 Train Loss: 0.52466 Validation Loss: 0.54807\n",
            "Epoch: 5036 Train Loss: 0.52298 Validation Loss: 0.54870\n",
            "Epoch: 5037 Train Loss: 0.52192 Validation Loss: 0.54928\n",
            "Epoch: 5038 Train Loss: 0.52180 Validation Loss: 0.54820\n",
            "Epoch: 5039 Train Loss: 0.52285 Validation Loss: 0.54845\n",
            "Epoch: 5040 Train Loss: 0.52285 Validation Loss: 0.54872\n",
            "Epoch: 5041 Train Loss: 0.52536 Validation Loss: 0.54948\n",
            "Epoch: 5042 Train Loss: 0.52543 Validation Loss: 0.54844\n",
            "Epoch: 5043 Train Loss: 0.52489 Validation Loss: 0.54871\n",
            "Epoch: 5044 Train Loss: 0.52490 Validation Loss: 0.54844\n",
            "Epoch: 5045 Train Loss: 0.52320 Validation Loss: 0.54823\n",
            "Epoch: 5046 Train Loss: 0.52270 Validation Loss: 0.54920\n",
            "Epoch: 5047 Train Loss: 0.52283 Validation Loss: 0.54878\n",
            "Epoch: 5048 Train Loss: 0.52482 Validation Loss: 0.54869\n",
            "Epoch: 5049 Train Loss: 0.52512 Validation Loss: 0.54899\n",
            "Epoch: 5050 Train Loss: 0.52326 Validation Loss: 0.54828\n",
            "Epoch: 5051 Train Loss: 0.52390 Validation Loss: 0.54877\n",
            "Epoch: 5052 Train Loss: 0.52288 Validation Loss: 0.54855\n",
            "Epoch: 5053 Train Loss: 0.52204 Validation Loss: 0.54844\n",
            "Epoch: 5054 Train Loss: 0.52418 Validation Loss: 0.54869\n",
            "Epoch: 5055 Train Loss: 0.52278 Validation Loss: 0.54838\n",
            "Epoch: 5056 Train Loss: 0.52151 Validation Loss: 0.54845\n",
            "Epoch: 5057 Train Loss: 0.52391 Validation Loss: 0.54872\n",
            "Epoch: 5058 Train Loss: 0.52167 Validation Loss: 0.54798\n",
            "Epoch: 5059 Train Loss: 0.52354 Validation Loss: 0.54972\n",
            "Epoch: 5060 Train Loss: 0.52744 Validation Loss: 0.54953\n",
            "Epoch: 5061 Train Loss: 0.52378 Validation Loss: 0.54832\n",
            "Epoch: 5062 Train Loss: 0.52461 Validation Loss: 0.54805\n",
            "Epoch: 5063 Train Loss: 0.52379 Validation Loss: 0.54876\n",
            "Epoch: 5064 Train Loss: 0.52803 Validation Loss: 0.54859\n",
            "Epoch: 5065 Train Loss: 0.52480 Validation Loss: 0.54841\n",
            "Epoch: 5066 Train Loss: 0.52482 Validation Loss: 0.54831\n",
            "Epoch: 5067 Train Loss: 0.51993 Validation Loss: 0.54837\n",
            "Epoch: 5068 Train Loss: 0.52293 Validation Loss: 0.54950\n",
            "Epoch: 5069 Train Loss: 0.52295 Validation Loss: 0.54821\n",
            "Epoch: 5070 Train Loss: 0.52225 Validation Loss: 0.54925\n",
            "Epoch: 5071 Train Loss: 0.52500 Validation Loss: 0.54936\n",
            "Epoch: 5072 Train Loss: 0.52470 Validation Loss: 0.54844\n",
            "Epoch: 5073 Train Loss: 0.52582 Validation Loss: 0.54851\n",
            "Epoch: 5074 Train Loss: 0.52115 Validation Loss: 0.54810\n",
            "Epoch: 5075 Train Loss: 0.52293 Validation Loss: 0.54816\n",
            "Epoch: 5076 Train Loss: 0.52189 Validation Loss: 0.54851\n",
            "Epoch: 5077 Train Loss: 0.52610 Validation Loss: 0.54901\n",
            "Epoch: 5078 Train Loss: 0.52386 Validation Loss: 0.54846\n",
            "Epoch: 5079 Train Loss: 0.52534 Validation Loss: 0.54825\n",
            "Epoch: 5080 Train Loss: 0.52375 Validation Loss: 0.54902\n",
            "Epoch: 5081 Train Loss: 0.52450 Validation Loss: 0.54884\n",
            "Epoch: 5082 Train Loss: 0.52404 Validation Loss: 0.54847\n",
            "Epoch: 5083 Train Loss: 0.52286 Validation Loss: 0.54845\n",
            "Epoch: 5084 Train Loss: 0.52419 Validation Loss: 0.54791\n",
            "Epoch: 5085 Train Loss: 0.52099 Validation Loss: 0.54917\n",
            "Epoch: 5086 Train Loss: 0.52724 Validation Loss: 0.54960\n",
            "Epoch: 5087 Train Loss: 0.52590 Validation Loss: 0.54809\n",
            "Epoch: 5088 Train Loss: 0.52416 Validation Loss: 0.54821\n",
            "Epoch: 5089 Train Loss: 0.52378 Validation Loss: 0.54876\n",
            "Epoch: 5090 Train Loss: 0.52396 Validation Loss: 0.54921\n",
            "Epoch: 5091 Train Loss: 0.52313 Validation Loss: 0.54908\n",
            "Epoch: 5092 Train Loss: 0.52183 Validation Loss: 0.54806\n",
            "Epoch: 5093 Train Loss: 0.52184 Validation Loss: 0.54833\n",
            "Epoch: 5094 Train Loss: 0.52287 Validation Loss: 0.54934\n",
            "Epoch: 5095 Train Loss: 0.52592 Validation Loss: 0.54838\n",
            "Epoch: 5096 Train Loss: 0.52274 Validation Loss: 0.54849\n",
            "Epoch: 5097 Train Loss: 0.52483 Validation Loss: 0.54866\n",
            "Epoch: 5098 Train Loss: 0.52178 Validation Loss: 0.54860\n",
            "Epoch: 5099 Train Loss: 0.52180 Validation Loss: 0.54867\n",
            "Epoch: 5100 Train Loss: 0.52108 Validation Loss: 0.54860\n",
            "Epoch: 5101 Train Loss: 0.52387 Validation Loss: 0.54890\n",
            "Epoch: 5102 Train Loss: 0.52397 Validation Loss: 0.54858\n",
            "Epoch: 5103 Train Loss: 0.52195 Validation Loss: 0.54827\n",
            "Epoch: 5104 Train Loss: 0.52133 Validation Loss: 0.54862\n",
            "Epoch: 5105 Train Loss: 0.52697 Validation Loss: 0.54850\n",
            "Epoch: 5106 Train Loss: 0.52094 Validation Loss: 0.54856\n",
            "Epoch: 5107 Train Loss: 0.52383 Validation Loss: 0.54889\n",
            "Epoch: 5108 Train Loss: 0.52094 Validation Loss: 0.54873\n",
            "Epoch: 5109 Train Loss: 0.52189 Validation Loss: 0.54857\n",
            "Epoch: 5110 Train Loss: 0.52372 Validation Loss: 0.54869\n",
            "Epoch: 5111 Train Loss: 0.52326 Validation Loss: 0.54880\n",
            "Epoch: 5112 Train Loss: 0.52414 Validation Loss: 0.54801\n",
            "Epoch: 5113 Train Loss: 0.52163 Validation Loss: 0.54917\n",
            "Epoch: 5114 Train Loss: 0.52657 Validation Loss: 0.54977\n",
            "Epoch: 5115 Train Loss: 0.52429 Validation Loss: 0.54874\n",
            "Epoch: 5116 Train Loss: 0.52289 Validation Loss: 0.54833\n",
            "Epoch: 5117 Train Loss: 0.52652 Validation Loss: 0.54945\n",
            "Epoch: 5118 Train Loss: 0.52312 Validation Loss: 0.54853\n",
            "Epoch: 5119 Train Loss: 0.52590 Validation Loss: 0.54898\n",
            "Epoch: 5120 Train Loss: 0.52518 Validation Loss: 0.54830\n",
            "Epoch: 5121 Train Loss: 0.52288 Validation Loss: 0.54872\n",
            "Epoch: 5122 Train Loss: 0.52098 Validation Loss: 0.54846\n",
            "Epoch: 5123 Train Loss: 0.52285 Validation Loss: 0.54924\n",
            "Epoch: 5124 Train Loss: 0.52487 Validation Loss: 0.54885\n",
            "Epoch: 5125 Train Loss: 0.52395 Validation Loss: 0.54864\n",
            "Epoch: 5126 Train Loss: 0.52078 Validation Loss: 0.54832\n",
            "Epoch: 5127 Train Loss: 0.52301 Validation Loss: 0.54874\n",
            "Epoch: 5128 Train Loss: 0.52347 Validation Loss: 0.54909\n",
            "Epoch: 5129 Train Loss: 0.52296 Validation Loss: 0.54877\n",
            "Epoch: 5130 Train Loss: 0.52696 Validation Loss: 0.54823\n",
            "Epoch: 5131 Train Loss: 0.52080 Validation Loss: 0.54833\n",
            "Epoch: 5132 Train Loss: 0.52126 Validation Loss: 0.54882\n",
            "Epoch: 5133 Train Loss: 0.51989 Validation Loss: 0.54882\n",
            "Epoch: 5134 Train Loss: 0.52963 Validation Loss: 0.54932\n",
            "Epoch: 5135 Train Loss: 0.52525 Validation Loss: 0.54785\n",
            "Epoch: 5136 Train Loss: 0.52395 Validation Loss: 0.54810\n",
            "Epoch: 5137 Train Loss: 0.52228 Validation Loss: 0.54926\n",
            "Epoch: 5138 Train Loss: 0.52495 Validation Loss: 0.54887\n",
            "Epoch: 5139 Train Loss: 0.52296 Validation Loss: 0.54841\n",
            "Epoch: 5140 Train Loss: 0.52486 Validation Loss: 0.54869\n",
            "Epoch: 5141 Train Loss: 0.52297 Validation Loss: 0.54835\n",
            "Epoch: 5142 Train Loss: 0.52136 Validation Loss: 0.54813\n",
            "Epoch: 5143 Train Loss: 0.52160 Validation Loss: 0.54893\n",
            "Epoch: 5144 Train Loss: 0.52384 Validation Loss: 0.54911\n",
            "Epoch: 5145 Train Loss: 0.52189 Validation Loss: 0.54870\n",
            "Epoch: 5146 Train Loss: 0.52320 Validation Loss: 0.54843\n",
            "Epoch: 5147 Train Loss: 0.52097 Validation Loss: 0.54852\n",
            "Epoch: 5148 Train Loss: 0.52792 Validation Loss: 0.54944\n",
            "Epoch: 5149 Train Loss: 0.52220 Validation Loss: 0.54836\n",
            "Epoch: 5150 Train Loss: 0.52080 Validation Loss: 0.54869\n",
            "Epoch: 5151 Train Loss: 0.52488 Validation Loss: 0.54913\n",
            "Epoch: 5152 Train Loss: 0.52400 Validation Loss: 0.54879\n",
            "Epoch: 5153 Train Loss: 0.52185 Validation Loss: 0.54851\n",
            "Epoch: 5154 Train Loss: 0.52391 Validation Loss: 0.54908\n",
            "Epoch: 5155 Train Loss: 0.52289 Validation Loss: 0.54817\n",
            "Epoch: 5156 Train Loss: 0.52408 Validation Loss: 0.54837\n",
            "Epoch: 5157 Train Loss: 0.52284 Validation Loss: 0.54901\n",
            "Epoch: 5158 Train Loss: 0.52177 Validation Loss: 0.54874\n",
            "Epoch: 5159 Train Loss: 0.52391 Validation Loss: 0.54875\n",
            "Epoch: 5160 Train Loss: 0.52290 Validation Loss: 0.54876\n",
            "Epoch: 5161 Train Loss: 0.52350 Validation Loss: 0.54871\n",
            "Epoch: 5162 Train Loss: 0.52204 Validation Loss: 0.54873\n",
            "Epoch: 5163 Train Loss: 0.52202 Validation Loss: 0.54968\n",
            "Epoch: 5164 Train Loss: 0.52185 Validation Loss: 0.54877\n",
            "Epoch: 5165 Train Loss: 0.52499 Validation Loss: 0.54852\n",
            "Epoch: 5166 Train Loss: 0.52226 Validation Loss: 0.54873\n",
            "Epoch: 5167 Train Loss: 0.52294 Validation Loss: 0.54812\n",
            "Epoch: 5168 Train Loss: 0.52322 Validation Loss: 0.54840\n",
            "Epoch: 5169 Train Loss: 0.52321 Validation Loss: 0.54879\n",
            "Epoch: 5170 Train Loss: 0.52611 Validation Loss: 0.54883\n",
            "Epoch: 5171 Train Loss: 0.52690 Validation Loss: 0.54869\n",
            "Epoch: 5172 Train Loss: 0.52083 Validation Loss: 0.54812\n",
            "Epoch: 5173 Train Loss: 0.51998 Validation Loss: 0.54827\n",
            "Epoch: 5174 Train Loss: 0.52382 Validation Loss: 0.54878\n",
            "Epoch: 5175 Train Loss: 0.52181 Validation Loss: 0.54873\n",
            "Epoch: 5176 Train Loss: 0.52388 Validation Loss: 0.54870\n",
            "Epoch: 5177 Train Loss: 0.52493 Validation Loss: 0.54821\n",
            "Epoch: 5178 Train Loss: 0.52487 Validation Loss: 0.54940\n",
            "Epoch: 5179 Train Loss: 0.52611 Validation Loss: 0.54817\n",
            "Epoch: 5180 Train Loss: 0.52479 Validation Loss: 0.54852\n",
            "Epoch: 5181 Train Loss: 0.52207 Validation Loss: 0.54887\n",
            "Epoch: 5182 Train Loss: 0.52520 Validation Loss: 0.54816\n",
            "Epoch: 5183 Train Loss: 0.52440 Validation Loss: 0.54915\n",
            "Epoch: 5184 Train Loss: 0.52598 Validation Loss: 0.54811\n",
            "Epoch: 5185 Train Loss: 0.52477 Validation Loss: 0.54839\n",
            "Epoch: 5186 Train Loss: 0.52309 Validation Loss: 0.54915\n",
            "Epoch: 5187 Train Loss: 0.52175 Validation Loss: 0.54879\n",
            "Epoch: 5188 Train Loss: 0.52214 Validation Loss: 0.54821\n",
            "Epoch: 5189 Train Loss: 0.52707 Validation Loss: 0.54895\n",
            "Epoch: 5190 Train Loss: 0.52227 Validation Loss: 0.54808\n",
            "Epoch: 5191 Train Loss: 0.52170 Validation Loss: 0.54855\n",
            "Epoch: 5192 Train Loss: 0.52505 Validation Loss: 0.54920\n",
            "Epoch: 5193 Train Loss: 0.52382 Validation Loss: 0.54895\n",
            "Epoch: 5194 Train Loss: 0.52584 Validation Loss: 0.54843\n",
            "Epoch: 5195 Train Loss: 0.51991 Validation Loss: 0.54825\n",
            "Epoch: 5196 Train Loss: 0.52095 Validation Loss: 0.54861\n",
            "Epoch: 5197 Train Loss: 0.52007 Validation Loss: 0.54876\n",
            "Epoch: 5198 Train Loss: 0.52385 Validation Loss: 0.54916\n",
            "Epoch: 5199 Train Loss: 0.52342 Validation Loss: 0.54843\n",
            "Epoch: 5200 Train Loss: 0.52591 Validation Loss: 0.54916\n",
            "Epoch: 5201 Train Loss: 0.52145 Validation Loss: 0.54819\n",
            "Epoch: 5202 Train Loss: 0.52471 Validation Loss: 0.54906\n",
            "Epoch: 5203 Train Loss: 0.52193 Validation Loss: 0.54920\n",
            "Epoch: 5204 Train Loss: 0.52487 Validation Loss: 0.54887\n",
            "Epoch: 5205 Train Loss: 0.52391 Validation Loss: 0.54876\n",
            "Epoch: 5206 Train Loss: 0.52513 Validation Loss: 0.54844\n",
            "Epoch: 5207 Train Loss: 0.52490 Validation Loss: 0.54861\n",
            "Epoch: 5208 Train Loss: 0.52089 Validation Loss: 0.54859\n",
            "Epoch: 5209 Train Loss: 0.52028 Validation Loss: 0.54853\n",
            "Epoch: 5210 Train Loss: 0.52272 Validation Loss: 0.54934\n",
            "Epoch: 5211 Train Loss: 0.52284 Validation Loss: 0.54914\n",
            "Epoch: 5212 Train Loss: 0.52287 Validation Loss: 0.54863\n",
            "Epoch: 5213 Train Loss: 0.52079 Validation Loss: 0.54849\n",
            "Epoch: 5214 Train Loss: 0.52178 Validation Loss: 0.54849\n",
            "Epoch: 5215 Train Loss: 0.52173 Validation Loss: 0.54861\n",
            "Epoch: 5216 Train Loss: 0.52280 Validation Loss: 0.54909\n",
            "Epoch: 5217 Train Loss: 0.52637 Validation Loss: 0.54842\n",
            "Epoch: 5218 Train Loss: 0.52579 Validation Loss: 0.54863\n",
            "Epoch: 5219 Train Loss: 0.52194 Validation Loss: 0.54852\n",
            "Epoch: 5220 Train Loss: 0.52281 Validation Loss: 0.54852\n",
            "Epoch: 5221 Train Loss: 0.52480 Validation Loss: 0.54865\n",
            "Epoch: 5222 Train Loss: 0.52498 Validation Loss: 0.54794\n",
            "Epoch: 5223 Train Loss: 0.52373 Validation Loss: 0.54865\n",
            "Epoch: 5224 Train Loss: 0.52309 Validation Loss: 0.54919\n",
            "Epoch: 5225 Train Loss: 0.52186 Validation Loss: 0.54840\n",
            "Epoch: 5226 Train Loss: 0.52537 Validation Loss: 0.54883\n",
            "Epoch: 5227 Train Loss: 0.52204 Validation Loss: 0.54811\n",
            "Epoch: 5228 Train Loss: 0.52176 Validation Loss: 0.54839\n",
            "Epoch: 5229 Train Loss: 0.52557 Validation Loss: 0.54940\n",
            "Epoch: 5230 Train Loss: 0.52265 Validation Loss: 0.54840\n",
            "Epoch: 5231 Train Loss: 0.52189 Validation Loss: 0.54823\n",
            "Epoch: 5232 Train Loss: 0.52194 Validation Loss: 0.54860\n",
            "Epoch: 5233 Train Loss: 0.52201 Validation Loss: 0.54916\n",
            "Epoch: 5234 Train Loss: 0.52087 Validation Loss: 0.54789\n",
            "Epoch: 5235 Train Loss: 0.52179 Validation Loss: 0.54985\n",
            "Epoch: 5236 Train Loss: 0.52103 Validation Loss: 0.54878\n",
            "Epoch: 5237 Train Loss: 0.52397 Validation Loss: 0.54900\n",
            "Epoch: 5238 Train Loss: 0.52621 Validation Loss: 0.54852\n",
            "Epoch: 5239 Train Loss: 0.52213 Validation Loss: 0.54812\n",
            "Epoch: 5240 Train Loss: 0.52327 Validation Loss: 0.54862\n",
            "Epoch: 5241 Train Loss: 0.52546 Validation Loss: 0.54913\n",
            "Epoch: 5242 Train Loss: 0.52437 Validation Loss: 0.54841\n",
            "Epoch: 5243 Train Loss: 0.52404 Validation Loss: 0.54847\n",
            "Epoch: 5244 Train Loss: 0.52483 Validation Loss: 0.54826\n",
            "Epoch: 5245 Train Loss: 0.52331 Validation Loss: 0.54890\n",
            "Epoch: 5246 Train Loss: 0.52695 Validation Loss: 0.54845\n",
            "Epoch: 5247 Train Loss: 0.52591 Validation Loss: 0.54863\n",
            "Epoch: 5248 Train Loss: 0.52478 Validation Loss: 0.54838\n",
            "Epoch: 5249 Train Loss: 0.52214 Validation Loss: 0.54818\n",
            "Epoch: 5250 Train Loss: 0.52402 Validation Loss: 0.54932\n",
            "Epoch: 5251 Train Loss: 0.52339 Validation Loss: 0.54915\n",
            "Epoch: 5252 Train Loss: 0.52206 Validation Loss: 0.54820\n",
            "Epoch: 5253 Train Loss: 0.52095 Validation Loss: 0.54893\n",
            "Epoch: 5254 Train Loss: 0.52208 Validation Loss: 0.54835\n",
            "Epoch: 5255 Train Loss: 0.52180 Validation Loss: 0.54876\n",
            "Epoch: 5256 Train Loss: 0.52187 Validation Loss: 0.54874\n",
            "Epoch: 5257 Train Loss: 0.52387 Validation Loss: 0.54894\n",
            "Epoch: 5258 Train Loss: 0.52477 Validation Loss: 0.54851\n",
            "Epoch: 5259 Train Loss: 0.52508 Validation Loss: 0.54880\n",
            "Epoch: 5260 Train Loss: 0.52392 Validation Loss: 0.54877\n",
            "Epoch: 5261 Train Loss: 0.52678 Validation Loss: 0.54826\n",
            "Epoch: 5262 Train Loss: 0.52352 Validation Loss: 0.54831\n",
            "Epoch: 5263 Train Loss: 0.52381 Validation Loss: 0.54850\n",
            "Epoch: 5264 Train Loss: 0.52200 Validation Loss: 0.54876\n",
            "Epoch: 5265 Train Loss: 0.52204 Validation Loss: 0.54918\n",
            "Epoch: 5266 Train Loss: 0.52491 Validation Loss: 0.54914\n",
            "Epoch: 5267 Train Loss: 0.52283 Validation Loss: 0.54890\n",
            "Epoch: 5268 Train Loss: 0.52401 Validation Loss: 0.54866\n",
            "Epoch: 5269 Train Loss: 0.52082 Validation Loss: 0.54824\n",
            "Epoch: 5270 Train Loss: 0.52488 Validation Loss: 0.54834\n",
            "Epoch: 5271 Train Loss: 0.52349 Validation Loss: 0.54912\n",
            "Epoch: 5272 Train Loss: 0.52100 Validation Loss: 0.54831\n",
            "Epoch: 5273 Train Loss: 0.52292 Validation Loss: 0.54868\n",
            "Epoch: 5274 Train Loss: 0.52303 Validation Loss: 0.54840\n",
            "Epoch: 5275 Train Loss: 0.52297 Validation Loss: 0.54884\n",
            "Epoch: 5276 Train Loss: 0.52141 Validation Loss: 0.54879\n",
            "Epoch: 5277 Train Loss: 0.52555 Validation Loss: 0.54806\n",
            "Epoch: 5278 Train Loss: 0.52318 Validation Loss: 0.54829\n",
            "Epoch: 5279 Train Loss: 0.52202 Validation Loss: 0.54942\n",
            "Epoch: 5280 Train Loss: 0.52620 Validation Loss: 0.54886\n",
            "Epoch: 5281 Train Loss: 0.52337 Validation Loss: 0.54820\n",
            "Epoch: 5282 Train Loss: 0.52271 Validation Loss: 0.54898\n",
            "Epoch: 5283 Train Loss: 0.52170 Validation Loss: 0.54914\n",
            "Epoch: 5284 Train Loss: 0.52509 Validation Loss: 0.54886\n",
            "Epoch: 5285 Train Loss: 0.51950 Validation Loss: 0.54791\n",
            "Epoch: 5286 Train Loss: 0.51998 Validation Loss: 0.54848\n",
            "Epoch: 5287 Train Loss: 0.52611 Validation Loss: 0.54999\n",
            "Epoch: 5288 Train Loss: 0.52393 Validation Loss: 0.54880\n",
            "Epoch: 5289 Train Loss: 0.52180 Validation Loss: 0.54833\n",
            "Epoch: 5290 Train Loss: 0.52384 Validation Loss: 0.54877\n",
            "Epoch: 5291 Train Loss: 0.52404 Validation Loss: 0.54826\n",
            "Epoch: 5292 Train Loss: 0.52636 Validation Loss: 0.54943\n",
            "Epoch: 5293 Train Loss: 0.52418 Validation Loss: 0.54868\n",
            "Epoch: 5294 Train Loss: 0.52387 Validation Loss: 0.54826\n",
            "Epoch: 5295 Train Loss: 0.52184 Validation Loss: 0.54837\n",
            "Epoch: 5296 Train Loss: 0.52594 Validation Loss: 0.54904\n",
            "Epoch: 5297 Train Loss: 0.52207 Validation Loss: 0.54813\n",
            "Epoch: 5298 Train Loss: 0.52657 Validation Loss: 0.54951\n",
            "Epoch: 5299 Train Loss: 0.52314 Validation Loss: 0.54805\n",
            "Epoch: 5300 Train Loss: 0.52086 Validation Loss: 0.54835\n",
            "Epoch: 5301 Train Loss: 0.52094 Validation Loss: 0.54880\n",
            "Epoch: 5302 Train Loss: 0.52198 Validation Loss: 0.54930\n",
            "Epoch: 5303 Train Loss: 0.52397 Validation Loss: 0.54893\n",
            "Epoch: 5304 Train Loss: 0.52290 Validation Loss: 0.54874\n",
            "Epoch: 5305 Train Loss: 0.52663 Validation Loss: 0.54836\n",
            "Epoch: 5306 Train Loss: 0.52216 Validation Loss: 0.54859\n",
            "Epoch: 5307 Train Loss: 0.52289 Validation Loss: 0.54910\n",
            "Epoch: 5308 Train Loss: 0.52809 Validation Loss: 0.54945\n",
            "Epoch: 5309 Train Loss: 0.52311 Validation Loss: 0.54843\n",
            "Epoch: 5310 Train Loss: 0.52280 Validation Loss: 0.54856\n",
            "Epoch: 5311 Train Loss: 0.52211 Validation Loss: 0.54828\n",
            "Epoch: 5312 Train Loss: 0.52318 Validation Loss: 0.54955\n",
            "Epoch: 5313 Train Loss: 0.52080 Validation Loss: 0.54840\n",
            "Epoch: 5314 Train Loss: 0.52191 Validation Loss: 0.54857\n",
            "Epoch: 5315 Train Loss: 0.52477 Validation Loss: 0.54885\n",
            "Epoch: 5316 Train Loss: 0.52290 Validation Loss: 0.54857\n",
            "Epoch: 5317 Train Loss: 0.52283 Validation Loss: 0.54845\n",
            "Epoch: 5318 Train Loss: 0.52279 Validation Loss: 0.54872\n",
            "Epoch: 5319 Train Loss: 0.52304 Validation Loss: 0.54898\n",
            "Epoch: 5320 Train Loss: 0.52188 Validation Loss: 0.54839\n",
            "Epoch: 5321 Train Loss: 0.52690 Validation Loss: 0.54885\n",
            "Epoch: 5322 Train Loss: 0.52323 Validation Loss: 0.54833\n",
            "Epoch: 5323 Train Loss: 0.52088 Validation Loss: 0.54846\n",
            "Epoch: 5324 Train Loss: 0.52095 Validation Loss: 0.54907\n",
            "Epoch: 5325 Train Loss: 0.52485 Validation Loss: 0.54842\n",
            "Epoch: 5326 Train Loss: 0.52321 Validation Loss: 0.54881\n",
            "Epoch: 5327 Train Loss: 0.52399 Validation Loss: 0.54858\n",
            "Epoch: 5328 Train Loss: 0.52099 Validation Loss: 0.54914\n",
            "Epoch: 5329 Train Loss: 0.52203 Validation Loss: 0.54860\n",
            "Epoch: 5330 Train Loss: 0.52182 Validation Loss: 0.54813\n",
            "Epoch: 5331 Train Loss: 0.52299 Validation Loss: 0.54860\n",
            "Epoch: 5332 Train Loss: 0.52105 Validation Loss: 0.54841\n",
            "Epoch: 5333 Train Loss: 0.52282 Validation Loss: 0.54910\n",
            "Epoch: 5334 Train Loss: 0.52493 Validation Loss: 0.54915\n",
            "Epoch: 5335 Train Loss: 0.52567 Validation Loss: 0.54836\n",
            "Epoch: 5336 Train Loss: 0.52220 Validation Loss: 0.54801\n",
            "Epoch: 5337 Train Loss: 0.52382 Validation Loss: 0.54895\n",
            "Epoch: 5338 Train Loss: 0.52531 Validation Loss: 0.54869\n",
            "Epoch: 5339 Train Loss: 0.52182 Validation Loss: 0.54789\n",
            "Epoch: 5340 Train Loss: 0.52412 Validation Loss: 0.54927\n",
            "Epoch: 5341 Train Loss: 0.52278 Validation Loss: 0.54842\n",
            "Epoch: 5342 Train Loss: 0.52367 Validation Loss: 0.54927\n",
            "Epoch: 5343 Train Loss: 0.52001 Validation Loss: 0.54874\n",
            "Epoch: 5344 Train Loss: 0.52205 Validation Loss: 0.54883\n",
            "Epoch: 5345 Train Loss: 0.52343 Validation Loss: 0.54907\n",
            "Epoch: 5346 Train Loss: 0.51982 Validation Loss: 0.54871\n",
            "Epoch: 5347 Train Loss: 0.52003 Validation Loss: 0.54845\n",
            "Epoch: 5348 Train Loss: 0.52236 Validation Loss: 0.54839\n",
            "Epoch: 5349 Train Loss: 0.52374 Validation Loss: 0.54924\n",
            "Epoch: 5350 Train Loss: 0.52211 Validation Loss: 0.54859\n",
            "Epoch: 5351 Train Loss: 0.52515 Validation Loss: 0.54936\n",
            "Epoch: 5352 Train Loss: 0.52612 Validation Loss: 0.54906\n",
            "Epoch: 5353 Train Loss: 0.52547 Validation Loss: 0.54767\n",
            "Epoch: 5354 Train Loss: 0.52304 Validation Loss: 0.54885\n",
            "Epoch: 5355 Train Loss: 0.52199 Validation Loss: 0.54858\n",
            "Epoch: 5356 Train Loss: 0.52381 Validation Loss: 0.54911\n",
            "Epoch: 5357 Train Loss: 0.52186 Validation Loss: 0.54895\n",
            "Epoch: 5358 Train Loss: 0.52275 Validation Loss: 0.54884\n",
            "Epoch: 5359 Train Loss: 0.52323 Validation Loss: 0.54834\n",
            "Epoch: 5360 Train Loss: 0.52190 Validation Loss: 0.54848\n",
            "Epoch: 5361 Train Loss: 0.52068 Validation Loss: 0.54972\n",
            "Epoch: 5362 Train Loss: 0.52297 Validation Loss: 0.54914\n",
            "Epoch: 5363 Train Loss: 0.52486 Validation Loss: 0.54876\n",
            "Epoch: 5364 Train Loss: 0.51991 Validation Loss: 0.54832\n",
            "Epoch: 5365 Train Loss: 0.52367 Validation Loss: 0.54888\n",
            "Epoch: 5366 Train Loss: 0.52587 Validation Loss: 0.54876\n",
            "Epoch: 5367 Train Loss: 0.52381 Validation Loss: 0.54847\n",
            "Epoch: 5368 Train Loss: 0.52397 Validation Loss: 0.54842\n",
            "Epoch: 5369 Train Loss: 0.52480 Validation Loss: 0.54925\n",
            "Epoch: 5370 Train Loss: 0.52365 Validation Loss: 0.54826\n",
            "Epoch: 5371 Train Loss: 0.52181 Validation Loss: 0.54819\n",
            "Epoch: 5372 Train Loss: 0.52284 Validation Loss: 0.54839\n",
            "Epoch: 5373 Train Loss: 0.52407 Validation Loss: 0.54864\n",
            "Epoch: 5374 Train Loss: 0.52383 Validation Loss: 0.54871\n",
            "Epoch: 5375 Train Loss: 0.52282 Validation Loss: 0.54894\n",
            "Epoch: 5376 Train Loss: 0.53031 Validation Loss: 0.54940\n",
            "Epoch: 5377 Train Loss: 0.52325 Validation Loss: 0.54778\n",
            "Epoch: 5378 Train Loss: 0.52323 Validation Loss: 0.54820\n",
            "Epoch: 5379 Train Loss: 0.52490 Validation Loss: 0.54910\n",
            "Epoch: 5380 Train Loss: 0.52290 Validation Loss: 0.54883\n",
            "Epoch: 5381 Train Loss: 0.52284 Validation Loss: 0.54862\n",
            "Epoch: 5382 Train Loss: 0.52294 Validation Loss: 0.54880\n",
            "Epoch: 5383 Train Loss: 0.52394 Validation Loss: 0.54835\n",
            "Epoch: 5384 Train Loss: 0.52191 Validation Loss: 0.54866\n",
            "Epoch: 5385 Train Loss: 0.52407 Validation Loss: 0.54909\n",
            "Epoch: 5386 Train Loss: 0.52093 Validation Loss: 0.54817\n",
            "Epoch: 5387 Train Loss: 0.52283 Validation Loss: 0.54851\n",
            "Epoch: 5388 Train Loss: 0.52315 Validation Loss: 0.54861\n",
            "Epoch: 5389 Train Loss: 0.52315 Validation Loss: 0.54850\n",
            "Epoch: 5390 Train Loss: 0.51988 Validation Loss: 0.54818\n",
            "Epoch: 5391 Train Loss: 0.52314 Validation Loss: 0.54847\n",
            "Epoch: 5392 Train Loss: 0.52612 Validation Loss: 0.54912\n",
            "Epoch: 5393 Train Loss: 0.52022 Validation Loss: 0.54855\n",
            "Epoch: 5394 Train Loss: 0.52296 Validation Loss: 0.54942\n",
            "Epoch: 5395 Train Loss: 0.52332 Validation Loss: 0.54864\n",
            "Epoch: 5396 Train Loss: 0.52192 Validation Loss: 0.54836\n",
            "Epoch: 5397 Train Loss: 0.52128 Validation Loss: 0.54845\n",
            "Epoch: 5398 Train Loss: 0.52193 Validation Loss: 0.54864\n",
            "Epoch: 5399 Train Loss: 0.52422 Validation Loss: 0.54875\n",
            "Epoch: 5400 Train Loss: 0.52425 Validation Loss: 0.54861\n",
            "Epoch: 5401 Train Loss: 0.52315 Validation Loss: 0.54862\n",
            "Epoch: 5402 Train Loss: 0.52575 Validation Loss: 0.54912\n",
            "Epoch: 5403 Train Loss: 0.52285 Validation Loss: 0.54858\n",
            "Epoch: 5404 Train Loss: 0.52389 Validation Loss: 0.54832\n",
            "Epoch: 5405 Train Loss: 0.52567 Validation Loss: 0.54929\n",
            "Epoch: 5406 Train Loss: 0.52323 Validation Loss: 0.54802\n",
            "Epoch: 5407 Train Loss: 0.52224 Validation Loss: 0.54901\n",
            "Epoch: 5408 Train Loss: 0.52103 Validation Loss: 0.54825\n",
            "Epoch: 5409 Train Loss: 0.52228 Validation Loss: 0.54860\n",
            "Epoch: 5410 Train Loss: 0.52191 Validation Loss: 0.54900\n",
            "Epoch: 5411 Train Loss: 0.52309 Validation Loss: 0.54810\n",
            "Epoch: 5412 Train Loss: 0.52309 Validation Loss: 0.54936\n",
            "Epoch: 5413 Train Loss: 0.52390 Validation Loss: 0.54868\n",
            "Epoch: 5414 Train Loss: 0.52181 Validation Loss: 0.54850\n",
            "Epoch: 5415 Train Loss: 0.52290 Validation Loss: 0.54847\n",
            "Epoch: 5416 Train Loss: 0.52109 Validation Loss: 0.54833\n",
            "Epoch: 5417 Train Loss: 0.51913 Validation Loss: 0.54864\n",
            "Epoch: 5418 Train Loss: 0.52501 Validation Loss: 0.54926\n",
            "Epoch: 5419 Train Loss: 0.52504 Validation Loss: 0.54881\n",
            "Epoch: 5420 Train Loss: 0.52791 Validation Loss: 0.54844\n",
            "Epoch: 5421 Train Loss: 0.52187 Validation Loss: 0.54817\n",
            "Epoch: 5422 Train Loss: 0.52030 Validation Loss: 0.54811\n",
            "Epoch: 5423 Train Loss: 0.52536 Validation Loss: 0.54936\n",
            "Epoch: 5424 Train Loss: 0.52297 Validation Loss: 0.54848\n",
            "Epoch: 5425 Train Loss: 0.52811 Validation Loss: 0.54889\n",
            "Epoch: 5426 Train Loss: 0.52191 Validation Loss: 0.54819\n",
            "Epoch: 5427 Train Loss: 0.52689 Validation Loss: 0.54868\n",
            "Epoch: 5428 Train Loss: 0.52376 Validation Loss: 0.54845\n",
            "Epoch: 5429 Train Loss: 0.52283 Validation Loss: 0.54831\n",
            "Epoch: 5430 Train Loss: 0.52387 Validation Loss: 0.54881\n",
            "Epoch: 5431 Train Loss: 0.52115 Validation Loss: 0.54834\n",
            "Epoch: 5432 Train Loss: 0.51921 Validation Loss: 0.54839\n",
            "Epoch: 5433 Train Loss: 0.52621 Validation Loss: 0.54988\n",
            "Epoch: 5434 Train Loss: 0.52383 Validation Loss: 0.54855\n",
            "Epoch: 5435 Train Loss: 0.52394 Validation Loss: 0.54852\n",
            "Epoch: 5436 Train Loss: 0.52700 Validation Loss: 0.54858\n",
            "Epoch: 5437 Train Loss: 0.52369 Validation Loss: 0.54801\n",
            "Epoch: 5438 Train Loss: 0.52613 Validation Loss: 0.54854\n",
            "Epoch: 5439 Train Loss: 0.52296 Validation Loss: 0.54832\n",
            "Epoch: 5440 Train Loss: 0.52405 Validation Loss: 0.54943\n",
            "Epoch: 5441 Train Loss: 0.52374 Validation Loss: 0.54868\n",
            "Epoch: 5442 Train Loss: 0.52404 Validation Loss: 0.54798\n",
            "Epoch: 5443 Train Loss: 0.52214 Validation Loss: 0.54825\n",
            "Epoch: 5444 Train Loss: 0.52537 Validation Loss: 0.54973\n",
            "Epoch: 5445 Train Loss: 0.52644 Validation Loss: 0.54799\n",
            "Epoch: 5446 Train Loss: 0.52488 Validation Loss: 0.54927\n",
            "Epoch: 5447 Train Loss: 0.52405 Validation Loss: 0.54828\n",
            "Epoch: 5448 Train Loss: 0.52192 Validation Loss: 0.54886\n",
            "Epoch: 5449 Train Loss: 0.52240 Validation Loss: 0.54809\n",
            "Epoch: 5450 Train Loss: 0.52374 Validation Loss: 0.54908\n",
            "Epoch: 5451 Train Loss: 0.52418 Validation Loss: 0.54938\n",
            "Epoch: 5452 Train Loss: 0.52476 Validation Loss: 0.54881\n",
            "Epoch: 5453 Train Loss: 0.52179 Validation Loss: 0.54813\n",
            "Epoch: 5454 Train Loss: 0.52255 Validation Loss: 0.54826\n",
            "Epoch: 5455 Train Loss: 0.52396 Validation Loss: 0.54837\n",
            "Epoch: 5456 Train Loss: 0.52175 Validation Loss: 0.54851\n",
            "Epoch: 5457 Train Loss: 0.52188 Validation Loss: 0.54856\n",
            "Epoch: 5458 Train Loss: 0.52386 Validation Loss: 0.54914\n",
            "Epoch: 5459 Train Loss: 0.52806 Validation Loss: 0.54921\n",
            "Epoch: 5460 Train Loss: 0.52223 Validation Loss: 0.54762\n",
            "Epoch: 5461 Train Loss: 0.52194 Validation Loss: 0.54933\n",
            "Epoch: 5462 Train Loss: 0.52197 Validation Loss: 0.54910\n",
            "Epoch: 5463 Train Loss: 0.52392 Validation Loss: 0.54860\n",
            "Epoch: 5464 Train Loss: 0.52406 Validation Loss: 0.54855\n",
            "Epoch: 5465 Train Loss: 0.52597 Validation Loss: 0.54870\n",
            "Epoch: 5466 Train Loss: 0.52509 Validation Loss: 0.54911\n",
            "Epoch: 5467 Train Loss: 0.52330 Validation Loss: 0.54798\n",
            "Epoch: 5468 Train Loss: 0.52482 Validation Loss: 0.54851\n",
            "Epoch: 5469 Train Loss: 0.52274 Validation Loss: 0.54863\n",
            "Epoch: 5470 Train Loss: 0.52393 Validation Loss: 0.54893\n",
            "Epoch: 5471 Train Loss: 0.52186 Validation Loss: 0.54873\n",
            "Epoch: 5472 Train Loss: 0.52353 Validation Loss: 0.54853\n",
            "Epoch: 5473 Train Loss: 0.52385 Validation Loss: 0.54999\n",
            "Epoch: 5474 Train Loss: 0.52433 Validation Loss: 0.54968\n",
            "Epoch: 5475 Train Loss: 0.51987 Validation Loss: 0.54816\n",
            "Epoch: 5476 Train Loss: 0.52429 Validation Loss: 0.54871\n",
            "Epoch: 5477 Train Loss: 0.51986 Validation Loss: 0.54839\n",
            "Epoch: 5478 Train Loss: 0.52391 Validation Loss: 0.54882\n",
            "Epoch: 5479 Train Loss: 0.52482 Validation Loss: 0.54806\n",
            "Epoch: 5480 Train Loss: 0.52283 Validation Loss: 0.54907\n",
            "Epoch: 5481 Train Loss: 0.52206 Validation Loss: 0.54863\n",
            "Epoch: 5482 Train Loss: 0.52423 Validation Loss: 0.54908\n",
            "Epoch: 5483 Train Loss: 0.52101 Validation Loss: 0.54816\n",
            "Epoch: 5484 Train Loss: 0.52274 Validation Loss: 0.54848\n",
            "Epoch: 5485 Train Loss: 0.51974 Validation Loss: 0.54880\n",
            "Epoch: 5486 Train Loss: 0.52392 Validation Loss: 0.54932\n",
            "Epoch: 5487 Train Loss: 0.52277 Validation Loss: 0.54851\n",
            "Epoch: 5488 Train Loss: 0.52279 Validation Loss: 0.54847\n",
            "Epoch: 5489 Train Loss: 0.52791 Validation Loss: 0.54883\n",
            "Epoch: 5490 Train Loss: 0.52201 Validation Loss: 0.54834\n",
            "Epoch: 5491 Train Loss: 0.52398 Validation Loss: 0.54875\n",
            "Epoch: 5492 Train Loss: 0.52183 Validation Loss: 0.54886\n",
            "Epoch: 5493 Train Loss: 0.52313 Validation Loss: 0.54860\n",
            "Epoch: 5494 Train Loss: 0.52348 Validation Loss: 0.54800\n",
            "Epoch: 5495 Train Loss: 0.52195 Validation Loss: 0.54930\n",
            "Epoch: 5496 Train Loss: 0.52493 Validation Loss: 0.54919\n",
            "Epoch: 5497 Train Loss: 0.52185 Validation Loss: 0.54856\n",
            "Epoch: 5498 Train Loss: 0.52389 Validation Loss: 0.54858\n",
            "Epoch: 5499 Train Loss: 0.52651 Validation Loss: 0.54912\n",
            "Epoch: 5500 Train Loss: 0.52418 Validation Loss: 0.54828\n",
            "Epoch: 5501 Train Loss: 0.52381 Validation Loss: 0.54852\n",
            "Epoch: 5502 Train Loss: 0.52226 Validation Loss: 0.54855\n",
            "Epoch: 5503 Train Loss: 0.52180 Validation Loss: 0.54848\n",
            "Epoch: 5504 Train Loss: 0.52283 Validation Loss: 0.54832\n",
            "Epoch: 5505 Train Loss: 0.52096 Validation Loss: 0.54874\n",
            "Epoch: 5506 Train Loss: 0.52290 Validation Loss: 0.54859\n",
            "Epoch: 5507 Train Loss: 0.52588 Validation Loss: 0.54894\n",
            "Epoch: 5508 Train Loss: 0.52643 Validation Loss: 0.54841\n",
            "Epoch: 5509 Train Loss: 0.52401 Validation Loss: 0.54837\n",
            "Epoch: 5510 Train Loss: 0.52083 Validation Loss: 0.54856\n",
            "Epoch: 5511 Train Loss: 0.52682 Validation Loss: 0.54906\n",
            "Epoch: 5512 Train Loss: 0.52177 Validation Loss: 0.54849\n",
            "Epoch: 5513 Train Loss: 0.52286 Validation Loss: 0.54895\n",
            "Epoch: 5514 Train Loss: 0.52210 Validation Loss: 0.54848\n",
            "Epoch: 5515 Train Loss: 0.52539 Validation Loss: 0.54937\n",
            "Epoch: 5516 Train Loss: 0.52245 Validation Loss: 0.54788\n",
            "Epoch: 5517 Train Loss: 0.52275 Validation Loss: 0.54844\n",
            "Epoch: 5518 Train Loss: 0.52101 Validation Loss: 0.54853\n",
            "Epoch: 5519 Train Loss: 0.52961 Validation Loss: 0.55017\n",
            "Epoch: 5520 Train Loss: 0.52511 Validation Loss: 0.54791\n",
            "Epoch: 5521 Train Loss: 0.52184 Validation Loss: 0.54834\n",
            "Epoch: 5522 Train Loss: 0.52192 Validation Loss: 0.54910\n",
            "Epoch: 5523 Train Loss: 0.52091 Validation Loss: 0.54862\n",
            "Epoch: 5524 Train Loss: 0.52278 Validation Loss: 0.54867\n",
            "Epoch: 5525 Train Loss: 0.52336 Validation Loss: 0.54829\n",
            "Epoch: 5526 Train Loss: 0.52445 Validation Loss: 0.54894\n",
            "Epoch: 5527 Train Loss: 0.52287 Validation Loss: 0.54835\n",
            "Epoch: 5528 Train Loss: 0.52505 Validation Loss: 0.54865\n",
            "Epoch: 5529 Train Loss: 0.52204 Validation Loss: 0.54850\n",
            "Epoch: 5530 Train Loss: 0.52287 Validation Loss: 0.54842\n",
            "Epoch: 5531 Train Loss: 0.52295 Validation Loss: 0.54901\n",
            "Epoch: 5532 Train Loss: 0.52125 Validation Loss: 0.54830\n",
            "Epoch: 5533 Train Loss: 0.52542 Validation Loss: 0.54996\n",
            "Epoch: 5534 Train Loss: 0.52499 Validation Loss: 0.54859\n",
            "Epoch: 5535 Train Loss: 0.52427 Validation Loss: 0.54834\n",
            "Epoch: 5536 Train Loss: 0.52283 Validation Loss: 0.54876\n",
            "Epoch: 5537 Train Loss: 0.52480 Validation Loss: 0.54875\n",
            "Epoch: 5538 Train Loss: 0.52682 Validation Loss: 0.54850\n",
            "Epoch: 5539 Train Loss: 0.52284 Validation Loss: 0.54865\n",
            "Epoch: 5540 Train Loss: 0.52075 Validation Loss: 0.54847\n",
            "Epoch: 5541 Train Loss: 0.52410 Validation Loss: 0.54857\n",
            "Epoch: 5542 Train Loss: 0.52188 Validation Loss: 0.54858\n",
            "Epoch: 5543 Train Loss: 0.52288 Validation Loss: 0.54821\n",
            "Epoch: 5544 Train Loss: 0.52490 Validation Loss: 0.54865\n",
            "Epoch: 5545 Train Loss: 0.52278 Validation Loss: 0.54868\n",
            "Epoch: 5546 Train Loss: 0.52420 Validation Loss: 0.54925\n",
            "Epoch: 5547 Train Loss: 0.52307 Validation Loss: 0.54863\n",
            "Epoch: 5548 Train Loss: 0.52223 Validation Loss: 0.54866\n",
            "Epoch: 5549 Train Loss: 0.52344 Validation Loss: 0.54792\n",
            "Epoch: 5550 Train Loss: 0.52299 Validation Loss: 0.54822\n",
            "Epoch: 5551 Train Loss: 0.52096 Validation Loss: 0.54929\n",
            "Epoch: 5552 Train Loss: 0.52841 Validation Loss: 0.54928\n",
            "Epoch: 5553 Train Loss: 0.52448 Validation Loss: 0.54801\n",
            "Epoch: 5554 Train Loss: 0.52184 Validation Loss: 0.54860\n",
            "Epoch: 5555 Train Loss: 0.52307 Validation Loss: 0.54879\n",
            "Epoch: 5556 Train Loss: 0.52379 Validation Loss: 0.54873\n",
            "Epoch: 5557 Train Loss: 0.52691 Validation Loss: 0.54870\n",
            "Epoch: 5558 Train Loss: 0.52400 Validation Loss: 0.54873\n",
            "Epoch: 5559 Train Loss: 0.52317 Validation Loss: 0.54849\n",
            "Epoch: 5560 Train Loss: 0.52586 Validation Loss: 0.54897\n",
            "Epoch: 5561 Train Loss: 0.52194 Validation Loss: 0.54872\n",
            "Epoch: 5562 Train Loss: 0.52626 Validation Loss: 0.54824\n",
            "Epoch: 5563 Train Loss: 0.52393 Validation Loss: 0.54894\n",
            "Epoch: 5564 Train Loss: 0.52082 Validation Loss: 0.54862\n",
            "Epoch: 5565 Train Loss: 0.52478 Validation Loss: 0.54889\n",
            "Epoch: 5566 Train Loss: 0.52201 Validation Loss: 0.54827\n",
            "Epoch: 5567 Train Loss: 0.52074 Validation Loss: 0.54864\n",
            "Epoch: 5568 Train Loss: 0.52331 Validation Loss: 0.54853\n",
            "Epoch: 5569 Train Loss: 0.52299 Validation Loss: 0.54904\n",
            "Epoch: 5570 Train Loss: 0.52209 Validation Loss: 0.54924\n",
            "Epoch: 5571 Train Loss: 0.52433 Validation Loss: 0.54864\n",
            "Epoch: 5572 Train Loss: 0.52386 Validation Loss: 0.54911\n",
            "Epoch: 5573 Train Loss: 0.52379 Validation Loss: 0.54837\n",
            "Epoch: 5574 Train Loss: 0.52299 Validation Loss: 0.54830\n",
            "Epoch: 5575 Train Loss: 0.52599 Validation Loss: 0.54845\n",
            "Epoch: 5576 Train Loss: 0.52430 Validation Loss: 0.54945\n",
            "Epoch: 5577 Train Loss: 0.52189 Validation Loss: 0.54819\n",
            "Epoch: 5578 Train Loss: 0.52207 Validation Loss: 0.54880\n",
            "Epoch: 5579 Train Loss: 0.52202 Validation Loss: 0.54835\n",
            "Epoch: 5580 Train Loss: 0.52492 Validation Loss: 0.54893\n",
            "Epoch: 5581 Train Loss: 0.52085 Validation Loss: 0.54837\n",
            "Epoch: 5582 Train Loss: 0.52391 Validation Loss: 0.54851\n",
            "Epoch: 5583 Train Loss: 0.52088 Validation Loss: 0.54838\n",
            "Epoch: 5584 Train Loss: 0.52355 Validation Loss: 0.54903\n",
            "Epoch: 5585 Train Loss: 0.52305 Validation Loss: 0.54839\n",
            "Epoch: 5586 Train Loss: 0.52179 Validation Loss: 0.54876\n",
            "Epoch: 5587 Train Loss: 0.52200 Validation Loss: 0.54880\n",
            "Epoch: 5588 Train Loss: 0.52383 Validation Loss: 0.54828\n",
            "Epoch: 5589 Train Loss: 0.52274 Validation Loss: 0.54860\n",
            "Epoch: 5590 Train Loss: 0.52397 Validation Loss: 0.54897\n",
            "Epoch: 5591 Train Loss: 0.52498 Validation Loss: 0.54842\n",
            "Epoch: 5592 Train Loss: 0.52096 Validation Loss: 0.54823\n",
            "Epoch: 5593 Train Loss: 0.52382 Validation Loss: 0.54903\n",
            "Epoch: 5594 Train Loss: 0.52083 Validation Loss: 0.54864\n",
            "Epoch: 5595 Train Loss: 0.52189 Validation Loss: 0.54908\n",
            "Epoch: 5596 Train Loss: 0.52196 Validation Loss: 0.54871\n",
            "Epoch: 5597 Train Loss: 0.52276 Validation Loss: 0.54861\n",
            "Epoch: 5598 Train Loss: 0.52476 Validation Loss: 0.54842\n",
            "Epoch: 5599 Train Loss: 0.52078 Validation Loss: 0.54843\n",
            "Epoch: 5600 Train Loss: 0.52630 Validation Loss: 0.54932\n",
            "Epoch: 5601 Train Loss: 0.52397 Validation Loss: 0.54826\n",
            "Epoch: 5602 Train Loss: 0.52203 Validation Loss: 0.54881\n",
            "Epoch: 5603 Train Loss: 0.52204 Validation Loss: 0.54828\n",
            "Epoch: 5604 Train Loss: 0.52595 Validation Loss: 0.54867\n",
            "Epoch: 5605 Train Loss: 0.52403 Validation Loss: 0.54855\n",
            "Epoch: 5606 Train Loss: 0.52185 Validation Loss: 0.54866\n",
            "Epoch: 5607 Train Loss: 0.52201 Validation Loss: 0.54819\n",
            "Epoch: 5608 Train Loss: 0.52499 Validation Loss: 0.54844\n",
            "Epoch: 5609 Train Loss: 0.52387 Validation Loss: 0.54884\n",
            "Epoch: 5610 Train Loss: 0.51888 Validation Loss: 0.54847\n",
            "Epoch: 5611 Train Loss: 0.52264 Validation Loss: 0.54852\n",
            "Epoch: 5612 Train Loss: 0.52277 Validation Loss: 0.54900\n",
            "Epoch: 5613 Train Loss: 0.52291 Validation Loss: 0.54894\n",
            "Epoch: 5614 Train Loss: 0.52518 Validation Loss: 0.54876\n",
            "Epoch: 5615 Train Loss: 0.52074 Validation Loss: 0.54818\n",
            "Epoch: 5616 Train Loss: 0.52801 Validation Loss: 0.54959\n",
            "Epoch: 5617 Train Loss: 0.52189 Validation Loss: 0.54886\n",
            "Epoch: 5618 Train Loss: 0.52188 Validation Loss: 0.54835\n",
            "Epoch: 5619 Train Loss: 0.52204 Validation Loss: 0.54870\n",
            "Epoch: 5620 Train Loss: 0.52382 Validation Loss: 0.54852\n",
            "Epoch: 5621 Train Loss: 0.52096 Validation Loss: 0.54813\n",
            "Epoch: 5622 Train Loss: 0.51909 Validation Loss: 0.54834\n",
            "Epoch: 5623 Train Loss: 0.52426 Validation Loss: 0.54967\n",
            "Epoch: 5624 Train Loss: 0.52290 Validation Loss: 0.54845\n",
            "Epoch: 5625 Train Loss: 0.52078 Validation Loss: 0.54878\n",
            "Epoch: 5626 Train Loss: 0.52516 Validation Loss: 0.54951\n",
            "Epoch: 5627 Train Loss: 0.52390 Validation Loss: 0.54863\n",
            "Epoch: 5628 Train Loss: 0.52287 Validation Loss: 0.54807\n",
            "Epoch: 5629 Train Loss: 0.52093 Validation Loss: 0.54842\n",
            "Epoch: 5630 Train Loss: 0.52181 Validation Loss: 0.54902\n",
            "Epoch: 5631 Train Loss: 0.52287 Validation Loss: 0.54916\n",
            "Epoch: 5632 Train Loss: 0.52268 Validation Loss: 0.54863\n",
            "Epoch: 5633 Train Loss: 0.52654 Validation Loss: 0.54925\n",
            "Epoch: 5634 Train Loss: 0.52263 Validation Loss: 0.54811\n",
            "Epoch: 5635 Train Loss: 0.52000 Validation Loss: 0.54805\n",
            "Epoch: 5636 Train Loss: 0.52274 Validation Loss: 0.54909\n",
            "Epoch: 5637 Train Loss: 0.52354 Validation Loss: 0.54852\n",
            "Epoch: 5638 Train Loss: 0.52601 Validation Loss: 0.54874\n",
            "Epoch: 5639 Train Loss: 0.52379 Validation Loss: 0.54899\n",
            "Epoch: 5640 Train Loss: 0.52326 Validation Loss: 0.54832\n",
            "Epoch: 5641 Train Loss: 0.52483 Validation Loss: 0.54881\n",
            "Epoch: 5642 Train Loss: 0.51974 Validation Loss: 0.54866\n",
            "Epoch: 5643 Train Loss: 0.52487 Validation Loss: 0.54897\n",
            "Epoch: 5644 Train Loss: 0.52175 Validation Loss: 0.54883\n",
            "Epoch: 5645 Train Loss: 0.52483 Validation Loss: 0.54889\n",
            "Epoch: 5646 Train Loss: 0.52085 Validation Loss: 0.54799\n",
            "Epoch: 5647 Train Loss: 0.52523 Validation Loss: 0.54926\n",
            "Epoch: 5648 Train Loss: 0.52294 Validation Loss: 0.54872\n",
            "Epoch: 5649 Train Loss: 0.52133 Validation Loss: 0.54819\n",
            "Epoch: 5650 Train Loss: 0.52204 Validation Loss: 0.54883\n",
            "Epoch: 5651 Train Loss: 0.52288 Validation Loss: 0.54906\n",
            "Epoch: 5652 Train Loss: 0.52230 Validation Loss: 0.54849\n",
            "Epoch: 5653 Train Loss: 0.52178 Validation Loss: 0.54903\n",
            "Epoch: 5654 Train Loss: 0.52553 Validation Loss: 0.54948\n",
            "Epoch: 5655 Train Loss: 0.52668 Validation Loss: 0.54808\n",
            "Epoch: 5656 Train Loss: 0.52084 Validation Loss: 0.54845\n",
            "Epoch: 5657 Train Loss: 0.52379 Validation Loss: 0.54879\n",
            "Epoch: 5658 Train Loss: 0.52402 Validation Loss: 0.54926\n",
            "Epoch: 5659 Train Loss: 0.52506 Validation Loss: 0.54862\n",
            "Epoch: 5660 Train Loss: 0.52609 Validation Loss: 0.54825\n",
            "Epoch: 5661 Train Loss: 0.52295 Validation Loss: 0.54813\n",
            "Epoch: 5662 Train Loss: 0.52846 Validation Loss: 0.54945\n",
            "Epoch: 5663 Train Loss: 0.52198 Validation Loss: 0.54798\n",
            "Epoch: 5664 Train Loss: 0.52086 Validation Loss: 0.54818\n",
            "Epoch: 5665 Train Loss: 0.52518 Validation Loss: 0.54937\n",
            "Epoch: 5666 Train Loss: 0.52187 Validation Loss: 0.54898\n",
            "Epoch: 5667 Train Loss: 0.52293 Validation Loss: 0.54819\n",
            "Epoch: 5668 Train Loss: 0.52094 Validation Loss: 0.54839\n",
            "Epoch: 5669 Train Loss: 0.52587 Validation Loss: 0.54858\n",
            "Epoch: 5670 Train Loss: 0.52307 Validation Loss: 0.54856\n",
            "Epoch: 5671 Train Loss: 0.52379 Validation Loss: 0.54872\n",
            "Epoch: 5672 Train Loss: 0.52208 Validation Loss: 0.54821\n",
            "Epoch: 5673 Train Loss: 0.52088 Validation Loss: 0.54872\n",
            "Epoch: 5674 Train Loss: 0.52088 Validation Loss: 0.54853\n",
            "Epoch: 5675 Train Loss: 0.52386 Validation Loss: 0.54931\n",
            "Epoch: 5676 Train Loss: 0.52251 Validation Loss: 0.54846\n",
            "Epoch: 5677 Train Loss: 0.52458 Validation Loss: 0.54951\n",
            "Epoch: 5678 Train Loss: 0.52670 Validation Loss: 0.54872\n",
            "Epoch: 5679 Train Loss: 0.52400 Validation Loss: 0.54823\n",
            "Epoch: 5680 Train Loss: 0.52197 Validation Loss: 0.54814\n",
            "Epoch: 5681 Train Loss: 0.52170 Validation Loss: 0.54878\n",
            "Epoch: 5682 Train Loss: 0.52528 Validation Loss: 0.54899\n",
            "Epoch: 5683 Train Loss: 0.51984 Validation Loss: 0.54824\n",
            "Epoch: 5684 Train Loss: 0.52700 Validation Loss: 0.54911\n",
            "Epoch: 5685 Train Loss: 0.52449 Validation Loss: 0.54823\n",
            "Epoch: 5686 Train Loss: 0.52580 Validation Loss: 0.54895\n",
            "Epoch: 5687 Train Loss: 0.52100 Validation Loss: 0.54917\n",
            "Epoch: 5688 Train Loss: 0.52380 Validation Loss: 0.54864\n",
            "Epoch: 5689 Train Loss: 0.52395 Validation Loss: 0.54865\n",
            "Epoch: 5690 Train Loss: 0.52311 Validation Loss: 0.54799\n",
            "Epoch: 5691 Train Loss: 0.51992 Validation Loss: 0.54857\n",
            "Epoch: 5692 Train Loss: 0.52376 Validation Loss: 0.54908\n",
            "Epoch: 5693 Train Loss: 0.52380 Validation Loss: 0.54898\n",
            "Epoch: 5694 Train Loss: 0.52395 Validation Loss: 0.54867\n",
            "Epoch: 5695 Train Loss: 0.51986 Validation Loss: 0.54806\n",
            "Epoch: 5696 Train Loss: 0.52393 Validation Loss: 0.54876\n",
            "Epoch: 5697 Train Loss: 0.52306 Validation Loss: 0.54845\n",
            "Epoch: 5698 Train Loss: 0.52400 Validation Loss: 0.54911\n",
            "Epoch: 5699 Train Loss: 0.52174 Validation Loss: 0.54859\n",
            "Epoch: 5700 Train Loss: 0.52320 Validation Loss: 0.54825\n",
            "Epoch: 5701 Train Loss: 0.52203 Validation Loss: 0.54877\n",
            "Epoch: 5702 Train Loss: 0.52313 Validation Loss: 0.54970\n",
            "Epoch: 5703 Train Loss: 0.52283 Validation Loss: 0.54889\n",
            "Epoch: 5704 Train Loss: 0.52228 Validation Loss: 0.54832\n",
            "Epoch: 5705 Train Loss: 0.52181 Validation Loss: 0.54885\n",
            "Epoch: 5706 Train Loss: 0.52224 Validation Loss: 0.54830\n",
            "Epoch: 5707 Train Loss: 0.52135 Validation Loss: 0.54889\n",
            "Epoch: 5708 Train Loss: 0.52292 Validation Loss: 0.54866\n",
            "Epoch: 5709 Train Loss: 0.52187 Validation Loss: 0.54878\n",
            "Epoch: 5710 Train Loss: 0.52410 Validation Loss: 0.54914\n",
            "Epoch: 5711 Train Loss: 0.52292 Validation Loss: 0.54851\n",
            "Epoch: 5712 Train Loss: 0.52298 Validation Loss: 0.54881\n",
            "Epoch: 5713 Train Loss: 0.52302 Validation Loss: 0.54838\n",
            "Epoch: 5714 Train Loss: 0.52385 Validation Loss: 0.54915\n",
            "Epoch: 5715 Train Loss: 0.52305 Validation Loss: 0.54839\n",
            "Epoch: 5716 Train Loss: 0.52114 Validation Loss: 0.54808\n",
            "Epoch: 5717 Train Loss: 0.52098 Validation Loss: 0.54900\n",
            "Epoch: 5718 Train Loss: 0.52285 Validation Loss: 0.54867\n",
            "Epoch: 5719 Train Loss: 0.52189 Validation Loss: 0.54834\n",
            "Epoch: 5720 Train Loss: 0.52557 Validation Loss: 0.54846\n",
            "Epoch: 5721 Train Loss: 0.52211 Validation Loss: 0.54865\n",
            "Epoch: 5722 Train Loss: 0.52421 Validation Loss: 0.54914\n",
            "Epoch: 5723 Train Loss: 0.52091 Validation Loss: 0.54840\n",
            "Epoch: 5724 Train Loss: 0.52185 Validation Loss: 0.54883\n",
            "Epoch: 5725 Train Loss: 0.52507 Validation Loss: 0.54863\n",
            "Epoch: 5726 Train Loss: 0.52499 Validation Loss: 0.54889\n",
            "Epoch: 5727 Train Loss: 0.52096 Validation Loss: 0.54822\n",
            "Epoch: 5728 Train Loss: 0.52491 Validation Loss: 0.54857\n",
            "Epoch: 5729 Train Loss: 0.51991 Validation Loss: 0.54831\n",
            "Epoch: 5730 Train Loss: 0.52272 Validation Loss: 0.54907\n",
            "Epoch: 5731 Train Loss: 0.52281 Validation Loss: 0.54877\n",
            "Epoch: 5732 Train Loss: 0.52383 Validation Loss: 0.54861\n",
            "Epoch: 5733 Train Loss: 0.52075 Validation Loss: 0.54872\n",
            "Epoch: 5734 Train Loss: 0.52492 Validation Loss: 0.54877\n",
            "Epoch: 5735 Train Loss: 0.52111 Validation Loss: 0.54831\n",
            "Epoch: 5736 Train Loss: 0.51985 Validation Loss: 0.54914\n",
            "Epoch: 5737 Train Loss: 0.52399 Validation Loss: 0.54914\n",
            "Epoch: 5738 Train Loss: 0.52079 Validation Loss: 0.54890\n",
            "Epoch: 5739 Train Loss: 0.52384 Validation Loss: 0.54883\n",
            "Epoch: 5740 Train Loss: 0.52174 Validation Loss: 0.54821\n",
            "Epoch: 5741 Train Loss: 0.52393 Validation Loss: 0.54844\n",
            "Epoch: 5742 Train Loss: 0.52194 Validation Loss: 0.54834\n",
            "Epoch: 5743 Train Loss: 0.52497 Validation Loss: 0.54923\n",
            "Epoch: 5744 Train Loss: 0.52308 Validation Loss: 0.54930\n",
            "Epoch: 5745 Train Loss: 0.52419 Validation Loss: 0.54789\n",
            "Epoch: 5746 Train Loss: 0.52206 Validation Loss: 0.54904\n",
            "Epoch: 5747 Train Loss: 0.52488 Validation Loss: 0.54885\n",
            "Epoch: 5748 Train Loss: 0.52276 Validation Loss: 0.54801\n",
            "Epoch: 5749 Train Loss: 0.52381 Validation Loss: 0.54899\n",
            "Epoch: 5750 Train Loss: 0.52600 Validation Loss: 0.54878\n",
            "Epoch: 5751 Train Loss: 0.52002 Validation Loss: 0.54818\n",
            "Epoch: 5752 Train Loss: 0.52297 Validation Loss: 0.54884\n",
            "Epoch: 5753 Train Loss: 0.52184 Validation Loss: 0.54881\n",
            "Epoch: 5754 Train Loss: 0.52600 Validation Loss: 0.54917\n",
            "Epoch: 5755 Train Loss: 0.52176 Validation Loss: 0.54779\n",
            "Epoch: 5756 Train Loss: 0.52113 Validation Loss: 0.54849\n",
            "Epoch: 5757 Train Loss: 0.52589 Validation Loss: 0.54929\n",
            "Epoch: 5758 Train Loss: 0.52435 Validation Loss: 0.54951\n",
            "Epoch: 5759 Train Loss: 0.52210 Validation Loss: 0.54787\n",
            "Epoch: 5760 Train Loss: 0.52226 Validation Loss: 0.54827\n",
            "Epoch: 5761 Train Loss: 0.52286 Validation Loss: 0.54892\n",
            "Epoch: 5762 Train Loss: 0.52286 Validation Loss: 0.54870\n",
            "Epoch: 5763 Train Loss: 0.52196 Validation Loss: 0.54864\n",
            "Epoch: 5764 Train Loss: 0.52343 Validation Loss: 0.54827\n",
            "Epoch: 5765 Train Loss: 0.52310 Validation Loss: 0.54845\n",
            "Epoch: 5766 Train Loss: 0.52168 Validation Loss: 0.54885\n",
            "Epoch: 5767 Train Loss: 0.52185 Validation Loss: 0.54903\n",
            "Epoch: 5768 Train Loss: 0.52099 Validation Loss: 0.54856\n",
            "Epoch: 5769 Train Loss: 0.52291 Validation Loss: 0.54925\n",
            "Epoch: 5770 Train Loss: 0.52144 Validation Loss: 0.54856\n",
            "Epoch: 5771 Train Loss: 0.52338 Validation Loss: 0.54892\n",
            "Epoch: 5772 Train Loss: 0.52297 Validation Loss: 0.54904\n",
            "Epoch: 5773 Train Loss: 0.52088 Validation Loss: 0.54851\n",
            "Epoch: 5774 Train Loss: 0.52205 Validation Loss: 0.54825\n",
            "Epoch: 5775 Train Loss: 0.51996 Validation Loss: 0.54838\n",
            "Epoch: 5776 Train Loss: 0.52501 Validation Loss: 0.54968\n",
            "Epoch: 5777 Train Loss: 0.52589 Validation Loss: 0.54872\n",
            "Epoch: 5778 Train Loss: 0.52375 Validation Loss: 0.54836\n",
            "Epoch: 5779 Train Loss: 0.52274 Validation Loss: 0.54852\n",
            "Epoch: 5780 Train Loss: 0.52207 Validation Loss: 0.54876\n",
            "Epoch: 5781 Train Loss: 0.52298 Validation Loss: 0.54870\n",
            "Epoch: 5782 Train Loss: 0.52341 Validation Loss: 0.54870\n",
            "Epoch: 5783 Train Loss: 0.52428 Validation Loss: 0.54789\n",
            "Epoch: 5784 Train Loss: 0.52313 Validation Loss: 0.54902\n",
            "Epoch: 5785 Train Loss: 0.52201 Validation Loss: 0.54842\n",
            "Epoch: 5786 Train Loss: 0.51995 Validation Loss: 0.54836\n",
            "Epoch: 5787 Train Loss: 0.52168 Validation Loss: 0.54918\n",
            "Epoch: 5788 Train Loss: 0.52397 Validation Loss: 0.54910\n",
            "Epoch: 5789 Train Loss: 0.52494 Validation Loss: 0.54844\n",
            "Epoch: 5790 Train Loss: 0.52186 Validation Loss: 0.54847\n",
            "Epoch: 5791 Train Loss: 0.52423 Validation Loss: 0.54895\n",
            "Epoch: 5792 Train Loss: 0.52084 Validation Loss: 0.54789\n",
            "Epoch: 5793 Train Loss: 0.52341 Validation Loss: 0.54983\n",
            "Epoch: 5794 Train Loss: 0.52274 Validation Loss: 0.54866\n",
            "Epoch: 5795 Train Loss: 0.52098 Validation Loss: 0.54839\n",
            "Epoch: 5796 Train Loss: 0.52504 Validation Loss: 0.54902\n",
            "Epoch: 5797 Train Loss: 0.52489 Validation Loss: 0.54956\n",
            "Epoch: 5798 Train Loss: 0.52481 Validation Loss: 0.54797\n",
            "Epoch: 5799 Train Loss: 0.52420 Validation Loss: 0.54879\n",
            "Epoch: 5800 Train Loss: 0.52628 Validation Loss: 0.54854\n",
            "Epoch: 5801 Train Loss: 0.52287 Validation Loss: 0.54827\n",
            "Epoch: 5802 Train Loss: 0.52296 Validation Loss: 0.54861\n",
            "Epoch: 5803 Train Loss: 0.52503 Validation Loss: 0.54900\n",
            "Epoch: 5804 Train Loss: 0.52570 Validation Loss: 0.54855\n",
            "Epoch: 5805 Train Loss: 0.52136 Validation Loss: 0.54788\n",
            "Epoch: 5806 Train Loss: 0.52355 Validation Loss: 0.54884\n",
            "Epoch: 5807 Train Loss: 0.52279 Validation Loss: 0.54911\n",
            "Epoch: 5808 Train Loss: 0.52323 Validation Loss: 0.54923\n",
            "Epoch: 5809 Train Loss: 0.52306 Validation Loss: 0.54861\n",
            "Epoch: 5810 Train Loss: 0.52410 Validation Loss: 0.54847\n",
            "Epoch: 5811 Train Loss: 0.52215 Validation Loss: 0.54835\n",
            "Epoch: 5812 Train Loss: 0.52123 Validation Loss: 0.54837\n",
            "Epoch: 5813 Train Loss: 0.52475 Validation Loss: 0.54955\n",
            "Epoch: 5814 Train Loss: 0.52404 Validation Loss: 0.54945\n",
            "Epoch: 5815 Train Loss: 0.52374 Validation Loss: 0.54856\n",
            "Epoch: 5816 Train Loss: 0.52296 Validation Loss: 0.54794\n",
            "Epoch: 5817 Train Loss: 0.52288 Validation Loss: 0.54824\n",
            "Epoch: 5818 Train Loss: 0.52200 Validation Loss: 0.54893\n",
            "Epoch: 5819 Train Loss: 0.52383 Validation Loss: 0.54879\n",
            "Epoch: 5820 Train Loss: 0.52183 Validation Loss: 0.54859\n",
            "Epoch: 5821 Train Loss: 0.52701 Validation Loss: 0.54925\n",
            "Epoch: 5822 Train Loss: 0.52465 Validation Loss: 0.54809\n",
            "Epoch: 5823 Train Loss: 0.52374 Validation Loss: 0.54866\n",
            "Epoch: 5824 Train Loss: 0.52208 Validation Loss: 0.54839\n",
            "Epoch: 5825 Train Loss: 0.52096 Validation Loss: 0.54868\n",
            "Epoch: 5826 Train Loss: 0.52807 Validation Loss: 0.54938\n",
            "Epoch: 5827 Train Loss: 0.52521 Validation Loss: 0.54866\n",
            "Epoch: 5828 Train Loss: 0.52475 Validation Loss: 0.54806\n",
            "Epoch: 5829 Train Loss: 0.52699 Validation Loss: 0.54813\n",
            "Epoch: 5830 Train Loss: 0.52103 Validation Loss: 0.54818\n",
            "Epoch: 5831 Train Loss: 0.52286 Validation Loss: 0.54887\n",
            "Epoch: 5832 Train Loss: 0.51980 Validation Loss: 0.54866\n",
            "Epoch: 5833 Train Loss: 0.52703 Validation Loss: 0.54915\n",
            "Epoch: 5834 Train Loss: 0.52358 Validation Loss: 0.54828\n",
            "Epoch: 5835 Train Loss: 0.52184 Validation Loss: 0.54911\n",
            "Epoch: 5836 Train Loss: 0.52493 Validation Loss: 0.54854\n",
            "Epoch: 5837 Train Loss: 0.52276 Validation Loss: 0.54894\n",
            "Epoch: 5838 Train Loss: 0.52433 Validation Loss: 0.54824\n",
            "Epoch: 5839 Train Loss: 0.52385 Validation Loss: 0.54928\n",
            "Epoch: 5840 Train Loss: 0.52414 Validation Loss: 0.54842\n",
            "Epoch: 5841 Train Loss: 0.52174 Validation Loss: 0.54898\n",
            "Epoch: 5842 Train Loss: 0.52338 Validation Loss: 0.54815\n",
            "Epoch: 5843 Train Loss: 0.51974 Validation Loss: 0.54910\n",
            "Epoch: 5844 Train Loss: 0.52193 Validation Loss: 0.54900\n",
            "Epoch: 5845 Train Loss: 0.52666 Validation Loss: 0.54973\n",
            "Epoch: 5846 Train Loss: 0.52041 Validation Loss: 0.54785\n",
            "Epoch: 5847 Train Loss: 0.52378 Validation Loss: 0.54849\n",
            "Epoch: 5848 Train Loss: 0.52489 Validation Loss: 0.54887\n",
            "Epoch: 5849 Train Loss: 0.52612 Validation Loss: 0.54906\n",
            "Epoch: 5850 Train Loss: 0.52270 Validation Loss: 0.54834\n",
            "Epoch: 5851 Train Loss: 0.52317 Validation Loss: 0.54851\n",
            "Epoch: 5852 Train Loss: 0.51994 Validation Loss: 0.54817\n",
            "Epoch: 5853 Train Loss: 0.52320 Validation Loss: 0.54904\n",
            "Epoch: 5854 Train Loss: 0.52376 Validation Loss: 0.54851\n",
            "Epoch: 5855 Train Loss: 0.52379 Validation Loss: 0.54876\n",
            "Epoch: 5856 Train Loss: 0.52286 Validation Loss: 0.54827\n",
            "Epoch: 5857 Train Loss: 0.52608 Validation Loss: 0.54917\n",
            "Epoch: 5858 Train Loss: 0.52409 Validation Loss: 0.54899\n",
            "Epoch: 5859 Train Loss: 0.52492 Validation Loss: 0.54803\n",
            "Epoch: 5860 Train Loss: 0.52181 Validation Loss: 0.54823\n",
            "Epoch: 5861 Train Loss: 0.52081 Validation Loss: 0.54866\n",
            "Epoch: 5862 Train Loss: 0.51920 Validation Loss: 0.54832\n",
            "Epoch: 5863 Train Loss: 0.52163 Validation Loss: 0.54943\n",
            "Epoch: 5864 Train Loss: 0.52361 Validation Loss: 0.54864\n",
            "Epoch: 5865 Train Loss: 0.52101 Validation Loss: 0.54888\n",
            "Epoch: 5866 Train Loss: 0.52243 Validation Loss: 0.54923\n",
            "Epoch: 5867 Train Loss: 0.52603 Validation Loss: 0.54900\n",
            "Epoch: 5868 Train Loss: 0.52398 Validation Loss: 0.54845\n",
            "Epoch: 5869 Train Loss: 0.52400 Validation Loss: 0.54837\n",
            "Epoch: 5870 Train Loss: 0.52824 Validation Loss: 0.54952\n",
            "Epoch: 5871 Train Loss: 0.52313 Validation Loss: 0.54826\n",
            "Epoch: 5872 Train Loss: 0.52611 Validation Loss: 0.54903\n",
            "Epoch: 5873 Train Loss: 0.52271 Validation Loss: 0.54831\n",
            "Epoch: 5874 Train Loss: 0.52094 Validation Loss: 0.54844\n",
            "Epoch: 5875 Train Loss: 0.52019 Validation Loss: 0.54878\n",
            "Epoch: 5876 Train Loss: 0.52393 Validation Loss: 0.54861\n",
            "Epoch: 5877 Train Loss: 0.52385 Validation Loss: 0.54882\n",
            "Epoch: 5878 Train Loss: 0.52385 Validation Loss: 0.54848\n",
            "Epoch: 5879 Train Loss: 0.52381 Validation Loss: 0.54818\n",
            "Epoch: 5880 Train Loss: 0.52200 Validation Loss: 0.54875\n",
            "Epoch: 5881 Train Loss: 0.52407 Validation Loss: 0.54909\n",
            "Epoch: 5882 Train Loss: 0.52565 Validation Loss: 0.54819\n",
            "Epoch: 5883 Train Loss: 0.52504 Validation Loss: 0.54854\n",
            "Epoch: 5884 Train Loss: 0.52102 Validation Loss: 0.54822\n",
            "Epoch: 5885 Train Loss: 0.52710 Validation Loss: 0.54894\n",
            "Epoch: 5886 Train Loss: 0.52578 Validation Loss: 0.54851\n",
            "Epoch: 5887 Train Loss: 0.52380 Validation Loss: 0.54844\n",
            "Epoch: 5888 Train Loss: 0.52630 Validation Loss: 0.54862\n",
            "Epoch: 5889 Train Loss: 0.52680 Validation Loss: 0.54851\n",
            "Epoch: 5890 Train Loss: 0.52283 Validation Loss: 0.54850\n",
            "Epoch: 5891 Train Loss: 0.52391 Validation Loss: 0.54900\n",
            "Epoch: 5892 Train Loss: 0.52197 Validation Loss: 0.54828\n",
            "Epoch: 5893 Train Loss: 0.52095 Validation Loss: 0.54833\n",
            "Epoch: 5894 Train Loss: 0.52082 Validation Loss: 0.54898\n",
            "Epoch: 5895 Train Loss: 0.52380 Validation Loss: 0.54905\n",
            "Epoch: 5896 Train Loss: 0.52175 Validation Loss: 0.54855\n",
            "Epoch: 5897 Train Loss: 0.52292 Validation Loss: 0.54859\n",
            "Epoch: 5898 Train Loss: 0.52094 Validation Loss: 0.54872\n",
            "Epoch: 5899 Train Loss: 0.52114 Validation Loss: 0.54905\n",
            "Epoch: 5900 Train Loss: 0.52270 Validation Loss: 0.54858\n",
            "Epoch: 5901 Train Loss: 0.52227 Validation Loss: 0.54800\n",
            "Epoch: 5902 Train Loss: 0.52083 Validation Loss: 0.54924\n",
            "Epoch: 5903 Train Loss: 0.52252 Validation Loss: 0.54864\n",
            "Epoch: 5904 Train Loss: 0.52198 Validation Loss: 0.54861\n",
            "Epoch: 5905 Train Loss: 0.52203 Validation Loss: 0.54874\n",
            "Epoch: 5906 Train Loss: 0.51980 Validation Loss: 0.54879\n",
            "Epoch: 5907 Train Loss: 0.52335 Validation Loss: 0.54848\n",
            "Epoch: 5908 Train Loss: 0.52539 Validation Loss: 0.54885\n",
            "Epoch: 5909 Train Loss: 0.52595 Validation Loss: 0.54854\n",
            "Epoch: 5910 Train Loss: 0.52192 Validation Loss: 0.54838\n",
            "Epoch: 5911 Train Loss: 0.52295 Validation Loss: 0.54907\n",
            "Epoch: 5912 Train Loss: 0.52102 Validation Loss: 0.54854\n",
            "Epoch: 5913 Train Loss: 0.52218 Validation Loss: 0.54874\n",
            "Epoch: 5914 Train Loss: 0.52395 Validation Loss: 0.54897\n",
            "Epoch: 5915 Train Loss: 0.52191 Validation Loss: 0.54833\n",
            "Epoch: 5916 Train Loss: 0.52173 Validation Loss: 0.54848\n",
            "Epoch: 5917 Train Loss: 0.52208 Validation Loss: 0.54850\n",
            "Epoch: 5918 Train Loss: 0.51982 Validation Loss: 0.54905\n",
            "Epoch: 5919 Train Loss: 0.51989 Validation Loss: 0.54869\n",
            "Epoch: 5920 Train Loss: 0.52172 Validation Loss: 0.54916\n",
            "Epoch: 5921 Train Loss: 0.52287 Validation Loss: 0.54904\n",
            "Epoch: 5922 Train Loss: 0.52307 Validation Loss: 0.54901\n",
            "Epoch: 5923 Train Loss: 0.52804 Validation Loss: 0.54871\n",
            "Epoch: 5924 Train Loss: 0.52394 Validation Loss: 0.54828\n",
            "Epoch: 5925 Train Loss: 0.52284 Validation Loss: 0.54863\n",
            "Epoch: 5926 Train Loss: 0.52775 Validation Loss: 0.54913\n",
            "Epoch: 5927 Train Loss: 0.52302 Validation Loss: 0.54779\n",
            "Epoch: 5928 Train Loss: 0.52486 Validation Loss: 0.54863\n",
            "Epoch: 5929 Train Loss: 0.51983 Validation Loss: 0.54849\n",
            "Epoch: 5930 Train Loss: 0.52187 Validation Loss: 0.54903\n",
            "Epoch: 5931 Train Loss: 0.52392 Validation Loss: 0.54897\n",
            "Epoch: 5932 Train Loss: 0.52473 Validation Loss: 0.54839\n",
            "Epoch: 5933 Train Loss: 0.52499 Validation Loss: 0.54811\n",
            "Epoch: 5934 Train Loss: 0.52194 Validation Loss: 0.54866\n",
            "Epoch: 5935 Train Loss: 0.52081 Validation Loss: 0.54870\n",
            "Epoch: 5936 Train Loss: 0.52619 Validation Loss: 0.54952\n",
            "Epoch: 5937 Train Loss: 0.52494 Validation Loss: 0.54855\n",
            "Epoch: 5938 Train Loss: 0.52329 Validation Loss: 0.54858\n",
            "Epoch: 5939 Train Loss: 0.52330 Validation Loss: 0.54834\n",
            "Epoch: 5940 Train Loss: 0.52285 Validation Loss: 0.54877\n",
            "Epoch: 5941 Train Loss: 0.52398 Validation Loss: 0.54849\n",
            "Epoch: 5942 Train Loss: 0.52344 Validation Loss: 0.54840\n",
            "Epoch: 5943 Train Loss: 0.52691 Validation Loss: 0.54909\n",
            "Epoch: 5944 Train Loss: 0.52588 Validation Loss: 0.54881\n",
            "Epoch: 5945 Train Loss: 0.52484 Validation Loss: 0.54840\n",
            "Epoch: 5946 Train Loss: 0.52497 Validation Loss: 0.54832\n",
            "Epoch: 5947 Train Loss: 0.52397 Validation Loss: 0.54819\n",
            "Epoch: 5948 Train Loss: 0.52108 Validation Loss: 0.54862\n",
            "Epoch: 5949 Train Loss: 0.52481 Validation Loss: 0.54878\n",
            "Epoch: 5950 Train Loss: 0.52116 Validation Loss: 0.54808\n",
            "Epoch: 5951 Train Loss: 0.52390 Validation Loss: 0.54912\n",
            "Epoch: 5952 Train Loss: 0.52224 Validation Loss: 0.54866\n",
            "Epoch: 5953 Train Loss: 0.52584 Validation Loss: 0.54902\n",
            "Epoch: 5954 Train Loss: 0.52390 Validation Loss: 0.54841\n",
            "Epoch: 5955 Train Loss: 0.52295 Validation Loss: 0.54853\n",
            "Epoch: 5956 Train Loss: 0.52283 Validation Loss: 0.54860\n",
            "Epoch: 5957 Train Loss: 0.52402 Validation Loss: 0.54851\n",
            "Epoch: 5958 Train Loss: 0.52345 Validation Loss: 0.54824\n",
            "Epoch: 5959 Train Loss: 0.52406 Validation Loss: 0.54920\n",
            "Epoch: 5960 Train Loss: 0.52391 Validation Loss: 0.54907\n",
            "Epoch: 5961 Train Loss: 0.52472 Validation Loss: 0.54832\n",
            "Epoch: 5962 Train Loss: 0.52302 Validation Loss: 0.54853\n",
            "Epoch: 5963 Train Loss: 0.52199 Validation Loss: 0.54824\n",
            "Epoch: 5964 Train Loss: 0.52190 Validation Loss: 0.54916\n",
            "Epoch: 5965 Train Loss: 0.52403 Validation Loss: 0.54894\n",
            "Epoch: 5966 Train Loss: 0.52096 Validation Loss: 0.54824\n",
            "Epoch: 5967 Train Loss: 0.52387 Validation Loss: 0.54915\n",
            "Epoch: 5968 Train Loss: 0.52905 Validation Loss: 0.54925\n",
            "Epoch: 5969 Train Loss: 0.52690 Validation Loss: 0.54840\n",
            "Epoch: 5970 Train Loss: 0.52225 Validation Loss: 0.54787\n",
            "Epoch: 5971 Train Loss: 0.52259 Validation Loss: 0.54879\n",
            "Epoch: 5972 Train Loss: 0.52302 Validation Loss: 0.54885\n",
            "Epoch: 5973 Train Loss: 0.52086 Validation Loss: 0.54877\n",
            "Epoch: 5974 Train Loss: 0.52210 Validation Loss: 0.54923\n",
            "Epoch: 5975 Train Loss: 0.52429 Validation Loss: 0.54934\n",
            "Epoch: 5976 Train Loss: 0.52389 Validation Loss: 0.54870\n",
            "Epoch: 5977 Train Loss: 0.52581 Validation Loss: 0.54818\n",
            "Epoch: 5978 Train Loss: 0.52386 Validation Loss: 0.54828\n",
            "Epoch: 5979 Train Loss: 0.52729 Validation Loss: 0.54893\n",
            "Epoch: 5980 Train Loss: 0.52309 Validation Loss: 0.54810\n",
            "Epoch: 5981 Train Loss: 0.52086 Validation Loss: 0.54854\n",
            "Epoch: 5982 Train Loss: 0.52629 Validation Loss: 0.54967\n",
            "Epoch: 5983 Train Loss: 0.52280 Validation Loss: 0.54868\n",
            "Epoch: 5984 Train Loss: 0.51901 Validation Loss: 0.54816\n",
            "Epoch: 5985 Train Loss: 0.52281 Validation Loss: 0.54920\n",
            "Epoch: 5986 Train Loss: 0.52591 Validation Loss: 0.54861\n",
            "Epoch: 5987 Train Loss: 0.52306 Validation Loss: 0.54884\n",
            "Epoch: 5988 Train Loss: 0.52373 Validation Loss: 0.54858\n",
            "Epoch: 5989 Train Loss: 0.52095 Validation Loss: 0.54818\n",
            "Epoch: 5990 Train Loss: 0.52713 Validation Loss: 0.54876\n",
            "Epoch: 5991 Train Loss: 0.52288 Validation Loss: 0.54868\n",
            "Epoch: 5992 Train Loss: 0.52582 Validation Loss: 0.54822\n",
            "Epoch: 5993 Train Loss: 0.52499 Validation Loss: 0.54857\n",
            "Epoch: 5994 Train Loss: 0.52500 Validation Loss: 0.54833\n",
            "Epoch: 5995 Train Loss: 0.52654 Validation Loss: 0.54915\n",
            "Epoch: 5996 Train Loss: 0.52623 Validation Loss: 0.54794\n",
            "Epoch: 5997 Train Loss: 0.52569 Validation Loss: 0.54904\n",
            "Epoch: 5998 Train Loss: 0.52425 Validation Loss: 0.54809\n",
            "Epoch: 5999 Train Loss: 0.52392 Validation Loss: 0.54876\n",
            "Epoch: 6000 Train Loss: 0.52401 Validation Loss: 0.54824\n",
            "Epoch: 6001 Train Loss: 0.52639 Validation Loss: 0.54939\n",
            "Epoch: 6002 Train Loss: 0.52089 Validation Loss: 0.54825\n",
            "Epoch: 6003 Train Loss: 0.52479 Validation Loss: 0.54859\n",
            "Epoch: 6004 Train Loss: 0.52687 Validation Loss: 0.54888\n",
            "Epoch: 6005 Train Loss: 0.52270 Validation Loss: 0.54833\n",
            "Epoch: 6006 Train Loss: 0.52384 Validation Loss: 0.54850\n",
            "Epoch: 6007 Train Loss: 0.52287 Validation Loss: 0.54851\n",
            "Epoch: 6008 Train Loss: 0.52613 Validation Loss: 0.54895\n",
            "Epoch: 6009 Train Loss: 0.51931 Validation Loss: 0.54810\n",
            "Epoch: 6010 Train Loss: 0.52441 Validation Loss: 0.54938\n",
            "Epoch: 6011 Train Loss: 0.52603 Validation Loss: 0.54876\n",
            "Epoch: 6012 Train Loss: 0.52195 Validation Loss: 0.54863\n",
            "Epoch: 6013 Train Loss: 0.52680 Validation Loss: 0.54864\n",
            "Epoch: 6014 Train Loss: 0.52186 Validation Loss: 0.54808\n",
            "Epoch: 6015 Train Loss: 0.52477 Validation Loss: 0.54877\n",
            "Epoch: 6016 Train Loss: 0.52496 Validation Loss: 0.54881\n",
            "Epoch: 6017 Train Loss: 0.52236 Validation Loss: 0.54878\n",
            "Epoch: 6018 Train Loss: 0.52493 Validation Loss: 0.54852\n",
            "Epoch: 6019 Train Loss: 0.52377 Validation Loss: 0.54839\n",
            "Epoch: 6020 Train Loss: 0.52307 Validation Loss: 0.54852\n",
            "Epoch: 6021 Train Loss: 0.52180 Validation Loss: 0.54886\n",
            "Epoch: 6022 Train Loss: 0.52279 Validation Loss: 0.54882\n",
            "Epoch: 6023 Train Loss: 0.52094 Validation Loss: 0.54849\n",
            "Epoch: 6024 Train Loss: 0.51902 Validation Loss: 0.54864\n",
            "Epoch: 6025 Train Loss: 0.52540 Validation Loss: 0.55019\n",
            "Epoch: 6026 Train Loss: 0.52287 Validation Loss: 0.54842\n",
            "Epoch: 6027 Train Loss: 0.52193 Validation Loss: 0.54851\n",
            "Epoch: 6028 Train Loss: 0.52297 Validation Loss: 0.54921\n",
            "Epoch: 6029 Train Loss: 0.52239 Validation Loss: 0.54833\n",
            "Epoch: 6030 Train Loss: 0.52661 Validation Loss: 0.54915\n",
            "Epoch: 6031 Train Loss: 0.52482 Validation Loss: 0.54861\n",
            "Epoch: 6032 Train Loss: 0.52424 Validation Loss: 0.54841\n",
            "Epoch: 6033 Train Loss: 0.52680 Validation Loss: 0.54856\n",
            "Epoch: 6034 Train Loss: 0.52444 Validation Loss: 0.54815\n",
            "Epoch: 6035 Train Loss: 0.52517 Validation Loss: 0.54910\n",
            "Epoch: 6036 Train Loss: 0.52378 Validation Loss: 0.54872\n",
            "Epoch: 6037 Train Loss: 0.52502 Validation Loss: 0.54841\n",
            "Epoch: 6038 Train Loss: 0.52475 Validation Loss: 0.54859\n",
            "Epoch: 6039 Train Loss: 0.52607 Validation Loss: 0.54896\n",
            "Epoch: 6040 Train Loss: 0.52275 Validation Loss: 0.54811\n",
            "Epoch: 6041 Train Loss: 0.52081 Validation Loss: 0.54838\n",
            "Epoch: 6042 Train Loss: 0.52295 Validation Loss: 0.54838\n",
            "Epoch: 6043 Train Loss: 0.52329 Validation Loss: 0.54898\n",
            "Epoch: 6044 Train Loss: 0.52407 Validation Loss: 0.54908\n",
            "Epoch: 6045 Train Loss: 0.52034 Validation Loss: 0.54781\n",
            "Epoch: 6046 Train Loss: 0.52312 Validation Loss: 0.54931\n",
            "Epoch: 6047 Train Loss: 0.52391 Validation Loss: 0.54880\n",
            "Epoch: 6048 Train Loss: 0.51978 Validation Loss: 0.54847\n",
            "Epoch: 6049 Train Loss: 0.52499 Validation Loss: 0.54856\n",
            "Epoch: 6050 Train Loss: 0.52539 Validation Loss: 0.54970\n",
            "Epoch: 6051 Train Loss: 0.52168 Validation Loss: 0.54844\n",
            "Epoch: 6052 Train Loss: 0.52199 Validation Loss: 0.54824\n",
            "Epoch: 6053 Train Loss: 0.52480 Validation Loss: 0.54868\n",
            "Epoch: 6054 Train Loss: 0.52378 Validation Loss: 0.54866\n",
            "Epoch: 6055 Train Loss: 0.52043 Validation Loss: 0.54799\n",
            "Epoch: 6056 Train Loss: 0.52194 Validation Loss: 0.54914\n",
            "Epoch: 6057 Train Loss: 0.52441 Validation Loss: 0.54925\n",
            "Epoch: 6058 Train Loss: 0.51993 Validation Loss: 0.54823\n",
            "Epoch: 6059 Train Loss: 0.52300 Validation Loss: 0.54830\n",
            "Epoch: 6060 Train Loss: 0.52391 Validation Loss: 0.54920\n",
            "Epoch: 6061 Train Loss: 0.52380 Validation Loss: 0.54900\n",
            "Epoch: 6062 Train Loss: 0.51988 Validation Loss: 0.54850\n",
            "Epoch: 6063 Train Loss: 0.52082 Validation Loss: 0.54864\n",
            "Epoch: 6064 Train Loss: 0.51980 Validation Loss: 0.54866\n",
            "Epoch: 6065 Train Loss: 0.52176 Validation Loss: 0.54896\n",
            "Epoch: 6066 Train Loss: 0.52384 Validation Loss: 0.54882\n",
            "Epoch: 6067 Train Loss: 0.52511 Validation Loss: 0.54952\n",
            "Epoch: 6068 Train Loss: 0.52289 Validation Loss: 0.54843\n",
            "Epoch: 6069 Train Loss: 0.52500 Validation Loss: 0.54820\n",
            "Epoch: 6070 Train Loss: 0.52209 Validation Loss: 0.54840\n",
            "Epoch: 6071 Train Loss: 0.52502 Validation Loss: 0.54939\n",
            "Epoch: 6072 Train Loss: 0.52147 Validation Loss: 0.54790\n",
            "Epoch: 6073 Train Loss: 0.52496 Validation Loss: 0.54912\n",
            "Epoch: 6074 Train Loss: 0.52398 Validation Loss: 0.54831\n",
            "Epoch: 6075 Train Loss: 0.52300 Validation Loss: 0.54887\n",
            "Epoch: 6076 Train Loss: 0.52218 Validation Loss: 0.54898\n",
            "Epoch: 6077 Train Loss: 0.52399 Validation Loss: 0.54830\n",
            "Epoch: 6078 Train Loss: 0.52598 Validation Loss: 0.54877\n",
            "Epoch: 6079 Train Loss: 0.52495 Validation Loss: 0.54866\n",
            "Epoch: 6080 Train Loss: 0.52281 Validation Loss: 0.54840\n",
            "Epoch: 6081 Train Loss: 0.52716 Validation Loss: 0.54861\n",
            "Epoch: 6082 Train Loss: 0.52298 Validation Loss: 0.54865\n",
            "Epoch: 6083 Train Loss: 0.52486 Validation Loss: 0.54889\n",
            "Epoch: 6084 Train Loss: 0.52312 Validation Loss: 0.54903\n",
            "Epoch: 6085 Train Loss: 0.52100 Validation Loss: 0.54830\n",
            "Epoch: 6086 Train Loss: 0.52496 Validation Loss: 0.54903\n",
            "Epoch: 6087 Train Loss: 0.52394 Validation Loss: 0.54848\n",
            "Epoch: 6088 Train Loss: 0.52544 Validation Loss: 0.54834\n",
            "Epoch: 6089 Train Loss: 0.52357 Validation Loss: 0.54960\n",
            "Epoch: 6090 Train Loss: 0.52362 Validation Loss: 0.54830\n",
            "Epoch: 6091 Train Loss: 0.52392 Validation Loss: 0.54829\n",
            "Epoch: 6092 Train Loss: 0.52094 Validation Loss: 0.54829\n",
            "Epoch: 6093 Train Loss: 0.52290 Validation Loss: 0.54848\n",
            "Epoch: 6094 Train Loss: 0.52174 Validation Loss: 0.54863\n",
            "Epoch: 6095 Train Loss: 0.52534 Validation Loss: 0.54905\n",
            "Epoch: 6096 Train Loss: 0.52820 Validation Loss: 0.54851\n",
            "Epoch: 6097 Train Loss: 0.52391 Validation Loss: 0.54871\n",
            "Epoch: 6098 Train Loss: 0.52529 Validation Loss: 0.54826\n",
            "Epoch: 6099 Train Loss: 0.52182 Validation Loss: 0.54842\n",
            "Epoch: 6100 Train Loss: 0.52484 Validation Loss: 0.54863\n",
            "Epoch: 6101 Train Loss: 0.52205 Validation Loss: 0.54942\n",
            "Epoch: 6102 Train Loss: 0.52181 Validation Loss: 0.54834\n",
            "Epoch: 6103 Train Loss: 0.52300 Validation Loss: 0.54861\n",
            "Epoch: 6104 Train Loss: 0.52497 Validation Loss: 0.54899\n",
            "Epoch: 6105 Train Loss: 0.52371 Validation Loss: 0.54848\n",
            "Epoch: 6106 Train Loss: 0.52424 Validation Loss: 0.54858\n",
            "Epoch: 6107 Train Loss: 0.52179 Validation Loss: 0.54818\n",
            "Epoch: 6108 Train Loss: 0.52336 Validation Loss: 0.54859\n",
            "Epoch: 6109 Train Loss: 0.52285 Validation Loss: 0.54830\n",
            "Epoch: 6110 Train Loss: 0.52492 Validation Loss: 0.54874\n",
            "Epoch: 6111 Train Loss: 0.52603 Validation Loss: 0.54864\n",
            "Epoch: 6112 Train Loss: 0.52413 Validation Loss: 0.54806\n",
            "Epoch: 6113 Train Loss: 0.52619 Validation Loss: 0.54899\n",
            "Epoch: 6114 Train Loss: 0.52120 Validation Loss: 0.54804\n",
            "Epoch: 6115 Train Loss: 0.52373 Validation Loss: 0.54886\n",
            "Epoch: 6116 Train Loss: 0.52113 Validation Loss: 0.54861\n",
            "Epoch: 6117 Train Loss: 0.52315 Validation Loss: 0.54899\n",
            "Epoch: 6118 Train Loss: 0.52185 Validation Loss: 0.54923\n",
            "Epoch: 6119 Train Loss: 0.52315 Validation Loss: 0.54923\n",
            "Epoch: 6120 Train Loss: 0.52405 Validation Loss: 0.54907\n",
            "Epoch: 6121 Train Loss: 0.52558 Validation Loss: 0.54829\n",
            "Epoch: 6122 Train Loss: 0.52291 Validation Loss: 0.54810\n",
            "Epoch: 6123 Train Loss: 0.52193 Validation Loss: 0.54864\n",
            "Epoch: 6124 Train Loss: 0.52590 Validation Loss: 0.54905\n",
            "Epoch: 6125 Train Loss: 0.52580 Validation Loss: 0.54872\n",
            "Epoch: 6126 Train Loss: 0.52120 Validation Loss: 0.54832\n",
            "Epoch: 6127 Train Loss: 0.52430 Validation Loss: 0.54848\n",
            "Epoch: 6128 Train Loss: 0.52295 Validation Loss: 0.54875\n",
            "Epoch: 6129 Train Loss: 0.52375 Validation Loss: 0.54875\n",
            "Epoch: 6130 Train Loss: 0.52589 Validation Loss: 0.54856\n",
            "Epoch: 6131 Train Loss: 0.52694 Validation Loss: 0.54888\n",
            "Epoch: 6132 Train Loss: 0.52509 Validation Loss: 0.54834\n",
            "Epoch: 6133 Train Loss: 0.52220 Validation Loss: 0.54807\n",
            "Epoch: 6134 Train Loss: 0.52097 Validation Loss: 0.54859\n",
            "Epoch: 6135 Train Loss: 0.52197 Validation Loss: 0.54936\n",
            "Epoch: 6136 Train Loss: 0.52829 Validation Loss: 0.54926\n",
            "Epoch: 6137 Train Loss: 0.52304 Validation Loss: 0.54804\n",
            "Epoch: 6138 Train Loss: 0.52599 Validation Loss: 0.54895\n",
            "Epoch: 6139 Train Loss: 0.52323 Validation Loss: 0.54905\n",
            "Epoch: 6140 Train Loss: 0.52675 Validation Loss: 0.54874\n",
            "Epoch: 6141 Train Loss: 0.52481 Validation Loss: 0.54819\n",
            "Epoch: 6142 Train Loss: 0.52118 Validation Loss: 0.54809\n",
            "Epoch: 6143 Train Loss: 0.52626 Validation Loss: 0.54908\n",
            "Epoch: 6144 Train Loss: 0.51978 Validation Loss: 0.54839\n",
            "Epoch: 6145 Train Loss: 0.52082 Validation Loss: 0.54887\n",
            "Epoch: 6146 Train Loss: 0.52431 Validation Loss: 0.54840\n",
            "Epoch: 6147 Train Loss: 0.52072 Validation Loss: 0.54877\n",
            "Epoch: 6148 Train Loss: 0.52594 Validation Loss: 0.54914\n",
            "Epoch: 6149 Train Loss: 0.52397 Validation Loss: 0.54870\n",
            "Epoch: 6150 Train Loss: 0.52411 Validation Loss: 0.54895\n",
            "Epoch: 6151 Train Loss: 0.52184 Validation Loss: 0.54838\n",
            "Epoch: 6152 Train Loss: 0.52595 Validation Loss: 0.54841\n",
            "Epoch: 6153 Train Loss: 0.52790 Validation Loss: 0.54870\n",
            "Epoch: 6154 Train Loss: 0.52076 Validation Loss: 0.54850\n",
            "Epoch: 6155 Train Loss: 0.52378 Validation Loss: 0.54852\n",
            "Epoch: 6156 Train Loss: 0.52398 Validation Loss: 0.54880\n",
            "Epoch: 6157 Train Loss: 0.52392 Validation Loss: 0.54862\n",
            "Epoch: 6158 Train Loss: 0.52235 Validation Loss: 0.54824\n",
            "Epoch: 6159 Train Loss: 0.52186 Validation Loss: 0.54878\n",
            "Epoch: 6160 Train Loss: 0.52527 Validation Loss: 0.54919\n",
            "Epoch: 6161 Train Loss: 0.52231 Validation Loss: 0.54860\n",
            "Epoch: 6162 Train Loss: 0.52294 Validation Loss: 0.54826\n",
            "Epoch: 6163 Train Loss: 0.52594 Validation Loss: 0.54877\n",
            "Epoch: 6164 Train Loss: 0.52687 Validation Loss: 0.54877\n",
            "Epoch: 6165 Train Loss: 0.52132 Validation Loss: 0.54780\n",
            "Epoch: 6166 Train Loss: 0.52293 Validation Loss: 0.54879\n",
            "Epoch: 6167 Train Loss: 0.52415 Validation Loss: 0.54901\n",
            "Epoch: 6168 Train Loss: 0.52711 Validation Loss: 0.54913\n",
            "Epoch: 6169 Train Loss: 0.52294 Validation Loss: 0.54819\n",
            "Epoch: 6170 Train Loss: 0.52395 Validation Loss: 0.54815\n",
            "Epoch: 6171 Train Loss: 0.52348 Validation Loss: 0.54922\n",
            "Epoch: 6172 Train Loss: 0.52083 Validation Loss: 0.54819\n",
            "Epoch: 6173 Train Loss: 0.52819 Validation Loss: 0.54907\n",
            "Epoch: 6174 Train Loss: 0.52187 Validation Loss: 0.54835\n",
            "Epoch: 6175 Train Loss: 0.52419 Validation Loss: 0.54821\n",
            "Epoch: 6176 Train Loss: 0.52371 Validation Loss: 0.54921\n",
            "Epoch: 6177 Train Loss: 0.52443 Validation Loss: 0.54971\n",
            "Epoch: 6178 Train Loss: 0.52316 Validation Loss: 0.54774\n",
            "Epoch: 6179 Train Loss: 0.52408 Validation Loss: 0.54830\n",
            "Epoch: 6180 Train Loss: 0.52107 Validation Loss: 0.54856\n",
            "Epoch: 6181 Train Loss: 0.52590 Validation Loss: 0.54928\n",
            "Epoch: 6182 Train Loss: 0.52640 Validation Loss: 0.54869\n",
            "Epoch: 6183 Train Loss: 0.52208 Validation Loss: 0.54829\n",
            "Epoch: 6184 Train Loss: 0.52175 Validation Loss: 0.54853\n",
            "Epoch: 6185 Train Loss: 0.52714 Validation Loss: 0.54897\n",
            "Epoch: 6186 Train Loss: 0.52205 Validation Loss: 0.54828\n",
            "Epoch: 6187 Train Loss: 0.52279 Validation Loss: 0.54888\n",
            "Epoch: 6188 Train Loss: 0.52287 Validation Loss: 0.54914\n",
            "Epoch: 6189 Train Loss: 0.52177 Validation Loss: 0.54861\n",
            "Epoch: 6190 Train Loss: 0.52295 Validation Loss: 0.54903\n",
            "Epoch: 6191 Train Loss: 0.52181 Validation Loss: 0.54864\n",
            "Epoch: 6192 Train Loss: 0.52285 Validation Loss: 0.54849\n",
            "Epoch: 6193 Train Loss: 0.52401 Validation Loss: 0.54897\n",
            "Epoch: 6194 Train Loss: 0.52192 Validation Loss: 0.54812\n",
            "Epoch: 6195 Train Loss: 0.52217 Validation Loss: 0.54856\n",
            "Epoch: 6196 Train Loss: 0.52546 Validation Loss: 0.54896\n",
            "Epoch: 6197 Train Loss: 0.52515 Validation Loss: 0.54865\n",
            "Epoch: 6198 Train Loss: 0.52471 Validation Loss: 0.54832\n",
            "Epoch: 6199 Train Loss: 0.52292 Validation Loss: 0.54815\n",
            "Epoch: 6200 Train Loss: 0.52434 Validation Loss: 0.54845\n",
            "Epoch: 6201 Train Loss: 0.52241 Validation Loss: 0.54835\n",
            "Epoch: 6202 Train Loss: 0.52387 Validation Loss: 0.54858\n",
            "Epoch: 6203 Train Loss: 0.52390 Validation Loss: 0.54875\n",
            "Epoch: 6204 Train Loss: 0.52510 Validation Loss: 0.54861\n",
            "Epoch: 6205 Train Loss: 0.52230 Validation Loss: 0.54804\n",
            "Epoch: 6206 Train Loss: 0.52382 Validation Loss: 0.54905\n",
            "Epoch: 6207 Train Loss: 0.52479 Validation Loss: 0.54883\n",
            "Epoch: 6208 Train Loss: 0.52185 Validation Loss: 0.54871\n",
            "Epoch: 6209 Train Loss: 0.52374 Validation Loss: 0.54873\n",
            "Epoch: 6210 Train Loss: 0.52625 Validation Loss: 0.54830\n",
            "Epoch: 6211 Train Loss: 0.52417 Validation Loss: 0.54888\n",
            "Epoch: 6212 Train Loss: 0.52188 Validation Loss: 0.54853\n",
            "Epoch: 6213 Train Loss: 0.52576 Validation Loss: 0.54860\n",
            "Epoch: 6214 Train Loss: 0.52281 Validation Loss: 0.54864\n",
            "Epoch: 6215 Train Loss: 0.52624 Validation Loss: 0.54897\n",
            "Epoch: 6216 Train Loss: 0.52097 Validation Loss: 0.54811\n",
            "Epoch: 6217 Train Loss: 0.52197 Validation Loss: 0.54891\n",
            "Epoch: 6218 Train Loss: 0.52535 Validation Loss: 0.54924\n",
            "Epoch: 6219 Train Loss: 0.52271 Validation Loss: 0.54842\n",
            "Epoch: 6220 Train Loss: 0.52334 Validation Loss: 0.54902\n",
            "Epoch: 6221 Train Loss: 0.52192 Validation Loss: 0.54809\n",
            "Epoch: 6222 Train Loss: 0.52481 Validation Loss: 0.54871\n",
            "Epoch: 6223 Train Loss: 0.52579 Validation Loss: 0.54880\n",
            "Epoch: 6224 Train Loss: 0.52119 Validation Loss: 0.54822\n",
            "Epoch: 6225 Train Loss: 0.52094 Validation Loss: 0.54863\n",
            "Epoch: 6226 Train Loss: 0.52494 Validation Loss: 0.54892\n",
            "Epoch: 6227 Train Loss: 0.51979 Validation Loss: 0.54889\n",
            "Epoch: 6228 Train Loss: 0.52296 Validation Loss: 0.54838\n",
            "Epoch: 6229 Train Loss: 0.52227 Validation Loss: 0.54873\n",
            "Epoch: 6230 Train Loss: 0.52312 Validation Loss: 0.54834\n",
            "Epoch: 6231 Train Loss: 0.52168 Validation Loss: 0.54915\n",
            "Epoch: 6232 Train Loss: 0.52003 Validation Loss: 0.54858\n",
            "Epoch: 6233 Train Loss: 0.52289 Validation Loss: 0.54898\n",
            "Epoch: 6234 Train Loss: 0.52276 Validation Loss: 0.54872\n",
            "Epoch: 6235 Train Loss: 0.52608 Validation Loss: 0.54896\n",
            "Epoch: 6236 Train Loss: 0.51982 Validation Loss: 0.54816\n",
            "Epoch: 6237 Train Loss: 0.52286 Validation Loss: 0.54894\n",
            "Epoch: 6238 Train Loss: 0.52759 Validation Loss: 0.54927\n",
            "Epoch: 6239 Train Loss: 0.52494 Validation Loss: 0.54832\n",
            "Epoch: 6240 Train Loss: 0.52502 Validation Loss: 0.54847\n",
            "Epoch: 6241 Train Loss: 0.52405 Validation Loss: 0.54859\n",
            "Epoch: 6242 Train Loss: 0.52310 Validation Loss: 0.54867\n",
            "Epoch: 6243 Train Loss: 0.52486 Validation Loss: 0.54847\n",
            "Epoch: 6244 Train Loss: 0.52632 Validation Loss: 0.54942\n",
            "Epoch: 6245 Train Loss: 0.52423 Validation Loss: 0.54832\n",
            "Epoch: 6246 Train Loss: 0.52105 Validation Loss: 0.54903\n",
            "Epoch: 6247 Train Loss: 0.52184 Validation Loss: 0.54857\n",
            "Epoch: 6248 Train Loss: 0.52395 Validation Loss: 0.54903\n",
            "Epoch: 6249 Train Loss: 0.52296 Validation Loss: 0.54883\n",
            "Epoch: 6250 Train Loss: 0.52193 Validation Loss: 0.54816\n",
            "Epoch: 6251 Train Loss: 0.52310 Validation Loss: 0.54842\n",
            "Epoch: 6252 Train Loss: 0.52398 Validation Loss: 0.54849\n",
            "Epoch: 6253 Train Loss: 0.52386 Validation Loss: 0.54854\n",
            "Epoch: 6254 Train Loss: 0.52578 Validation Loss: 0.54960\n",
            "Epoch: 6255 Train Loss: 0.52667 Validation Loss: 0.54826\n",
            "Epoch: 6256 Train Loss: 0.52415 Validation Loss: 0.54841\n",
            "Epoch: 6257 Train Loss: 0.52494 Validation Loss: 0.54820\n",
            "Epoch: 6258 Train Loss: 0.51981 Validation Loss: 0.54857\n",
            "Epoch: 6259 Train Loss: 0.52394 Validation Loss: 0.54871\n",
            "Epoch: 6260 Train Loss: 0.52694 Validation Loss: 0.54854\n",
            "Epoch: 6261 Train Loss: 0.52287 Validation Loss: 0.54871\n",
            "Epoch: 6262 Train Loss: 0.52735 Validation Loss: 0.54872\n",
            "Epoch: 6263 Train Loss: 0.52187 Validation Loss: 0.54829\n",
            "Epoch: 6264 Train Loss: 0.52475 Validation Loss: 0.54879\n",
            "Epoch: 6265 Train Loss: 0.52189 Validation Loss: 0.54839\n",
            "Epoch: 6266 Train Loss: 0.52327 Validation Loss: 0.54907\n",
            "Epoch: 6267 Train Loss: 0.52505 Validation Loss: 0.54830\n",
            "Epoch: 6268 Train Loss: 0.52920 Validation Loss: 0.54917\n",
            "Epoch: 6269 Train Loss: 0.52519 Validation Loss: 0.54843\n",
            "Epoch: 6270 Train Loss: 0.52206 Validation Loss: 0.54842\n",
            "Epoch: 6271 Train Loss: 0.52387 Validation Loss: 0.54848\n",
            "Epoch: 6272 Train Loss: 0.52592 Validation Loss: 0.54869\n",
            "Epoch: 6273 Train Loss: 0.52017 Validation Loss: 0.54792\n",
            "Epoch: 6274 Train Loss: 0.52071 Validation Loss: 0.54866\n",
            "Epoch: 6275 Train Loss: 0.52170 Validation Loss: 0.54935\n",
            "Epoch: 6276 Train Loss: 0.52291 Validation Loss: 0.54916\n",
            "Epoch: 6277 Train Loss: 0.52201 Validation Loss: 0.54848\n",
            "Epoch: 6278 Train Loss: 0.52271 Validation Loss: 0.54843\n",
            "Epoch: 6279 Train Loss: 0.51982 Validation Loss: 0.54839\n",
            "Epoch: 6280 Train Loss: 0.52604 Validation Loss: 0.54904\n",
            "Epoch: 6281 Train Loss: 0.52393 Validation Loss: 0.54836\n",
            "Epoch: 6282 Train Loss: 0.51893 Validation Loss: 0.54852\n",
            "Epoch: 6283 Train Loss: 0.52517 Validation Loss: 0.54852\n",
            "Epoch: 6284 Train Loss: 0.52396 Validation Loss: 0.54925\n",
            "Epoch: 6285 Train Loss: 0.52095 Validation Loss: 0.54873\n",
            "Epoch: 6286 Train Loss: 0.52217 Validation Loss: 0.54844\n",
            "Epoch: 6287 Train Loss: 0.52354 Validation Loss: 0.54844\n",
            "Epoch: 6288 Train Loss: 0.52487 Validation Loss: 0.54968\n",
            "Epoch: 6289 Train Loss: 0.52353 Validation Loss: 0.54826\n",
            "Epoch: 6290 Train Loss: 0.52423 Validation Loss: 0.54848\n",
            "Epoch: 6291 Train Loss: 0.52284 Validation Loss: 0.54892\n",
            "Epoch: 6292 Train Loss: 0.52172 Validation Loss: 0.54877\n",
            "Epoch: 6293 Train Loss: 0.52629 Validation Loss: 0.54937\n",
            "Epoch: 6294 Train Loss: 0.52283 Validation Loss: 0.54847\n",
            "Epoch: 6295 Train Loss: 0.52489 Validation Loss: 0.54835\n",
            "Epoch: 6296 Train Loss: 0.52185 Validation Loss: 0.54866\n",
            "Epoch: 6297 Train Loss: 0.52085 Validation Loss: 0.54831\n",
            "Epoch: 6298 Train Loss: 0.52293 Validation Loss: 0.54885\n",
            "Epoch: 6299 Train Loss: 0.52416 Validation Loss: 0.54918\n",
            "Epoch: 6300 Train Loss: 0.52288 Validation Loss: 0.54838\n",
            "Epoch: 6301 Train Loss: 0.52436 Validation Loss: 0.54854\n",
            "Epoch: 6302 Train Loss: 0.52079 Validation Loss: 0.54825\n",
            "Epoch: 6303 Train Loss: 0.52705 Validation Loss: 0.54892\n",
            "Epoch: 6304 Train Loss: 0.52384 Validation Loss: 0.54867\n",
            "Epoch: 6305 Train Loss: 0.52379 Validation Loss: 0.54849\n",
            "Epoch: 6306 Train Loss: 0.52386 Validation Loss: 0.54840\n",
            "Epoch: 6307 Train Loss: 0.52173 Validation Loss: 0.54846\n",
            "Epoch: 6308 Train Loss: 0.52793 Validation Loss: 0.54894\n",
            "Epoch: 6309 Train Loss: 0.52082 Validation Loss: 0.54828\n",
            "Epoch: 6310 Train Loss: 0.52399 Validation Loss: 0.54873\n",
            "Epoch: 6311 Train Loss: 0.52378 Validation Loss: 0.54841\n",
            "Epoch: 6312 Train Loss: 0.52289 Validation Loss: 0.54849\n",
            "Epoch: 6313 Train Loss: 0.52182 Validation Loss: 0.54879\n",
            "Epoch: 6314 Train Loss: 0.52484 Validation Loss: 0.54866\n",
            "Epoch: 6315 Train Loss: 0.52293 Validation Loss: 0.54851\n",
            "Epoch: 6316 Train Loss: 0.52426 Validation Loss: 0.54868\n",
            "Epoch: 6317 Train Loss: 0.52614 Validation Loss: 0.54840\n",
            "Epoch: 6318 Train Loss: 0.52391 Validation Loss: 0.54820\n",
            "Epoch: 6319 Train Loss: 0.52274 Validation Loss: 0.54886\n",
            "Epoch: 6320 Train Loss: 0.52328 Validation Loss: 0.54832\n",
            "Epoch: 6321 Train Loss: 0.52280 Validation Loss: 0.54913\n",
            "Epoch: 6322 Train Loss: 0.52635 Validation Loss: 0.54880\n",
            "Epoch: 6323 Train Loss: 0.52185 Validation Loss: 0.54888\n",
            "Epoch: 6324 Train Loss: 0.52182 Validation Loss: 0.54910\n",
            "Epoch: 6325 Train Loss: 0.52320 Validation Loss: 0.54837\n",
            "Epoch: 6326 Train Loss: 0.52282 Validation Loss: 0.54857\n",
            "Epoch: 6327 Train Loss: 0.52088 Validation Loss: 0.54922\n",
            "Epoch: 6328 Train Loss: 0.52289 Validation Loss: 0.54902\n",
            "Epoch: 6329 Train Loss: 0.52190 Validation Loss: 0.54841\n",
            "Epoch: 6330 Train Loss: 0.52184 Validation Loss: 0.54878\n",
            "Epoch: 6331 Train Loss: 0.52174 Validation Loss: 0.54901\n",
            "Epoch: 6332 Train Loss: 0.52409 Validation Loss: 0.54922\n",
            "Epoch: 6333 Train Loss: 0.52410 Validation Loss: 0.54803\n",
            "Epoch: 6334 Train Loss: 0.52190 Validation Loss: 0.54869\n",
            "Epoch: 6335 Train Loss: 0.52093 Validation Loss: 0.54859\n",
            "Epoch: 6336 Train Loss: 0.52370 Validation Loss: 0.54949\n",
            "Epoch: 6337 Train Loss: 0.52088 Validation Loss: 0.54846\n",
            "Epoch: 6338 Train Loss: 0.52384 Validation Loss: 0.54874\n",
            "Epoch: 6339 Train Loss: 0.52093 Validation Loss: 0.54851\n",
            "Epoch: 6340 Train Loss: 0.52586 Validation Loss: 0.54902\n",
            "Epoch: 6341 Train Loss: 0.52371 Validation Loss: 0.54854\n",
            "Epoch: 6342 Train Loss: 0.52338 Validation Loss: 0.54846\n",
            "Epoch: 6343 Train Loss: 0.52283 Validation Loss: 0.54814\n",
            "Epoch: 6344 Train Loss: 0.52701 Validation Loss: 0.54882\n",
            "Epoch: 6345 Train Loss: 0.52377 Validation Loss: 0.54848\n",
            "Epoch: 6346 Train Loss: 0.52091 Validation Loss: 0.54861\n",
            "Epoch: 6347 Train Loss: 0.52489 Validation Loss: 0.54872\n",
            "Epoch: 6348 Train Loss: 0.52421 Validation Loss: 0.54843\n",
            "Epoch: 6349 Train Loss: 0.52501 Validation Loss: 0.54939\n",
            "Epoch: 6350 Train Loss: 0.52214 Validation Loss: 0.54841\n",
            "Epoch: 6351 Train Loss: 0.52112 Validation Loss: 0.54918\n",
            "Epoch: 6352 Train Loss: 0.51985 Validation Loss: 0.54889\n",
            "Epoch: 6353 Train Loss: 0.52337 Validation Loss: 0.54873\n",
            "Epoch: 6354 Train Loss: 0.52305 Validation Loss: 0.54853\n",
            "Epoch: 6355 Train Loss: 0.52403 Validation Loss: 0.54878\n",
            "Epoch: 6356 Train Loss: 0.52020 Validation Loss: 0.54808\n",
            "Epoch: 6357 Train Loss: 0.52155 Validation Loss: 0.54917\n",
            "Epoch: 6358 Train Loss: 0.52187 Validation Loss: 0.54883\n",
            "Epoch: 6359 Train Loss: 0.52582 Validation Loss: 0.54877\n",
            "Epoch: 6360 Train Loss: 0.52422 Validation Loss: 0.54811\n",
            "Epoch: 6361 Train Loss: 0.52202 Validation Loss: 0.54868\n",
            "Epoch: 6362 Train Loss: 0.52294 Validation Loss: 0.54890\n",
            "Epoch: 6363 Train Loss: 0.52286 Validation Loss: 0.54874\n",
            "Epoch: 6364 Train Loss: 0.52506 Validation Loss: 0.54837\n",
            "Epoch: 6365 Train Loss: 0.52558 Validation Loss: 0.54950\n",
            "Epoch: 6366 Train Loss: 0.52265 Validation Loss: 0.54829\n",
            "Epoch: 6367 Train Loss: 0.52299 Validation Loss: 0.54806\n",
            "Epoch: 6368 Train Loss: 0.52079 Validation Loss: 0.54847\n",
            "Epoch: 6369 Train Loss: 0.52272 Validation Loss: 0.54880\n",
            "Epoch: 6370 Train Loss: 0.52098 Validation Loss: 0.54877\n",
            "Epoch: 6371 Train Loss: 0.52198 Validation Loss: 0.54845\n",
            "Epoch: 6372 Train Loss: 0.52374 Validation Loss: 0.54884\n",
            "Epoch: 6373 Train Loss: 0.52666 Validation Loss: 0.54981\n",
            "Epoch: 6374 Train Loss: 0.52354 Validation Loss: 0.54829\n",
            "Epoch: 6375 Train Loss: 0.52012 Validation Loss: 0.54786\n",
            "Epoch: 6376 Train Loss: 0.52589 Validation Loss: 0.54922\n",
            "Epoch: 6377 Train Loss: 0.52178 Validation Loss: 0.54887\n",
            "Epoch: 6378 Train Loss: 0.52082 Validation Loss: 0.54847\n",
            "Epoch: 6379 Train Loss: 0.52381 Validation Loss: 0.54892\n",
            "Epoch: 6380 Train Loss: 0.52600 Validation Loss: 0.54875\n",
            "Epoch: 6381 Train Loss: 0.52114 Validation Loss: 0.54810\n",
            "Epoch: 6382 Train Loss: 0.52305 Validation Loss: 0.54937\n",
            "Epoch: 6383 Train Loss: 0.52819 Validation Loss: 0.54870\n",
            "Epoch: 6384 Train Loss: 0.52143 Validation Loss: 0.54799\n",
            "Epoch: 6385 Train Loss: 0.52339 Validation Loss: 0.54876\n",
            "Epoch: 6386 Train Loss: 0.52460 Validation Loss: 0.54958\n",
            "Epoch: 6387 Train Loss: 0.52291 Validation Loss: 0.54828\n",
            "Epoch: 6388 Train Loss: 0.52296 Validation Loss: 0.54868\n",
            "Epoch: 6389 Train Loss: 0.52285 Validation Loss: 0.54847\n",
            "Epoch: 6390 Train Loss: 0.52285 Validation Loss: 0.54883\n",
            "Epoch: 6391 Train Loss: 0.52190 Validation Loss: 0.54858\n",
            "Epoch: 6392 Train Loss: 0.52501 Validation Loss: 0.54950\n",
            "Epoch: 6393 Train Loss: 0.52105 Validation Loss: 0.54839\n",
            "Epoch: 6394 Train Loss: 0.52274 Validation Loss: 0.54872\n",
            "Epoch: 6395 Train Loss: 0.52292 Validation Loss: 0.54872\n",
            "Epoch: 6396 Train Loss: 0.52303 Validation Loss: 0.54892\n",
            "Epoch: 6397 Train Loss: 0.52409 Validation Loss: 0.54904\n",
            "Epoch: 6398 Train Loss: 0.52180 Validation Loss: 0.54826\n",
            "Epoch: 6399 Train Loss: 0.52297 Validation Loss: 0.54861\n",
            "Epoch: 6400 Train Loss: 0.52229 Validation Loss: 0.54834\n",
            "Epoch: 6401 Train Loss: 0.52529 Validation Loss: 0.54862\n",
            "Epoch: 6402 Train Loss: 0.52080 Validation Loss: 0.54851\n",
            "Epoch: 6403 Train Loss: 0.52203 Validation Loss: 0.54841\n",
            "Epoch: 6404 Train Loss: 0.52331 Validation Loss: 0.54926\n",
            "Epoch: 6405 Train Loss: 0.52276 Validation Loss: 0.54822\n",
            "Epoch: 6406 Train Loss: 0.52237 Validation Loss: 0.54802\n",
            "Epoch: 6407 Train Loss: 0.52517 Validation Loss: 0.54966\n",
            "Epoch: 6408 Train Loss: 0.52568 Validation Loss: 0.54832\n",
            "Epoch: 6409 Train Loss: 0.52211 Validation Loss: 0.54812\n",
            "Epoch: 6410 Train Loss: 0.52545 Validation Loss: 0.54909\n",
            "Epoch: 6411 Train Loss: 0.52373 Validation Loss: 0.54842\n",
            "Epoch: 6412 Train Loss: 0.52525 Validation Loss: 0.54828\n",
            "Epoch: 6413 Train Loss: 0.52213 Validation Loss: 0.54823\n",
            "Epoch: 6414 Train Loss: 0.52247 Validation Loss: 0.54945\n",
            "Epoch: 6415 Train Loss: 0.52298 Validation Loss: 0.54838\n",
            "Epoch: 6416 Train Loss: 0.52392 Validation Loss: 0.54877\n",
            "Epoch: 6417 Train Loss: 0.52807 Validation Loss: 0.54904\n",
            "Epoch: 6418 Train Loss: 0.52376 Validation Loss: 0.54850\n",
            "Epoch: 6419 Train Loss: 0.52110 Validation Loss: 0.54818\n",
            "Epoch: 6420 Train Loss: 0.52189 Validation Loss: 0.54857\n",
            "Epoch: 6421 Train Loss: 0.52150 Validation Loss: 0.54891\n",
            "Epoch: 6422 Train Loss: 0.52424 Validation Loss: 0.54884\n",
            "Epoch: 6423 Train Loss: 0.52622 Validation Loss: 0.54830\n",
            "Epoch: 6424 Train Loss: 0.52328 Validation Loss: 0.54834\n",
            "Epoch: 6425 Train Loss: 0.52485 Validation Loss: 0.54889\n",
            "Epoch: 6426 Train Loss: 0.52530 Validation Loss: 0.54917\n",
            "Epoch: 6427 Train Loss: 0.52076 Validation Loss: 0.54840\n",
            "Epoch: 6428 Train Loss: 0.52392 Validation Loss: 0.54845\n",
            "Epoch: 6429 Train Loss: 0.52290 Validation Loss: 0.54881\n",
            "Epoch: 6430 Train Loss: 0.52446 Validation Loss: 0.54854\n",
            "Epoch: 6431 Train Loss: 0.52384 Validation Loss: 0.54837\n",
            "Epoch: 6432 Train Loss: 0.52502 Validation Loss: 0.54804\n",
            "Epoch: 6433 Train Loss: 0.52394 Validation Loss: 0.54926\n",
            "Epoch: 6434 Train Loss: 0.52392 Validation Loss: 0.54876\n",
            "Epoch: 6435 Train Loss: 0.52710 Validation Loss: 0.54828\n",
            "Epoch: 6436 Train Loss: 0.52184 Validation Loss: 0.54814\n",
            "Epoch: 6437 Train Loss: 0.52256 Validation Loss: 0.54866\n",
            "Epoch: 6438 Train Loss: 0.52485 Validation Loss: 0.54869\n",
            "Epoch: 6439 Train Loss: 0.52400 Validation Loss: 0.54826\n",
            "Epoch: 6440 Train Loss: 0.52381 Validation Loss: 0.54842\n",
            "Epoch: 6441 Train Loss: 0.52394 Validation Loss: 0.54820\n",
            "Epoch: 6442 Train Loss: 0.52173 Validation Loss: 0.54858\n",
            "Epoch: 6443 Train Loss: 0.52385 Validation Loss: 0.54920\n",
            "Epoch: 6444 Train Loss: 0.52809 Validation Loss: 0.54923\n",
            "Epoch: 6445 Train Loss: 0.52289 Validation Loss: 0.54780\n",
            "Epoch: 6446 Train Loss: 0.52099 Validation Loss: 0.54874\n",
            "Epoch: 6447 Train Loss: 0.52397 Validation Loss: 0.54896\n",
            "Epoch: 6448 Train Loss: 0.52283 Validation Loss: 0.54820\n",
            "Epoch: 6449 Train Loss: 0.52154 Validation Loss: 0.54986\n",
            "Epoch: 6450 Train Loss: 0.52306 Validation Loss: 0.54827\n",
            "Epoch: 6451 Train Loss: 0.52900 Validation Loss: 0.54886\n",
            "Epoch: 6452 Train Loss: 0.52502 Validation Loss: 0.54837\n",
            "Epoch: 6453 Train Loss: 0.52292 Validation Loss: 0.54875\n",
            "Epoch: 6454 Train Loss: 0.52190 Validation Loss: 0.54861\n",
            "Epoch: 6455 Train Loss: 0.52198 Validation Loss: 0.54840\n",
            "Epoch: 6456 Train Loss: 0.52515 Validation Loss: 0.54853\n",
            "Epoch: 6457 Train Loss: 0.52623 Validation Loss: 0.54920\n",
            "Epoch: 6458 Train Loss: 0.52680 Validation Loss: 0.54803\n",
            "Epoch: 6459 Train Loss: 0.52353 Validation Loss: 0.54915\n",
            "Epoch: 6460 Train Loss: 0.52638 Validation Loss: 0.54820\n",
            "Epoch: 6461 Train Loss: 0.52431 Validation Loss: 0.54946\n",
            "Epoch: 6462 Train Loss: 0.52478 Validation Loss: 0.54847\n",
            "Epoch: 6463 Train Loss: 0.52185 Validation Loss: 0.54843\n",
            "Epoch: 6464 Train Loss: 0.52191 Validation Loss: 0.54853\n",
            "Epoch: 6465 Train Loss: 0.52614 Validation Loss: 0.54863\n",
            "Epoch: 6466 Train Loss: 0.52418 Validation Loss: 0.54834\n",
            "Epoch: 6467 Train Loss: 0.52342 Validation Loss: 0.54906\n",
            "Epoch: 6468 Train Loss: 0.52287 Validation Loss: 0.54828\n",
            "Epoch: 6469 Train Loss: 0.52520 Validation Loss: 0.54831\n",
            "Epoch: 6470 Train Loss: 0.52301 Validation Loss: 0.54830\n",
            "Epoch: 6471 Train Loss: 0.52065 Validation Loss: 0.54904\n",
            "Epoch: 6472 Train Loss: 0.52184 Validation Loss: 0.54882\n",
            "Epoch: 6473 Train Loss: 0.52496 Validation Loss: 0.54897\n",
            "Epoch: 6474 Train Loss: 0.52180 Validation Loss: 0.54844\n",
            "Epoch: 6475 Train Loss: 0.52309 Validation Loss: 0.54891\n",
            "Epoch: 6476 Train Loss: 0.52059 Validation Loss: 0.54789\n",
            "Epoch: 6477 Train Loss: 0.52212 Validation Loss: 0.54893\n",
            "Epoch: 6478 Train Loss: 0.52282 Validation Loss: 0.54886\n",
            "Epoch: 6479 Train Loss: 0.52094 Validation Loss: 0.54859\n",
            "Epoch: 6480 Train Loss: 0.52377 Validation Loss: 0.54877\n",
            "Epoch: 6481 Train Loss: 0.52500 Validation Loss: 0.54901\n",
            "Epoch: 6482 Train Loss: 0.52757 Validation Loss: 0.54824\n",
            "Epoch: 6483 Train Loss: 0.52353 Validation Loss: 0.54782\n",
            "Epoch: 6484 Train Loss: 0.52291 Validation Loss: 0.54875\n",
            "Epoch: 6485 Train Loss: 0.52398 Validation Loss: 0.54930\n",
            "Epoch: 6486 Train Loss: 0.52275 Validation Loss: 0.54838\n",
            "Epoch: 6487 Train Loss: 0.52583 Validation Loss: 0.54843\n",
            "Epoch: 6488 Train Loss: 0.52660 Validation Loss: 0.54861\n",
            "Epoch: 6489 Train Loss: 0.52115 Validation Loss: 0.54847\n",
            "Epoch: 6490 Train Loss: 0.52286 Validation Loss: 0.54873\n",
            "Epoch: 6491 Train Loss: 0.52093 Validation Loss: 0.54856\n",
            "Epoch: 6492 Train Loss: 0.52182 Validation Loss: 0.54900\n",
            "Epoch: 6493 Train Loss: 0.52290 Validation Loss: 0.54888\n",
            "Epoch: 6494 Train Loss: 0.52174 Validation Loss: 0.54860\n",
            "Epoch: 6495 Train Loss: 0.52480 Validation Loss: 0.54875\n",
            "Epoch: 6496 Train Loss: 0.52096 Validation Loss: 0.54820\n",
            "Epoch: 6497 Train Loss: 0.52500 Validation Loss: 0.54939\n",
            "Epoch: 6498 Train Loss: 0.52285 Validation Loss: 0.54829\n",
            "Epoch: 6499 Train Loss: 0.52746 Validation Loss: 0.54877\n",
            "Epoch: 6500 Train Loss: 0.52462 Validation Loss: 0.54870\n",
            "Epoch: 6501 Train Loss: 0.52215 Validation Loss: 0.54839\n",
            "Epoch: 6502 Train Loss: 0.52448 Validation Loss: 0.54888\n",
            "Epoch: 6503 Train Loss: 0.52489 Validation Loss: 0.54860\n",
            "Epoch: 6504 Train Loss: 0.52281 Validation Loss: 0.54867\n",
            "Epoch: 6505 Train Loss: 0.52477 Validation Loss: 0.54927\n",
            "Epoch: 6506 Train Loss: 0.52387 Validation Loss: 0.54813\n",
            "Epoch: 6507 Train Loss: 0.52337 Validation Loss: 0.54850\n",
            "Epoch: 6508 Train Loss: 0.52601 Validation Loss: 0.54838\n",
            "Epoch: 6509 Train Loss: 0.52196 Validation Loss: 0.54810\n",
            "Epoch: 6510 Train Loss: 0.52072 Validation Loss: 0.54889\n",
            "Epoch: 6511 Train Loss: 0.52416 Validation Loss: 0.54935\n",
            "Epoch: 6512 Train Loss: 0.52169 Validation Loss: 0.54830\n",
            "Epoch: 6513 Train Loss: 0.52285 Validation Loss: 0.54869\n",
            "Epoch: 6514 Train Loss: 0.52390 Validation Loss: 0.54838\n",
            "Epoch: 6515 Train Loss: 0.52287 Validation Loss: 0.54858\n",
            "Epoch: 6516 Train Loss: 0.52383 Validation Loss: 0.54877\n",
            "Epoch: 6517 Train Loss: 0.52129 Validation Loss: 0.54834\n",
            "Epoch: 6518 Train Loss: 0.52419 Validation Loss: 0.54939\n",
            "Epoch: 6519 Train Loss: 0.52143 Validation Loss: 0.54807\n",
            "Epoch: 6520 Train Loss: 0.52788 Validation Loss: 0.54944\n",
            "Epoch: 6521 Train Loss: 0.52277 Validation Loss: 0.54860\n",
            "Epoch: 6522 Train Loss: 0.52382 Validation Loss: 0.54872\n",
            "Epoch: 6523 Train Loss: 0.52296 Validation Loss: 0.54825\n",
            "Epoch: 6524 Train Loss: 0.52102 Validation Loss: 0.54863\n",
            "Epoch: 6525 Train Loss: 0.52096 Validation Loss: 0.54842\n",
            "Epoch: 6526 Train Loss: 0.52166 Validation Loss: 0.54906\n",
            "Epoch: 6527 Train Loss: 0.52342 Validation Loss: 0.54829\n",
            "Epoch: 6528 Train Loss: 0.52267 Validation Loss: 0.54906\n",
            "Epoch: 6529 Train Loss: 0.51989 Validation Loss: 0.54901\n",
            "Epoch: 6530 Train Loss: 0.52846 Validation Loss: 0.54937\n",
            "Epoch: 6531 Train Loss: 0.52212 Validation Loss: 0.54822\n",
            "Epoch: 6532 Train Loss: 0.52293 Validation Loss: 0.54849\n",
            "Epoch: 6533 Train Loss: 0.52531 Validation Loss: 0.54869\n",
            "Epoch: 6534 Train Loss: 0.52214 Validation Loss: 0.54802\n",
            "Epoch: 6535 Train Loss: 0.52415 Validation Loss: 0.54927\n",
            "Epoch: 6536 Train Loss: 0.52318 Validation Loss: 0.54818\n",
            "Epoch: 6537 Train Loss: 0.52472 Validation Loss: 0.54867\n",
            "Epoch: 6538 Train Loss: 0.51985 Validation Loss: 0.54841\n",
            "Epoch: 6539 Train Loss: 0.52370 Validation Loss: 0.54888\n",
            "Epoch: 6540 Train Loss: 0.52401 Validation Loss: 0.54897\n",
            "Epoch: 6541 Train Loss: 0.52219 Validation Loss: 0.54810\n",
            "Epoch: 6542 Train Loss: 0.52078 Validation Loss: 0.54898\n",
            "Epoch: 6543 Train Loss: 0.52119 Validation Loss: 0.54892\n",
            "Epoch: 6544 Train Loss: 0.52388 Validation Loss: 0.54849\n",
            "Epoch: 6545 Train Loss: 0.52399 Validation Loss: 0.54864\n",
            "Epoch: 6546 Train Loss: 0.52407 Validation Loss: 0.54888\n",
            "Epoch: 6547 Train Loss: 0.52271 Validation Loss: 0.54844\n",
            "Epoch: 6548 Train Loss: 0.52285 Validation Loss: 0.54821\n",
            "Epoch: 6549 Train Loss: 0.52380 Validation Loss: 0.54870\n",
            "Epoch: 6550 Train Loss: 0.52388 Validation Loss: 0.54877\n",
            "Epoch: 6551 Train Loss: 0.52099 Validation Loss: 0.54871\n",
            "Epoch: 6552 Train Loss: 0.52924 Validation Loss: 0.54865\n",
            "Epoch: 6553 Train Loss: 0.52475 Validation Loss: 0.54817\n",
            "Epoch: 6554 Train Loss: 0.52392 Validation Loss: 0.54830\n",
            "Epoch: 6555 Train Loss: 0.52489 Validation Loss: 0.54872\n",
            "Epoch: 6556 Train Loss: 0.52389 Validation Loss: 0.54827\n",
            "Epoch: 6557 Train Loss: 0.52190 Validation Loss: 0.54844\n",
            "Epoch: 6558 Train Loss: 0.52206 Validation Loss: 0.54905\n",
            "Epoch: 6559 Train Loss: 0.52295 Validation Loss: 0.54876\n",
            "Epoch: 6560 Train Loss: 0.52375 Validation Loss: 0.54842\n",
            "Epoch: 6561 Train Loss: 0.52559 Validation Loss: 0.54867\n",
            "Epoch: 6562 Train Loss: 0.52630 Validation Loss: 0.54797\n",
            "Epoch: 6563 Train Loss: 0.52177 Validation Loss: 0.54907\n",
            "Epoch: 6564 Train Loss: 0.52343 Validation Loss: 0.54882\n",
            "Epoch: 6565 Train Loss: 0.52512 Validation Loss: 0.54892\n",
            "Epoch: 6566 Train Loss: 0.52731 Validation Loss: 0.54866\n",
            "Epoch: 6567 Train Loss: 0.52427 Validation Loss: 0.54814\n",
            "Epoch: 6568 Train Loss: 0.52377 Validation Loss: 0.54884\n",
            "Epoch: 6569 Train Loss: 0.52113 Validation Loss: 0.54888\n",
            "Epoch: 6570 Train Loss: 0.52385 Validation Loss: 0.54854\n",
            "Epoch: 6571 Train Loss: 0.52379 Validation Loss: 0.54854\n",
            "Epoch: 6572 Train Loss: 0.52195 Validation Loss: 0.54842\n",
            "Epoch: 6573 Train Loss: 0.52179 Validation Loss: 0.54888\n",
            "Epoch: 6574 Train Loss: 0.52319 Validation Loss: 0.54965\n",
            "Epoch: 6575 Train Loss: 0.51984 Validation Loss: 0.54841\n",
            "Epoch: 6576 Train Loss: 0.52274 Validation Loss: 0.54844\n",
            "Epoch: 6577 Train Loss: 0.52176 Validation Loss: 0.54850\n",
            "Epoch: 6578 Train Loss: 0.52278 Validation Loss: 0.54881\n",
            "Epoch: 6579 Train Loss: 0.52607 Validation Loss: 0.54885\n",
            "Epoch: 6580 Train Loss: 0.52197 Validation Loss: 0.54844\n",
            "Epoch: 6581 Train Loss: 0.52129 Validation Loss: 0.54852\n",
            "Epoch: 6582 Train Loss: 0.52460 Validation Loss: 0.54962\n",
            "Epoch: 6583 Train Loss: 0.52273 Validation Loss: 0.54845\n",
            "Epoch: 6584 Train Loss: 0.52132 Validation Loss: 0.54797\n",
            "Epoch: 6585 Train Loss: 0.52075 Validation Loss: 0.54864\n",
            "Epoch: 6586 Train Loss: 0.52652 Validation Loss: 0.55007\n",
            "Epoch: 6587 Train Loss: 0.52237 Validation Loss: 0.54806\n",
            "Epoch: 6588 Train Loss: 0.52270 Validation Loss: 0.54891\n",
            "Epoch: 6589 Train Loss: 0.52102 Validation Loss: 0.54868\n",
            "Epoch: 6590 Train Loss: 0.52477 Validation Loss: 0.54920\n",
            "Epoch: 6591 Train Loss: 0.52507 Validation Loss: 0.54924\n",
            "Epoch: 6592 Train Loss: 0.52358 Validation Loss: 0.54830\n",
            "Epoch: 6593 Train Loss: 0.52315 Validation Loss: 0.54789\n",
            "Epoch: 6594 Train Loss: 0.52113 Validation Loss: 0.54870\n",
            "Epoch: 6595 Train Loss: 0.52177 Validation Loss: 0.54895\n",
            "Epoch: 6596 Train Loss: 0.52077 Validation Loss: 0.54908\n",
            "Epoch: 6597 Train Loss: 0.52182 Validation Loss: 0.54868\n",
            "Epoch: 6598 Train Loss: 0.52282 Validation Loss: 0.54909\n",
            "Epoch: 6599 Train Loss: 0.52701 Validation Loss: 0.54909\n",
            "Epoch: 6600 Train Loss: 0.52627 Validation Loss: 0.54805\n",
            "Epoch: 6601 Train Loss: 0.52183 Validation Loss: 0.54871\n",
            "Epoch: 6602 Train Loss: 0.52189 Validation Loss: 0.54853\n",
            "Epoch: 6603 Train Loss: 0.52293 Validation Loss: 0.54916\n",
            "Epoch: 6604 Train Loss: 0.52185 Validation Loss: 0.54862\n",
            "Epoch: 6605 Train Loss: 0.52106 Validation Loss: 0.54843\n",
            "Epoch: 6606 Train Loss: 0.52108 Validation Loss: 0.54833\n",
            "Epoch: 6607 Train Loss: 0.52397 Validation Loss: 0.54848\n",
            "Epoch: 6608 Train Loss: 0.52540 Validation Loss: 0.54944\n",
            "Epoch: 6609 Train Loss: 0.52471 Validation Loss: 0.54845\n",
            "Epoch: 6610 Train Loss: 0.52263 Validation Loss: 0.54838\n",
            "Epoch: 6611 Train Loss: 0.52220 Validation Loss: 0.54785\n",
            "Epoch: 6612 Train Loss: 0.52554 Validation Loss: 0.54965\n",
            "Epoch: 6613 Train Loss: 0.52183 Validation Loss: 0.54856\n",
            "Epoch: 6614 Train Loss: 0.52286 Validation Loss: 0.54821\n",
            "Epoch: 6615 Train Loss: 0.52093 Validation Loss: 0.54896\n",
            "Epoch: 6616 Train Loss: 0.52396 Validation Loss: 0.54858\n",
            "Epoch: 6617 Train Loss: 0.52282 Validation Loss: 0.54848\n",
            "Epoch: 6618 Train Loss: 0.52212 Validation Loss: 0.54855\n",
            "Epoch: 6619 Train Loss: 0.52532 Validation Loss: 0.54937\n",
            "Epoch: 6620 Train Loss: 0.52172 Validation Loss: 0.54834\n",
            "Epoch: 6621 Train Loss: 0.52299 Validation Loss: 0.54844\n",
            "Epoch: 6622 Train Loss: 0.52111 Validation Loss: 0.54834\n",
            "Epoch: 6623 Train Loss: 0.52166 Validation Loss: 0.54930\n",
            "Epoch: 6624 Train Loss: 0.52300 Validation Loss: 0.54923\n",
            "Epoch: 6625 Train Loss: 0.52205 Validation Loss: 0.54852\n",
            "Epoch: 6626 Train Loss: 0.52371 Validation Loss: 0.54835\n",
            "Epoch: 6627 Train Loss: 0.52386 Validation Loss: 0.54844\n",
            "Epoch: 6628 Train Loss: 0.52177 Validation Loss: 0.54881\n",
            "Epoch: 6629 Train Loss: 0.52302 Validation Loss: 0.54867\n",
            "Epoch: 6630 Train Loss: 0.52484 Validation Loss: 0.54972\n",
            "Epoch: 6631 Train Loss: 0.52105 Validation Loss: 0.54812\n",
            "Epoch: 6632 Train Loss: 0.52496 Validation Loss: 0.54871\n",
            "Epoch: 6633 Train Loss: 0.52191 Validation Loss: 0.54841\n",
            "Epoch: 6634 Train Loss: 0.52389 Validation Loss: 0.54894\n",
            "Epoch: 6635 Train Loss: 0.52287 Validation Loss: 0.54822\n",
            "Epoch: 6636 Train Loss: 0.52474 Validation Loss: 0.54853\n",
            "Epoch: 6637 Train Loss: 0.52375 Validation Loss: 0.54878\n",
            "Epoch: 6638 Train Loss: 0.52189 Validation Loss: 0.54784\n",
            "Epoch: 6639 Train Loss: 0.51973 Validation Loss: 0.54994\n",
            "Epoch: 6640 Train Loss: 0.52122 Validation Loss: 0.54915\n",
            "Epoch: 6641 Train Loss: 0.52300 Validation Loss: 0.54836\n",
            "Epoch: 6642 Train Loss: 0.52415 Validation Loss: 0.54897\n",
            "Epoch: 6643 Train Loss: 0.52368 Validation Loss: 0.54840\n",
            "Epoch: 6644 Train Loss: 0.52285 Validation Loss: 0.54832\n",
            "Epoch: 6645 Train Loss: 0.52294 Validation Loss: 0.54834\n",
            "Epoch: 6646 Train Loss: 0.52069 Validation Loss: 0.54870\n",
            "Epoch: 6647 Train Loss: 0.52281 Validation Loss: 0.54896\n",
            "Epoch: 6648 Train Loss: 0.52292 Validation Loss: 0.54887\n",
            "Epoch: 6649 Train Loss: 0.52279 Validation Loss: 0.54890\n",
            "Epoch: 6650 Train Loss: 0.52336 Validation Loss: 0.54833\n",
            "Epoch: 6651 Train Loss: 0.52473 Validation Loss: 0.54881\n",
            "Epoch: 6652 Train Loss: 0.52238 Validation Loss: 0.54840\n",
            "Epoch: 6653 Train Loss: 0.52085 Validation Loss: 0.54893\n",
            "Epoch: 6654 Train Loss: 0.52299 Validation Loss: 0.54911\n",
            "Epoch: 6655 Train Loss: 0.52486 Validation Loss: 0.54828\n",
            "Epoch: 6656 Train Loss: 0.52374 Validation Loss: 0.54841\n",
            "Epoch: 6657 Train Loss: 0.52498 Validation Loss: 0.54876\n",
            "Epoch: 6658 Train Loss: 0.52185 Validation Loss: 0.54847\n",
            "Epoch: 6659 Train Loss: 0.52638 Validation Loss: 0.54857\n",
            "Epoch: 6660 Train Loss: 0.52477 Validation Loss: 0.54829\n",
            "Epoch: 6661 Train Loss: 0.52580 Validation Loss: 0.54835\n",
            "Epoch: 6662 Train Loss: 0.52709 Validation Loss: 0.54856\n",
            "Epoch: 6663 Train Loss: 0.52393 Validation Loss: 0.54854\n",
            "Epoch: 6664 Train Loss: 0.52454 Validation Loss: 0.54821\n",
            "Epoch: 6665 Train Loss: 0.52170 Validation Loss: 0.54901\n",
            "Epoch: 6666 Train Loss: 0.52288 Validation Loss: 0.54901\n",
            "Epoch: 6667 Train Loss: 0.52603 Validation Loss: 0.54912\n",
            "Epoch: 6668 Train Loss: 0.52393 Validation Loss: 0.54861\n",
            "Epoch: 6669 Train Loss: 0.52101 Validation Loss: 0.54834\n",
            "Epoch: 6670 Train Loss: 0.52208 Validation Loss: 0.54850\n",
            "Epoch: 6671 Train Loss: 0.52409 Validation Loss: 0.54938\n",
            "Epoch: 6672 Train Loss: 0.52084 Validation Loss: 0.54835\n",
            "Epoch: 6673 Train Loss: 0.52387 Validation Loss: 0.54878\n",
            "Epoch: 6674 Train Loss: 0.52486 Validation Loss: 0.54865\n",
            "Epoch: 6675 Train Loss: 0.51985 Validation Loss: 0.54841\n",
            "Epoch: 6676 Train Loss: 0.52420 Validation Loss: 0.54926\n",
            "Epoch: 6677 Train Loss: 0.52084 Validation Loss: 0.54846\n",
            "Epoch: 6678 Train Loss: 0.52383 Validation Loss: 0.54838\n",
            "Epoch: 6679 Train Loss: 0.52209 Validation Loss: 0.54909\n",
            "Epoch: 6680 Train Loss: 0.52593 Validation Loss: 0.54854\n",
            "Epoch: 6681 Train Loss: 0.52150 Validation Loss: 0.54812\n",
            "Epoch: 6682 Train Loss: 0.52185 Validation Loss: 0.54914\n",
            "Epoch: 6683 Train Loss: 0.52390 Validation Loss: 0.54924\n",
            "Epoch: 6684 Train Loss: 0.52488 Validation Loss: 0.54831\n",
            "Epoch: 6685 Train Loss: 0.52095 Validation Loss: 0.54829\n",
            "Epoch: 6686 Train Loss: 0.52006 Validation Loss: 0.54876\n",
            "Epoch: 6687 Train Loss: 0.52300 Validation Loss: 0.54860\n",
            "Epoch: 6688 Train Loss: 0.52535 Validation Loss: 0.54931\n",
            "Epoch: 6689 Train Loss: 0.52093 Validation Loss: 0.54832\n",
            "Epoch: 6690 Train Loss: 0.52302 Validation Loss: 0.54863\n",
            "Epoch: 6691 Train Loss: 0.52323 Validation Loss: 0.54806\n",
            "Epoch: 6692 Train Loss: 0.52509 Validation Loss: 0.54849\n",
            "Epoch: 6693 Train Loss: 0.52385 Validation Loss: 0.54885\n",
            "Epoch: 6694 Train Loss: 0.52485 Validation Loss: 0.54872\n",
            "Epoch: 6695 Train Loss: 0.52335 Validation Loss: 0.54825\n",
            "Epoch: 6696 Train Loss: 0.52195 Validation Loss: 0.54899\n",
            "Epoch: 6697 Train Loss: 0.52177 Validation Loss: 0.54908\n",
            "Epoch: 6698 Train Loss: 0.52175 Validation Loss: 0.54860\n",
            "Epoch: 6699 Train Loss: 0.52286 Validation Loss: 0.54881\n",
            "Epoch: 6700 Train Loss: 0.52272 Validation Loss: 0.54859\n",
            "Epoch: 6701 Train Loss: 0.52276 Validation Loss: 0.54836\n",
            "Epoch: 6702 Train Loss: 0.52694 Validation Loss: 0.54856\n",
            "Epoch: 6703 Train Loss: 0.52472 Validation Loss: 0.54846\n",
            "Epoch: 6704 Train Loss: 0.52305 Validation Loss: 0.54804\n",
            "Epoch: 6705 Train Loss: 0.52109 Validation Loss: 0.54825\n",
            "Epoch: 6706 Train Loss: 0.52070 Validation Loss: 0.54882\n",
            "Epoch: 6707 Train Loss: 0.52310 Validation Loss: 0.54876\n",
            "Epoch: 6708 Train Loss: 0.52501 Validation Loss: 0.54887\n",
            "Epoch: 6709 Train Loss: 0.52185 Validation Loss: 0.54833\n",
            "Epoch: 6710 Train Loss: 0.52276 Validation Loss: 0.54864\n",
            "Epoch: 6711 Train Loss: 0.52288 Validation Loss: 0.54838\n",
            "Epoch: 6712 Train Loss: 0.52283 Validation Loss: 0.54867\n",
            "Epoch: 6713 Train Loss: 0.52276 Validation Loss: 0.54900\n",
            "Epoch: 6714 Train Loss: 0.52616 Validation Loss: 0.54931\n",
            "Epoch: 6715 Train Loss: 0.52375 Validation Loss: 0.54819\n",
            "Epoch: 6716 Train Loss: 0.52302 Validation Loss: 0.54838\n",
            "Epoch: 6717 Train Loss: 0.52317 Validation Loss: 0.54881\n",
            "Epoch: 6718 Train Loss: 0.52290 Validation Loss: 0.54839\n",
            "Epoch: 6719 Train Loss: 0.52391 Validation Loss: 0.54865\n",
            "Epoch: 6720 Train Loss: 0.52111 Validation Loss: 0.54835\n",
            "Epoch: 6721 Train Loss: 0.52296 Validation Loss: 0.54860\n",
            "Epoch: 6722 Train Loss: 0.52251 Validation Loss: 0.54974\n",
            "Epoch: 6723 Train Loss: 0.52287 Validation Loss: 0.54833\n",
            "Epoch: 6724 Train Loss: 0.52182 Validation Loss: 0.54871\n",
            "Epoch: 6725 Train Loss: 0.52516 Validation Loss: 0.54896\n",
            "Epoch: 6726 Train Loss: 0.52290 Validation Loss: 0.54831\n",
            "Epoch: 6727 Train Loss: 0.52104 Validation Loss: 0.54808\n",
            "Epoch: 6728 Train Loss: 0.52682 Validation Loss: 0.54907\n",
            "Epoch: 6729 Train Loss: 0.52283 Validation Loss: 0.54890\n",
            "Epoch: 6730 Train Loss: 0.52310 Validation Loss: 0.54886\n",
            "Epoch: 6731 Train Loss: 0.52306 Validation Loss: 0.54847\n",
            "Epoch: 6732 Train Loss: 0.52399 Validation Loss: 0.54817\n",
            "Epoch: 6733 Train Loss: 0.52488 Validation Loss: 0.54906\n",
            "Epoch: 6734 Train Loss: 0.52401 Validation Loss: 0.54897\n",
            "Epoch: 6735 Train Loss: 0.52286 Validation Loss: 0.54831\n",
            "Epoch: 6736 Train Loss: 0.52210 Validation Loss: 0.54869\n",
            "Epoch: 6737 Train Loss: 0.52376 Validation Loss: 0.54842\n",
            "Epoch: 6738 Train Loss: 0.52382 Validation Loss: 0.54863\n",
            "Epoch: 6739 Train Loss: 0.52185 Validation Loss: 0.54868\n",
            "Epoch: 6740 Train Loss: 0.52238 Validation Loss: 0.54898\n",
            "Epoch: 6741 Train Loss: 0.52277 Validation Loss: 0.54808\n",
            "Epoch: 6742 Train Loss: 0.52099 Validation Loss: 0.54841\n",
            "Epoch: 6743 Train Loss: 0.52067 Validation Loss: 0.54907\n",
            "Epoch: 6744 Train Loss: 0.52387 Validation Loss: 0.54938\n",
            "Epoch: 6745 Train Loss: 0.52399 Validation Loss: 0.54893\n",
            "Epoch: 6746 Train Loss: 0.52455 Validation Loss: 0.54822\n",
            "Epoch: 6747 Train Loss: 0.52576 Validation Loss: 0.54835\n",
            "Epoch: 6748 Train Loss: 0.52284 Validation Loss: 0.54848\n",
            "Epoch: 6749 Train Loss: 0.52283 Validation Loss: 0.54850\n",
            "Epoch: 6750 Train Loss: 0.52487 Validation Loss: 0.54869\n",
            "Epoch: 6751 Train Loss: 0.51983 Validation Loss: 0.54785\n",
            "Epoch: 6752 Train Loss: 0.52919 Validation Loss: 0.55082\n",
            "Epoch: 6753 Train Loss: 0.52202 Validation Loss: 0.54803\n",
            "Epoch: 6754 Train Loss: 0.52290 Validation Loss: 0.54853\n",
            "Epoch: 6755 Train Loss: 0.52131 Validation Loss: 0.54892\n",
            "Epoch: 6756 Train Loss: 0.52195 Validation Loss: 0.54829\n",
            "Epoch: 6757 Train Loss: 0.52213 Validation Loss: 0.54823\n",
            "Epoch: 6758 Train Loss: 0.52287 Validation Loss: 0.54904\n",
            "Epoch: 6759 Train Loss: 0.52396 Validation Loss: 0.54865\n",
            "Epoch: 6760 Train Loss: 0.52300 Validation Loss: 0.54917\n",
            "Epoch: 6761 Train Loss: 0.52191 Validation Loss: 0.54824\n",
            "Epoch: 6762 Train Loss: 0.52393 Validation Loss: 0.54882\n",
            "Epoch: 6763 Train Loss: 0.52081 Validation Loss: 0.54837\n",
            "Epoch: 6764 Train Loss: 0.52070 Validation Loss: 0.54862\n",
            "Epoch: 6765 Train Loss: 0.52284 Validation Loss: 0.54901\n",
            "Epoch: 6766 Train Loss: 0.52485 Validation Loss: 0.54870\n",
            "Epoch: 6767 Train Loss: 0.52352 Validation Loss: 0.54846\n",
            "Epoch: 6768 Train Loss: 0.51983 Validation Loss: 0.54862\n",
            "Epoch: 6769 Train Loss: 0.52390 Validation Loss: 0.54914\n",
            "Epoch: 6770 Train Loss: 0.52382 Validation Loss: 0.54878\n",
            "Epoch: 6771 Train Loss: 0.52181 Validation Loss: 0.54889\n",
            "Epoch: 6772 Train Loss: 0.52498 Validation Loss: 0.54873\n",
            "Epoch: 6773 Train Loss: 0.52374 Validation Loss: 0.54843\n",
            "Epoch: 6774 Train Loss: 0.52303 Validation Loss: 0.54831\n",
            "Epoch: 6775 Train Loss: 0.51996 Validation Loss: 0.54845\n",
            "Epoch: 6776 Train Loss: 0.52332 Validation Loss: 0.54880\n",
            "Epoch: 6777 Train Loss: 0.52609 Validation Loss: 0.54875\n",
            "Epoch: 6778 Train Loss: 0.52186 Validation Loss: 0.54841\n",
            "Epoch: 6779 Train Loss: 0.52099 Validation Loss: 0.54826\n",
            "Epoch: 6780 Train Loss: 0.52178 Validation Loss: 0.54869\n",
            "Epoch: 6781 Train Loss: 0.52380 Validation Loss: 0.54872\n",
            "Epoch: 6782 Train Loss: 0.52465 Validation Loss: 0.54846\n",
            "Epoch: 6783 Train Loss: 0.52114 Validation Loss: 0.54953\n",
            "Epoch: 6784 Train Loss: 0.52506 Validation Loss: 0.54914\n",
            "Epoch: 6785 Train Loss: 0.52280 Validation Loss: 0.54845\n",
            "Epoch: 6786 Train Loss: 0.52278 Validation Loss: 0.54865\n",
            "Epoch: 6787 Train Loss: 0.52276 Validation Loss: 0.54861\n",
            "Epoch: 6788 Train Loss: 0.52413 Validation Loss: 0.54882\n",
            "Epoch: 6789 Train Loss: 0.52312 Validation Loss: 0.54794\n",
            "Epoch: 6790 Train Loss: 0.52313 Validation Loss: 0.54868\n",
            "Epoch: 6791 Train Loss: 0.52289 Validation Loss: 0.54849\n",
            "Epoch: 6792 Train Loss: 0.52563 Validation Loss: 0.54896\n",
            "Epoch: 6793 Train Loss: 0.52376 Validation Loss: 0.54813\n",
            "Epoch: 6794 Train Loss: 0.52137 Validation Loss: 0.54815\n",
            "Epoch: 6795 Train Loss: 0.52510 Validation Loss: 0.54931\n",
            "Epoch: 6796 Train Loss: 0.52313 Validation Loss: 0.54878\n",
            "Epoch: 6797 Train Loss: 0.52106 Validation Loss: 0.54822\n",
            "Epoch: 6798 Train Loss: 0.52294 Validation Loss: 0.54905\n",
            "Epoch: 6799 Train Loss: 0.52223 Validation Loss: 0.54830\n",
            "Epoch: 6800 Train Loss: 0.52367 Validation Loss: 0.54919\n",
            "Epoch: 6801 Train Loss: 0.52381 Validation Loss: 0.54901\n",
            "Epoch: 6802 Train Loss: 0.52366 Validation Loss: 0.54851\n",
            "Epoch: 6803 Train Loss: 0.52500 Validation Loss: 0.54879\n",
            "Epoch: 6804 Train Loss: 0.52098 Validation Loss: 0.54809\n",
            "Epoch: 6805 Train Loss: 0.52291 Validation Loss: 0.54912\n",
            "Epoch: 6806 Train Loss: 0.52281 Validation Loss: 0.54885\n",
            "Epoch: 6807 Train Loss: 0.52411 Validation Loss: 0.54858\n",
            "Epoch: 6808 Train Loss: 0.52388 Validation Loss: 0.54901\n",
            "Epoch: 6809 Train Loss: 0.52491 Validation Loss: 0.54852\n",
            "Epoch: 6810 Train Loss: 0.52688 Validation Loss: 0.54844\n",
            "Epoch: 6811 Train Loss: 0.52798 Validation Loss: 0.54876\n",
            "Epoch: 6812 Train Loss: 0.52277 Validation Loss: 0.54831\n",
            "Epoch: 6813 Train Loss: 0.52106 Validation Loss: 0.54836\n",
            "Epoch: 6814 Train Loss: 0.52596 Validation Loss: 0.54875\n",
            "Epoch: 6815 Train Loss: 0.51999 Validation Loss: 0.54811\n",
            "Epoch: 6816 Train Loss: 0.52390 Validation Loss: 0.54851\n",
            "Epoch: 6817 Train Loss: 0.52309 Validation Loss: 0.54852\n",
            "Epoch: 6818 Train Loss: 0.52598 Validation Loss: 0.54934\n",
            "Epoch: 6819 Train Loss: 0.52107 Validation Loss: 0.54830\n",
            "Epoch: 6820 Train Loss: 0.52301 Validation Loss: 0.54926\n",
            "Epoch: 6821 Train Loss: 0.52390 Validation Loss: 0.54837\n",
            "Epoch: 6822 Train Loss: 0.52210 Validation Loss: 0.54822\n",
            "Epoch: 6823 Train Loss: 0.52270 Validation Loss: 0.54909\n",
            "Epoch: 6824 Train Loss: 0.52491 Validation Loss: 0.54906\n",
            "Epoch: 6825 Train Loss: 0.52203 Validation Loss: 0.54916\n",
            "Epoch: 6826 Train Loss: 0.52091 Validation Loss: 0.54824\n",
            "Epoch: 6827 Train Loss: 0.52269 Validation Loss: 0.54870\n",
            "Epoch: 6828 Train Loss: 0.52330 Validation Loss: 0.54919\n",
            "Epoch: 6829 Train Loss: 0.52191 Validation Loss: 0.54859\n",
            "Epoch: 6830 Train Loss: 0.52290 Validation Loss: 0.54847\n",
            "Epoch: 6831 Train Loss: 0.52310 Validation Loss: 0.54883\n",
            "Epoch: 6832 Train Loss: 0.51979 Validation Loss: 0.54830\n",
            "Epoch: 6833 Train Loss: 0.52171 Validation Loss: 0.54868\n",
            "Epoch: 6834 Train Loss: 0.52413 Validation Loss: 0.54862\n",
            "Epoch: 6835 Train Loss: 0.52323 Validation Loss: 0.54836\n",
            "Epoch: 6836 Train Loss: 0.52429 Validation Loss: 0.54936\n",
            "Epoch: 6837 Train Loss: 0.52596 Validation Loss: 0.54880\n",
            "Epoch: 6838 Train Loss: 0.52176 Validation Loss: 0.54798\n",
            "Epoch: 6839 Train Loss: 0.52397 Validation Loss: 0.54877\n",
            "Epoch: 6840 Train Loss: 0.52099 Validation Loss: 0.54899\n",
            "Epoch: 6841 Train Loss: 0.52447 Validation Loss: 0.54910\n",
            "Epoch: 6842 Train Loss: 0.52498 Validation Loss: 0.54884\n",
            "Epoch: 6843 Train Loss: 0.52620 Validation Loss: 0.54910\n",
            "Epoch: 6844 Train Loss: 0.52083 Validation Loss: 0.54816\n",
            "Epoch: 6845 Train Loss: 0.52415 Validation Loss: 0.54814\n",
            "Epoch: 6846 Train Loss: 0.52404 Validation Loss: 0.54917\n",
            "Epoch: 6847 Train Loss: 0.52371 Validation Loss: 0.54857\n",
            "Epoch: 6848 Train Loss: 0.52394 Validation Loss: 0.54886\n",
            "Epoch: 6849 Train Loss: 0.52392 Validation Loss: 0.54823\n",
            "Epoch: 6850 Train Loss: 0.52398 Validation Loss: 0.54897\n",
            "Epoch: 6851 Train Loss: 0.52423 Validation Loss: 0.54861\n",
            "Epoch: 6852 Train Loss: 0.52185 Validation Loss: 0.54882\n",
            "Epoch: 6853 Train Loss: 0.52587 Validation Loss: 0.54875\n",
            "Epoch: 6854 Train Loss: 0.52382 Validation Loss: 0.54872\n",
            "Epoch: 6855 Train Loss: 0.52091 Validation Loss: 0.54837\n",
            "Epoch: 6856 Train Loss: 0.52621 Validation Loss: 0.54937\n",
            "Epoch: 6857 Train Loss: 0.52376 Validation Loss: 0.54840\n",
            "Epoch: 6858 Train Loss: 0.52194 Validation Loss: 0.54826\n",
            "Epoch: 6859 Train Loss: 0.52490 Validation Loss: 0.54898\n",
            "Epoch: 6860 Train Loss: 0.52175 Validation Loss: 0.54836\n",
            "Epoch: 6861 Train Loss: 0.52305 Validation Loss: 0.54898\n",
            "Epoch: 6862 Train Loss: 0.52237 Validation Loss: 0.54818\n",
            "Epoch: 6863 Train Loss: 0.52466 Validation Loss: 0.54938\n",
            "Epoch: 6864 Train Loss: 0.52388 Validation Loss: 0.54871\n",
            "Epoch: 6865 Train Loss: 0.52180 Validation Loss: 0.54859\n",
            "Epoch: 6866 Train Loss: 0.52602 Validation Loss: 0.54928\n",
            "Epoch: 6867 Train Loss: 0.52479 Validation Loss: 0.54839\n",
            "Epoch: 6868 Train Loss: 0.52485 Validation Loss: 0.54865\n",
            "Epoch: 6869 Train Loss: 0.52487 Validation Loss: 0.54830\n",
            "Epoch: 6870 Train Loss: 0.52302 Validation Loss: 0.54867\n",
            "Epoch: 6871 Train Loss: 0.52147 Validation Loss: 0.54890\n",
            "Epoch: 6872 Train Loss: 0.52384 Validation Loss: 0.54912\n",
            "Epoch: 6873 Train Loss: 0.52402 Validation Loss: 0.54837\n",
            "Epoch: 6874 Train Loss: 0.52489 Validation Loss: 0.54881\n",
            "Epoch: 6875 Train Loss: 0.52303 Validation Loss: 0.54815\n",
            "Epoch: 6876 Train Loss: 0.52279 Validation Loss: 0.54834\n",
            "Epoch: 6877 Train Loss: 0.52610 Validation Loss: 0.54960\n",
            "Epoch: 6878 Train Loss: 0.52291 Validation Loss: 0.54865\n",
            "Epoch: 6879 Train Loss: 0.52474 Validation Loss: 0.54807\n",
            "Epoch: 6880 Train Loss: 0.52400 Validation Loss: 0.54840\n",
            "Epoch: 6881 Train Loss: 0.52419 Validation Loss: 0.54862\n",
            "Epoch: 6882 Train Loss: 0.52342 Validation Loss: 0.54816\n",
            "Epoch: 6883 Train Loss: 0.52507 Validation Loss: 0.54918\n",
            "Epoch: 6884 Train Loss: 0.52591 Validation Loss: 0.54896\n",
            "Epoch: 6885 Train Loss: 0.52488 Validation Loss: 0.54802\n",
            "Epoch: 6886 Train Loss: 0.52387 Validation Loss: 0.54852\n",
            "Epoch: 6887 Train Loss: 0.52209 Validation Loss: 0.54809\n",
            "Epoch: 6888 Train Loss: 0.52448 Validation Loss: 0.54919\n",
            "Epoch: 6889 Train Loss: 0.52281 Validation Loss: 0.54842\n",
            "Epoch: 6890 Train Loss: 0.52277 Validation Loss: 0.54865\n",
            "Epoch: 6891 Train Loss: 0.52184 Validation Loss: 0.54845\n",
            "Epoch: 6892 Train Loss: 0.52177 Validation Loss: 0.54883\n",
            "Epoch: 6893 Train Loss: 0.52427 Validation Loss: 0.54884\n",
            "Epoch: 6894 Train Loss: 0.52495 Validation Loss: 0.54866\n",
            "Epoch: 6895 Train Loss: 0.52191 Validation Loss: 0.54807\n",
            "Epoch: 6896 Train Loss: 0.52601 Validation Loss: 0.54837\n",
            "Epoch: 6897 Train Loss: 0.52713 Validation Loss: 0.54885\n",
            "Epoch: 6898 Train Loss: 0.52223 Validation Loss: 0.54839\n",
            "Epoch: 6899 Train Loss: 0.52149 Validation Loss: 0.54846\n",
            "Epoch: 6900 Train Loss: 0.52532 Validation Loss: 0.54891\n",
            "Epoch: 6901 Train Loss: 0.52715 Validation Loss: 0.54851\n",
            "Epoch: 6902 Train Loss: 0.52189 Validation Loss: 0.54831\n",
            "Epoch: 6903 Train Loss: 0.52463 Validation Loss: 0.54923\n",
            "Epoch: 6904 Train Loss: 0.52360 Validation Loss: 0.54840\n",
            "Epoch: 6905 Train Loss: 0.52083 Validation Loss: 0.54803\n",
            "Epoch: 6906 Train Loss: 0.52186 Validation Loss: 0.54842\n",
            "Epoch: 6907 Train Loss: 0.52388 Validation Loss: 0.54897\n",
            "Epoch: 6908 Train Loss: 0.52175 Validation Loss: 0.54836\n",
            "Epoch: 6909 Train Loss: 0.52390 Validation Loss: 0.54897\n",
            "Epoch: 6910 Train Loss: 0.52624 Validation Loss: 0.54862\n",
            "Epoch: 6911 Train Loss: 0.52173 Validation Loss: 0.54813\n",
            "Epoch: 6912 Train Loss: 0.52197 Validation Loss: 0.54852\n",
            "Epoch: 6913 Train Loss: 0.52291 Validation Loss: 0.54888\n",
            "Epoch: 6914 Train Loss: 0.52302 Validation Loss: 0.54787\n",
            "Epoch: 6915 Train Loss: 0.52519 Validation Loss: 0.54910\n",
            "Epoch: 6916 Train Loss: 0.52486 Validation Loss: 0.54868\n",
            "Epoch: 6917 Train Loss: 0.52202 Validation Loss: 0.54866\n",
            "Epoch: 6918 Train Loss: 0.52178 Validation Loss: 0.54848\n",
            "Epoch: 6919 Train Loss: 0.52187 Validation Loss: 0.54900\n",
            "Epoch: 6920 Train Loss: 0.52024 Validation Loss: 0.54850\n",
            "Epoch: 6921 Train Loss: 0.52487 Validation Loss: 0.54928\n",
            "Epoch: 6922 Train Loss: 0.52299 Validation Loss: 0.54874\n",
            "Epoch: 6923 Train Loss: 0.52196 Validation Loss: 0.54818\n",
            "Epoch: 6924 Train Loss: 0.52415 Validation Loss: 0.54887\n",
            "Epoch: 6925 Train Loss: 0.52298 Validation Loss: 0.54859\n",
            "Epoch: 6926 Train Loss: 0.52277 Validation Loss: 0.54850\n",
            "Epoch: 6927 Train Loss: 0.52901 Validation Loss: 0.54904\n",
            "Epoch: 6928 Train Loss: 0.52480 Validation Loss: 0.54852\n",
            "Epoch: 6929 Train Loss: 0.52090 Validation Loss: 0.54795\n",
            "Epoch: 6930 Train Loss: 0.52431 Validation Loss: 0.54869\n",
            "Epoch: 6931 Train Loss: 0.52480 Validation Loss: 0.54853\n",
            "Epoch: 6932 Train Loss: 0.52279 Validation Loss: 0.54837\n",
            "Epoch: 6933 Train Loss: 0.52121 Validation Loss: 0.54875\n",
            "Epoch: 6934 Train Loss: 0.52794 Validation Loss: 0.54884\n",
            "Epoch: 6935 Train Loss: 0.52506 Validation Loss: 0.54818\n",
            "Epoch: 6936 Train Loss: 0.52293 Validation Loss: 0.54827\n",
            "Epoch: 6937 Train Loss: 0.52512 Validation Loss: 0.54860\n",
            "Epoch: 6938 Train Loss: 0.52090 Validation Loss: 0.54841\n",
            "Epoch: 6939 Train Loss: 0.52071 Validation Loss: 0.54916\n",
            "Epoch: 6940 Train Loss: 0.52389 Validation Loss: 0.54916\n",
            "Epoch: 6941 Train Loss: 0.52574 Validation Loss: 0.54863\n",
            "Epoch: 6942 Train Loss: 0.52350 Validation Loss: 0.54849\n",
            "Epoch: 6943 Train Loss: 0.52497 Validation Loss: 0.54908\n",
            "Epoch: 6944 Train Loss: 0.52326 Validation Loss: 0.54830\n",
            "Epoch: 6945 Train Loss: 0.52337 Validation Loss: 0.54813\n",
            "Epoch: 6946 Train Loss: 0.52469 Validation Loss: 0.54916\n",
            "Epoch: 6947 Train Loss: 0.52233 Validation Loss: 0.54937\n",
            "Epoch: 6948 Train Loss: 0.52367 Validation Loss: 0.54836\n",
            "Epoch: 6949 Train Loss: 0.52349 Validation Loss: 0.54816\n",
            "Epoch: 6950 Train Loss: 0.52485 Validation Loss: 0.54921\n",
            "Epoch: 6951 Train Loss: 0.52191 Validation Loss: 0.54886\n",
            "Epoch: 6952 Train Loss: 0.52630 Validation Loss: 0.54819\n",
            "Epoch: 6953 Train Loss: 0.52301 Validation Loss: 0.54835\n",
            "Epoch: 6954 Train Loss: 0.52276 Validation Loss: 0.54895\n",
            "Epoch: 6955 Train Loss: 0.52501 Validation Loss: 0.54915\n",
            "Epoch: 6956 Train Loss: 0.52079 Validation Loss: 0.54864\n",
            "Epoch: 6957 Train Loss: 0.52181 Validation Loss: 0.54872\n",
            "Epoch: 6958 Train Loss: 0.52489 Validation Loss: 0.54851\n",
            "Epoch: 6959 Train Loss: 0.52300 Validation Loss: 0.54910\n",
            "Epoch: 6960 Train Loss: 0.52323 Validation Loss: 0.54806\n",
            "Epoch: 6961 Train Loss: 0.51988 Validation Loss: 0.54846\n",
            "Epoch: 6962 Train Loss: 0.52089 Validation Loss: 0.54920\n",
            "Epoch: 6963 Train Loss: 0.52330 Validation Loss: 0.54947\n",
            "Epoch: 6964 Train Loss: 0.52211 Validation Loss: 0.54838\n",
            "Epoch: 6965 Train Loss: 0.52485 Validation Loss: 0.54884\n",
            "Epoch: 6966 Train Loss: 0.52967 Validation Loss: 0.54946\n",
            "Epoch: 6967 Train Loss: 0.52511 Validation Loss: 0.54797\n",
            "Epoch: 6968 Train Loss: 0.52495 Validation Loss: 0.54838\n",
            "Epoch: 6969 Train Loss: 0.52333 Validation Loss: 0.54816\n",
            "Epoch: 6970 Train Loss: 0.52414 Validation Loss: 0.54918\n",
            "Epoch: 6971 Train Loss: 0.52077 Validation Loss: 0.54892\n",
            "Epoch: 6972 Train Loss: 0.52513 Validation Loss: 0.54859\n",
            "Epoch: 6973 Train Loss: 0.52188 Validation Loss: 0.54842\n",
            "Epoch: 6974 Train Loss: 0.52127 Validation Loss: 0.54854\n",
            "Epoch: 6975 Train Loss: 0.52104 Validation Loss: 0.54834\n",
            "Epoch: 6976 Train Loss: 0.52245 Validation Loss: 0.54888\n",
            "Epoch: 6977 Train Loss: 0.52445 Validation Loss: 0.54852\n",
            "Epoch: 6978 Train Loss: 0.52374 Validation Loss: 0.54895\n",
            "Epoch: 6979 Train Loss: 0.52325 Validation Loss: 0.54924\n",
            "Epoch: 6980 Train Loss: 0.52394 Validation Loss: 0.54835\n",
            "Epoch: 6981 Train Loss: 0.52499 Validation Loss: 0.54816\n",
            "Epoch: 6982 Train Loss: 0.52377 Validation Loss: 0.54834\n",
            "Epoch: 6983 Train Loss: 0.52622 Validation Loss: 0.54896\n",
            "Epoch: 6984 Train Loss: 0.52048 Validation Loss: 0.54778\n",
            "Epoch: 6985 Train Loss: 0.52665 Validation Loss: 0.54942\n",
            "Epoch: 6986 Train Loss: 0.52564 Validation Loss: 0.54824\n",
            "Epoch: 6987 Train Loss: 0.52391 Validation Loss: 0.54828\n",
            "Epoch: 6988 Train Loss: 0.52633 Validation Loss: 0.54969\n",
            "Epoch: 6989 Train Loss: 0.52168 Validation Loss: 0.54822\n",
            "Epoch: 6990 Train Loss: 0.52394 Validation Loss: 0.54879\n",
            "Epoch: 6991 Train Loss: 0.52324 Validation Loss: 0.54796\n",
            "Epoch: 6992 Train Loss: 0.52176 Validation Loss: 0.54891\n",
            "Epoch: 6993 Train Loss: 0.52298 Validation Loss: 0.54863\n",
            "Epoch: 6994 Train Loss: 0.52388 Validation Loss: 0.54884\n",
            "Epoch: 6995 Train Loss: 0.52746 Validation Loss: 0.54906\n",
            "Epoch: 6996 Train Loss: 0.51908 Validation Loss: 0.54816\n",
            "Epoch: 6997 Train Loss: 0.52643 Validation Loss: 0.54867\n",
            "Epoch: 6998 Train Loss: 0.52388 Validation Loss: 0.54826\n",
            "Epoch: 6999 Train Loss: 0.52096 Validation Loss: 0.54825\n",
            "Epoch: 7000 Train Loss: 0.52069 Validation Loss: 0.54933\n",
            "Epoch: 7001 Train Loss: 0.52544 Validation Loss: 0.54980\n",
            "Epoch: 7002 Train Loss: 0.52309 Validation Loss: 0.54811\n",
            "Epoch: 7003 Train Loss: 0.52724 Validation Loss: 0.54898\n",
            "Epoch: 7004 Train Loss: 0.52282 Validation Loss: 0.54827\n",
            "Epoch: 7005 Train Loss: 0.52385 Validation Loss: 0.54835\n",
            "Epoch: 7006 Train Loss: 0.52405 Validation Loss: 0.54841\n",
            "Epoch: 7007 Train Loss: 0.52175 Validation Loss: 0.54853\n",
            "Epoch: 7008 Train Loss: 0.52182 Validation Loss: 0.54838\n",
            "Epoch: 7009 Train Loss: 0.52131 Validation Loss: 0.54837\n",
            "Epoch: 7010 Train Loss: 0.52351 Validation Loss: 0.54924\n",
            "Epoch: 7011 Train Loss: 0.52232 Validation Loss: 0.54816\n",
            "Epoch: 7012 Train Loss: 0.52242 Validation Loss: 0.54932\n",
            "Epoch: 7013 Train Loss: 0.52656 Validation Loss: 0.54928\n",
            "Epoch: 7014 Train Loss: 0.52271 Validation Loss: 0.54762\n",
            "Epoch: 7015 Train Loss: 0.52401 Validation Loss: 0.54901\n",
            "Epoch: 7016 Train Loss: 0.52078 Validation Loss: 0.54876\n",
            "Epoch: 7017 Train Loss: 0.52284 Validation Loss: 0.54868\n",
            "Epoch: 7018 Train Loss: 0.52192 Validation Loss: 0.54878\n",
            "Epoch: 7019 Train Loss: 0.52288 Validation Loss: 0.54812\n",
            "Epoch: 7020 Train Loss: 0.52218 Validation Loss: 0.54898\n",
            "Epoch: 7021 Train Loss: 0.52095 Validation Loss: 0.54844\n",
            "Epoch: 7022 Train Loss: 0.52127 Validation Loss: 0.54866\n",
            "Epoch: 7023 Train Loss: 0.52397 Validation Loss: 0.54878\n",
            "Epoch: 7024 Train Loss: 0.52186 Validation Loss: 0.54850\n",
            "Epoch: 7025 Train Loss: 0.52275 Validation Loss: 0.54891\n",
            "Epoch: 7026 Train Loss: 0.52180 Validation Loss: 0.54880\n",
            "Epoch: 7027 Train Loss: 0.52360 Validation Loss: 0.54890\n",
            "Epoch: 7028 Train Loss: 0.52385 Validation Loss: 0.54806\n",
            "Epoch: 7029 Train Loss: 0.52303 Validation Loss: 0.54908\n",
            "Epoch: 7030 Train Loss: 0.52311 Validation Loss: 0.54843\n",
            "Epoch: 7031 Train Loss: 0.52397 Validation Loss: 0.54919\n",
            "Epoch: 7032 Train Loss: 0.52379 Validation Loss: 0.54874\n",
            "Epoch: 7033 Train Loss: 0.52513 Validation Loss: 0.54864\n",
            "Epoch: 7034 Train Loss: 0.52201 Validation Loss: 0.54809\n",
            "Epoch: 7035 Train Loss: 0.52290 Validation Loss: 0.54889\n",
            "Epoch: 7036 Train Loss: 0.52189 Validation Loss: 0.54912\n",
            "Epoch: 7037 Train Loss: 0.52200 Validation Loss: 0.54851\n",
            "Epoch: 7038 Train Loss: 0.52275 Validation Loss: 0.54826\n",
            "Epoch: 7039 Train Loss: 0.52281 Validation Loss: 0.54851\n",
            "Epoch: 7040 Train Loss: 0.52386 Validation Loss: 0.54883\n",
            "Epoch: 7041 Train Loss: 0.52183 Validation Loss: 0.54877\n",
            "Epoch: 7042 Train Loss: 0.52206 Validation Loss: 0.54879\n",
            "Epoch: 7043 Train Loss: 0.52404 Validation Loss: 0.54846\n",
            "Epoch: 7044 Train Loss: 0.52130 Validation Loss: 0.54789\n",
            "Epoch: 7045 Train Loss: 0.52171 Validation Loss: 0.54880\n",
            "Epoch: 7046 Train Loss: 0.52405 Validation Loss: 0.54858\n",
            "Epoch: 7047 Train Loss: 0.52437 Validation Loss: 0.54953\n",
            "Epoch: 7048 Train Loss: 0.52375 Validation Loss: 0.54838\n",
            "Epoch: 7049 Train Loss: 0.52544 Validation Loss: 0.54903\n",
            "Epoch: 7050 Train Loss: 0.52515 Validation Loss: 0.54811\n",
            "Epoch: 7051 Train Loss: 0.52191 Validation Loss: 0.54812\n",
            "Epoch: 7052 Train Loss: 0.52080 Validation Loss: 0.54873\n",
            "Epoch: 7053 Train Loss: 0.52383 Validation Loss: 0.54934\n",
            "Epoch: 7054 Train Loss: 0.52181 Validation Loss: 0.54860\n",
            "Epoch: 7055 Train Loss: 0.52591 Validation Loss: 0.54906\n",
            "Epoch: 7056 Train Loss: 0.52378 Validation Loss: 0.54852\n",
            "Epoch: 7057 Train Loss: 0.52685 Validation Loss: 0.54818\n",
            "Epoch: 7058 Train Loss: 0.52404 Validation Loss: 0.54811\n",
            "Epoch: 7059 Train Loss: 0.52292 Validation Loss: 0.54812\n",
            "Epoch: 7060 Train Loss: 0.52200 Validation Loss: 0.54860\n",
            "Epoch: 7061 Train Loss: 0.52075 Validation Loss: 0.54864\n",
            "Epoch: 7062 Train Loss: 0.52715 Validation Loss: 0.54892\n",
            "Epoch: 7063 Train Loss: 0.52305 Validation Loss: 0.54835\n",
            "Epoch: 7064 Train Loss: 0.52178 Validation Loss: 0.54867\n",
            "Epoch: 7065 Train Loss: 0.52317 Validation Loss: 0.54866\n",
            "Epoch: 7066 Train Loss: 0.52380 Validation Loss: 0.54876\n",
            "Epoch: 7067 Train Loss: 0.52422 Validation Loss: 0.54899\n",
            "Epoch: 7068 Train Loss: 0.52396 Validation Loss: 0.54804\n",
            "Epoch: 7069 Train Loss: 0.52488 Validation Loss: 0.54932\n",
            "Epoch: 7070 Train Loss: 0.52477 Validation Loss: 0.54870\n",
            "Epoch: 7071 Train Loss: 0.52510 Validation Loss: 0.54862\n",
            "Epoch: 7072 Train Loss: 0.52388 Validation Loss: 0.54892\n",
            "Epoch: 7073 Train Loss: 0.52316 Validation Loss: 0.54824\n",
            "Epoch: 7074 Train Loss: 0.52858 Validation Loss: 0.54936\n",
            "Epoch: 7075 Train Loss: 0.52265 Validation Loss: 0.54786\n",
            "Epoch: 7076 Train Loss: 0.52363 Validation Loss: 0.54924\n",
            "Epoch: 7077 Train Loss: 0.52376 Validation Loss: 0.54805\n",
            "Epoch: 7078 Train Loss: 0.52472 Validation Loss: 0.54884\n",
            "Epoch: 7079 Train Loss: 0.52380 Validation Loss: 0.54891\n",
            "Epoch: 7080 Train Loss: 0.52589 Validation Loss: 0.54910\n",
            "Epoch: 7081 Train Loss: 0.52382 Validation Loss: 0.54825\n",
            "Epoch: 7082 Train Loss: 0.52102 Validation Loss: 0.54805\n",
            "Epoch: 7083 Train Loss: 0.52311 Validation Loss: 0.54931\n",
            "Epoch: 7084 Train Loss: 0.52313 Validation Loss: 0.54861\n",
            "Epoch: 7085 Train Loss: 0.52418 Validation Loss: 0.54907\n",
            "Epoch: 7086 Train Loss: 0.52311 Validation Loss: 0.54836\n",
            "Epoch: 7087 Train Loss: 0.52486 Validation Loss: 0.54895\n",
            "Epoch: 7088 Train Loss: 0.52198 Validation Loss: 0.54830\n",
            "Epoch: 7089 Train Loss: 0.52387 Validation Loss: 0.54849\n",
            "Epoch: 7090 Train Loss: 0.52291 Validation Loss: 0.54870\n",
            "Epoch: 7091 Train Loss: 0.52412 Validation Loss: 0.54863\n",
            "Epoch: 7092 Train Loss: 0.52180 Validation Loss: 0.54895\n",
            "Epoch: 7093 Train Loss: 0.52178 Validation Loss: 0.54829\n",
            "Epoch: 7094 Train Loss: 0.52407 Validation Loss: 0.54887\n",
            "Epoch: 7095 Train Loss: 0.52397 Validation Loss: 0.54814\n",
            "Epoch: 7096 Train Loss: 0.52311 Validation Loss: 0.54876\n",
            "Epoch: 7097 Train Loss: 0.52230 Validation Loss: 0.54796\n",
            "Epoch: 7098 Train Loss: 0.52107 Validation Loss: 0.54872\n",
            "Epoch: 7099 Train Loss: 0.52431 Validation Loss: 0.54844\n",
            "Epoch: 7100 Train Loss: 0.52291 Validation Loss: 0.54885\n",
            "Epoch: 7101 Train Loss: 0.52407 Validation Loss: 0.54896\n",
            "Epoch: 7102 Train Loss: 0.52280 Validation Loss: 0.54893\n",
            "Epoch: 7103 Train Loss: 0.52478 Validation Loss: 0.54844\n",
            "Epoch: 7104 Train Loss: 0.52376 Validation Loss: 0.54849\n",
            "Epoch: 7105 Train Loss: 0.52009 Validation Loss: 0.54892\n",
            "Epoch: 7106 Train Loss: 0.52789 Validation Loss: 0.54867\n",
            "Epoch: 7107 Train Loss: 0.52177 Validation Loss: 0.54831\n",
            "Epoch: 7108 Train Loss: 0.52086 Validation Loss: 0.54822\n",
            "Epoch: 7109 Train Loss: 0.52484 Validation Loss: 0.54875\n",
            "Epoch: 7110 Train Loss: 0.52181 Validation Loss: 0.54885\n",
            "Epoch: 7111 Train Loss: 0.52193 Validation Loss: 0.54834\n",
            "Epoch: 7112 Train Loss: 0.52389 Validation Loss: 0.54875\n",
            "Epoch: 7113 Train Loss: 0.52078 Validation Loss: 0.54869\n",
            "Epoch: 7114 Train Loss: 0.52400 Validation Loss: 0.54865\n",
            "Epoch: 7115 Train Loss: 0.52492 Validation Loss: 0.54827\n",
            "Epoch: 7116 Train Loss: 0.52413 Validation Loss: 0.54940\n",
            "Epoch: 7117 Train Loss: 0.52283 Validation Loss: 0.54823\n",
            "Epoch: 7118 Train Loss: 0.52380 Validation Loss: 0.54866\n",
            "Epoch: 7119 Train Loss: 0.52823 Validation Loss: 0.54898\n",
            "Epoch: 7120 Train Loss: 0.52502 Validation Loss: 0.54822\n",
            "Epoch: 7121 Train Loss: 0.52092 Validation Loss: 0.54809\n",
            "Epoch: 7122 Train Loss: 0.52435 Validation Loss: 0.54914\n",
            "Epoch: 7123 Train Loss: 0.52591 Validation Loss: 0.54860\n",
            "Epoch: 7124 Train Loss: 0.52124 Validation Loss: 0.54827\n",
            "Epoch: 7125 Train Loss: 0.52380 Validation Loss: 0.54856\n",
            "Epoch: 7126 Train Loss: 0.52075 Validation Loss: 0.54889\n",
            "Epoch: 7127 Train Loss: 0.52495 Validation Loss: 0.54914\n",
            "Epoch: 7128 Train Loss: 0.52534 Validation Loss: 0.54930\n",
            "Epoch: 7129 Train Loss: 0.52383 Validation Loss: 0.54798\n",
            "Epoch: 7130 Train Loss: 0.52299 Validation Loss: 0.54847\n",
            "Epoch: 7131 Train Loss: 0.52294 Validation Loss: 0.54889\n",
            "Epoch: 7132 Train Loss: 0.52384 Validation Loss: 0.54838\n",
            "Epoch: 7133 Train Loss: 0.52096 Validation Loss: 0.54866\n",
            "Epoch: 7134 Train Loss: 0.52422 Validation Loss: 0.54841\n",
            "Epoch: 7135 Train Loss: 0.52273 Validation Loss: 0.54881\n",
            "Epoch: 7136 Train Loss: 0.52302 Validation Loss: 0.54876\n",
            "Epoch: 7137 Train Loss: 0.52180 Validation Loss: 0.54840\n",
            "Epoch: 7138 Train Loss: 0.52004 Validation Loss: 0.54820\n",
            "Epoch: 7139 Train Loss: 0.52059 Validation Loss: 0.54915\n",
            "Epoch: 7140 Train Loss: 0.51985 Validation Loss: 0.54921\n",
            "Epoch: 7141 Train Loss: 0.52293 Validation Loss: 0.54850\n",
            "Epoch: 7142 Train Loss: 0.52292 Validation Loss: 0.54906\n",
            "Epoch: 7143 Train Loss: 0.52013 Validation Loss: 0.54896\n",
            "Epoch: 7144 Train Loss: 0.52343 Validation Loss: 0.54845\n",
            "Epoch: 7145 Train Loss: 0.52838 Validation Loss: 0.54988\n",
            "Epoch: 7146 Train Loss: 0.52204 Validation Loss: 0.54815\n",
            "Epoch: 7147 Train Loss: 0.52287 Validation Loss: 0.54897\n",
            "Epoch: 7148 Train Loss: 0.52294 Validation Loss: 0.54842\n",
            "Epoch: 7149 Train Loss: 0.52192 Validation Loss: 0.54857\n",
            "Epoch: 7150 Train Loss: 0.52303 Validation Loss: 0.54908\n",
            "Epoch: 7151 Train Loss: 0.52293 Validation Loss: 0.54867\n",
            "Epoch: 7152 Train Loss: 0.52080 Validation Loss: 0.54847\n",
            "Epoch: 7153 Train Loss: 0.52222 Validation Loss: 0.54842\n",
            "Epoch: 7154 Train Loss: 0.51999 Validation Loss: 0.54902\n",
            "Epoch: 7155 Train Loss: 0.52023 Validation Loss: 0.54936\n",
            "Epoch: 7156 Train Loss: 0.52378 Validation Loss: 0.54867\n",
            "Epoch: 7157 Train Loss: 0.52592 Validation Loss: 0.54843\n",
            "Epoch: 7158 Train Loss: 0.52028 Validation Loss: 0.54810\n",
            "Epoch: 7159 Train Loss: 0.52399 Validation Loss: 0.54953\n",
            "Epoch: 7160 Train Loss: 0.52395 Validation Loss: 0.54874\n",
            "Epoch: 7161 Train Loss: 0.52026 Validation Loss: 0.54806\n",
            "Epoch: 7162 Train Loss: 0.52382 Validation Loss: 0.54888\n",
            "Epoch: 7163 Train Loss: 0.52083 Validation Loss: 0.54872\n",
            "Epoch: 7164 Train Loss: 0.52293 Validation Loss: 0.54895\n",
            "Epoch: 7165 Train Loss: 0.52402 Validation Loss: 0.54829\n",
            "Epoch: 7166 Train Loss: 0.52294 Validation Loss: 0.54888\n",
            "Epoch: 7167 Train Loss: 0.52450 Validation Loss: 0.54843\n",
            "Epoch: 7168 Train Loss: 0.52871 Validation Loss: 0.54965\n",
            "Epoch: 7169 Train Loss: 0.52577 Validation Loss: 0.54809\n",
            "Epoch: 7170 Train Loss: 0.52518 Validation Loss: 0.54849\n",
            "Epoch: 7171 Train Loss: 0.52108 Validation Loss: 0.54838\n",
            "Epoch: 7172 Train Loss: 0.52297 Validation Loss: 0.54923\n",
            "Epoch: 7173 Train Loss: 0.52545 Validation Loss: 0.54925\n",
            "Epoch: 7174 Train Loss: 0.52332 Validation Loss: 0.54792\n",
            "Epoch: 7175 Train Loss: 0.51974 Validation Loss: 0.54843\n",
            "Epoch: 7176 Train Loss: 0.52126 Validation Loss: 0.54949\n",
            "Epoch: 7177 Train Loss: 0.52194 Validation Loss: 0.54854\n",
            "Epoch: 7178 Train Loss: 0.52282 Validation Loss: 0.54869\n",
            "Epoch: 7179 Train Loss: 0.52296 Validation Loss: 0.54888\n",
            "Epoch: 7180 Train Loss: 0.52079 Validation Loss: 0.54888\n",
            "Epoch: 7181 Train Loss: 0.52114 Validation Loss: 0.54910\n",
            "Epoch: 7182 Train Loss: 0.52338 Validation Loss: 0.54830\n",
            "Epoch: 7183 Train Loss: 0.52307 Validation Loss: 0.54833\n",
            "Epoch: 7184 Train Loss: 0.52363 Validation Loss: 0.54940\n",
            "Epoch: 7185 Train Loss: 0.52285 Validation Loss: 0.54895\n",
            "Epoch: 7186 Train Loss: 0.52426 Validation Loss: 0.54883\n",
            "Epoch: 7187 Train Loss: 0.52392 Validation Loss: 0.54859\n",
            "Epoch: 7188 Train Loss: 0.52696 Validation Loss: 0.54871\n",
            "Epoch: 7189 Train Loss: 0.52190 Validation Loss: 0.54802\n",
            "Epoch: 7190 Train Loss: 0.52479 Validation Loss: 0.54851\n",
            "Epoch: 7191 Train Loss: 0.52282 Validation Loss: 0.54889\n",
            "Epoch: 7192 Train Loss: 0.52750 Validation Loss: 0.54866\n",
            "Epoch: 7193 Train Loss: 0.52177 Validation Loss: 0.54870\n",
            "Epoch: 7194 Train Loss: 0.52505 Validation Loss: 0.54895\n",
            "Epoch: 7195 Train Loss: 0.52564 Validation Loss: 0.54807\n",
            "Epoch: 7196 Train Loss: 0.52396 Validation Loss: 0.54817\n",
            "Epoch: 7197 Train Loss: 0.52481 Validation Loss: 0.54835\n",
            "Epoch: 7198 Train Loss: 0.52088 Validation Loss: 0.54850\n",
            "Epoch: 7199 Train Loss: 0.52203 Validation Loss: 0.54870\n",
            "Epoch: 7200 Train Loss: 0.52297 Validation Loss: 0.54896\n",
            "Epoch: 7201 Train Loss: 0.52492 Validation Loss: 0.54859\n",
            "Epoch: 7202 Train Loss: 0.52207 Validation Loss: 0.54837\n",
            "Epoch: 7203 Train Loss: 0.52315 Validation Loss: 0.54874\n",
            "Epoch: 7204 Train Loss: 0.52709 Validation Loss: 0.54921\n",
            "Epoch: 7205 Train Loss: 0.52389 Validation Loss: 0.54836\n",
            "Epoch: 7206 Train Loss: 0.52189 Validation Loss: 0.54843\n",
            "Epoch: 7207 Train Loss: 0.52393 Validation Loss: 0.54842\n",
            "Epoch: 7208 Train Loss: 0.52081 Validation Loss: 0.54862\n",
            "Epoch: 7209 Train Loss: 0.52300 Validation Loss: 0.54885\n",
            "Epoch: 7210 Train Loss: 0.52419 Validation Loss: 0.54835\n",
            "Epoch: 7211 Train Loss: 0.52352 Validation Loss: 0.54905\n",
            "Epoch: 7212 Train Loss: 0.51897 Validation Loss: 0.54826\n",
            "Epoch: 7213 Train Loss: 0.51974 Validation Loss: 0.54886\n",
            "Epoch: 7214 Train Loss: 0.52482 Validation Loss: 0.54882\n",
            "Epoch: 7215 Train Loss: 0.52395 Validation Loss: 0.54880\n",
            "Epoch: 7216 Train Loss: 0.52477 Validation Loss: 0.54874\n",
            "Epoch: 7217 Train Loss: 0.52278 Validation Loss: 0.54853\n",
            "Epoch: 7218 Train Loss: 0.52483 Validation Loss: 0.54866\n",
            "Epoch: 7219 Train Loss: 0.52602 Validation Loss: 0.54868\n",
            "Epoch: 7220 Train Loss: 0.52405 Validation Loss: 0.54811\n",
            "Epoch: 7221 Train Loss: 0.52453 Validation Loss: 0.54892\n",
            "Epoch: 7222 Train Loss: 0.52610 Validation Loss: 0.54805\n",
            "Epoch: 7223 Train Loss: 0.52183 Validation Loss: 0.54877\n",
            "Epoch: 7224 Train Loss: 0.52286 Validation Loss: 0.54900\n",
            "Epoch: 7225 Train Loss: 0.52480 Validation Loss: 0.54865\n",
            "Epoch: 7226 Train Loss: 0.52227 Validation Loss: 0.54799\n",
            "Epoch: 7227 Train Loss: 0.52169 Validation Loss: 0.54925\n",
            "Epoch: 7228 Train Loss: 0.52183 Validation Loss: 0.54867\n",
            "Epoch: 7229 Train Loss: 0.52108 Validation Loss: 0.54853\n",
            "Epoch: 7230 Train Loss: 0.52793 Validation Loss: 0.54857\n",
            "Epoch: 7231 Train Loss: 0.52402 Validation Loss: 0.54901\n",
            "Epoch: 7232 Train Loss: 0.52802 Validation Loss: 0.54843\n",
            "Epoch: 7233 Train Loss: 0.52601 Validation Loss: 0.54839\n",
            "Epoch: 7234 Train Loss: 0.52193 Validation Loss: 0.54827\n",
            "Epoch: 7235 Train Loss: 0.52419 Validation Loss: 0.54861\n",
            "Epoch: 7236 Train Loss: 0.52586 Validation Loss: 0.54875\n",
            "Epoch: 7237 Train Loss: 0.52178 Validation Loss: 0.54858\n",
            "Epoch: 7238 Train Loss: 0.52104 Validation Loss: 0.54864\n",
            "Epoch: 7239 Train Loss: 0.52200 Validation Loss: 0.54849\n",
            "Epoch: 7240 Train Loss: 0.52430 Validation Loss: 0.54835\n",
            "Epoch: 7241 Train Loss: 0.52395 Validation Loss: 0.54949\n",
            "Epoch: 7242 Train Loss: 0.52322 Validation Loss: 0.54860\n",
            "Epoch: 7243 Train Loss: 0.52349 Validation Loss: 0.54877\n",
            "Epoch: 7244 Train Loss: 0.52593 Validation Loss: 0.54801\n",
            "Epoch: 7245 Train Loss: 0.52431 Validation Loss: 0.54883\n",
            "Epoch: 7246 Train Loss: 0.52180 Validation Loss: 0.54821\n",
            "Epoch: 7247 Train Loss: 0.52108 Validation Loss: 0.54797\n",
            "Epoch: 7248 Train Loss: 0.52618 Validation Loss: 0.54922\n",
            "Epoch: 7249 Train Loss: 0.53136 Validation Loss: 0.54922\n",
            "Epoch: 7250 Train Loss: 0.52293 Validation Loss: 0.54844\n",
            "Epoch: 7251 Train Loss: 0.52082 Validation Loss: 0.54831\n",
            "Epoch: 7252 Train Loss: 0.52384 Validation Loss: 0.54868\n",
            "Epoch: 7253 Train Loss: 0.52397 Validation Loss: 0.54842\n",
            "Epoch: 7254 Train Loss: 0.52765 Validation Loss: 0.54967\n",
            "Epoch: 7255 Train Loss: 0.52174 Validation Loss: 0.54805\n",
            "Epoch: 7256 Train Loss: 0.52417 Validation Loss: 0.54811\n",
            "Epoch: 7257 Train Loss: 0.52168 Validation Loss: 0.54902\n",
            "Epoch: 7258 Train Loss: 0.52391 Validation Loss: 0.54864\n",
            "Epoch: 7259 Train Loss: 0.52102 Validation Loss: 0.54877\n",
            "Epoch: 7260 Train Loss: 0.52487 Validation Loss: 0.54873\n",
            "Epoch: 7261 Train Loss: 0.52347 Validation Loss: 0.54834\n",
            "Epoch: 7262 Train Loss: 0.52295 Validation Loss: 0.54829\n",
            "Epoch: 7263 Train Loss: 0.52092 Validation Loss: 0.54867\n",
            "Epoch: 7264 Train Loss: 0.52488 Validation Loss: 0.54897\n",
            "Epoch: 7265 Train Loss: 0.52215 Validation Loss: 0.54811\n",
            "Epoch: 7266 Train Loss: 0.52161 Validation Loss: 0.54924\n",
            "Epoch: 7267 Train Loss: 0.52037 Validation Loss: 0.54924\n",
            "Epoch: 7268 Train Loss: 0.52429 Validation Loss: 0.54886\n",
            "Epoch: 7269 Train Loss: 0.52403 Validation Loss: 0.54928\n",
            "Epoch: 7270 Train Loss: 0.52229 Validation Loss: 0.54825\n",
            "Epoch: 7271 Train Loss: 0.52599 Validation Loss: 0.54932\n",
            "Epoch: 7272 Train Loss: 0.52336 Validation Loss: 0.54815\n",
            "Epoch: 7273 Train Loss: 0.52471 Validation Loss: 0.54874\n",
            "Epoch: 7274 Train Loss: 0.52287 Validation Loss: 0.54867\n",
            "Epoch: 7275 Train Loss: 0.52508 Validation Loss: 0.54951\n",
            "Epoch: 7276 Train Loss: 0.52367 Validation Loss: 0.54863\n",
            "Epoch: 7277 Train Loss: 0.52621 Validation Loss: 0.54824\n",
            "Epoch: 7278 Train Loss: 0.52176 Validation Loss: 0.54865\n",
            "Epoch: 7279 Train Loss: 0.52306 Validation Loss: 0.54858\n",
            "Epoch: 7280 Train Loss: 0.52281 Validation Loss: 0.54838\n",
            "Epoch: 7281 Train Loss: 0.52294 Validation Loss: 0.54934\n",
            "Epoch: 7282 Train Loss: 0.52204 Validation Loss: 0.54840\n",
            "Epoch: 7283 Train Loss: 0.52172 Validation Loss: 0.54897\n",
            "Epoch: 7284 Train Loss: 0.52314 Validation Loss: 0.54881\n",
            "Epoch: 7285 Train Loss: 0.52387 Validation Loss: 0.54798\n",
            "Epoch: 7286 Train Loss: 0.52097 Validation Loss: 0.54838\n",
            "Epoch: 7287 Train Loss: 0.52084 Validation Loss: 0.54899\n",
            "Epoch: 7288 Train Loss: 0.52180 Validation Loss: 0.54888\n",
            "Epoch: 7289 Train Loss: 0.52382 Validation Loss: 0.54855\n",
            "Epoch: 7290 Train Loss: 0.52294 Validation Loss: 0.54859\n",
            "Epoch: 7291 Train Loss: 0.53042 Validation Loss: 0.54922\n",
            "Epoch: 7292 Train Loss: 0.52184 Validation Loss: 0.54824\n",
            "Epoch: 7293 Train Loss: 0.52294 Validation Loss: 0.54871\n",
            "Epoch: 7294 Train Loss: 0.52580 Validation Loss: 0.54837\n",
            "Epoch: 7295 Train Loss: 0.52485 Validation Loss: 0.54848\n",
            "Epoch: 7296 Train Loss: 0.52485 Validation Loss: 0.54879\n",
            "Epoch: 7297 Train Loss: 0.52118 Validation Loss: 0.54857\n",
            "Epoch: 7298 Train Loss: 0.52081 Validation Loss: 0.54865\n",
            "Epoch: 7299 Train Loss: 0.52637 Validation Loss: 0.54952\n",
            "Epoch: 7300 Train Loss: 0.52582 Validation Loss: 0.54801\n",
            "Epoch: 7301 Train Loss: 0.52096 Validation Loss: 0.54881\n",
            "Epoch: 7302 Train Loss: 0.52349 Validation Loss: 0.54859\n",
            "Epoch: 7303 Train Loss: 0.52481 Validation Loss: 0.54900\n",
            "Epoch: 7304 Train Loss: 0.52959 Validation Loss: 0.54947\n",
            "Epoch: 7305 Train Loss: 0.52484 Validation Loss: 0.54784\n",
            "Epoch: 7306 Train Loss: 0.52199 Validation Loss: 0.54824\n",
            "Epoch: 7307 Train Loss: 0.52490 Validation Loss: 0.54873\n",
            "Epoch: 7308 Train Loss: 0.52381 Validation Loss: 0.54894\n",
            "Epoch: 7309 Train Loss: 0.52261 Validation Loss: 0.54838\n",
            "Epoch: 7310 Train Loss: 0.52373 Validation Loss: 0.54888\n",
            "Epoch: 7311 Train Loss: 0.52530 Validation Loss: 0.54951\n",
            "Epoch: 7312 Train Loss: 0.52384 Validation Loss: 0.54872\n",
            "Epoch: 7313 Train Loss: 0.52379 Validation Loss: 0.54802\n",
            "Epoch: 7314 Train Loss: 0.52530 Validation Loss: 0.54895\n",
            "Epoch: 7315 Train Loss: 0.52395 Validation Loss: 0.54860\n",
            "Epoch: 7316 Train Loss: 0.52202 Validation Loss: 0.54878\n",
            "Epoch: 7317 Train Loss: 0.52513 Validation Loss: 0.54948\n",
            "Epoch: 7318 Train Loss: 0.52369 Validation Loss: 0.54805\n",
            "Epoch: 7319 Train Loss: 0.52376 Validation Loss: 0.54853\n",
            "Epoch: 7320 Train Loss: 0.52302 Validation Loss: 0.54870\n",
            "Epoch: 7321 Train Loss: 0.52296 Validation Loss: 0.54908\n",
            "Epoch: 7322 Train Loss: 0.52395 Validation Loss: 0.54931\n",
            "Epoch: 7323 Train Loss: 0.52506 Validation Loss: 0.54856\n",
            "Epoch: 7324 Train Loss: 0.52189 Validation Loss: 0.54861\n",
            "Epoch: 7325 Train Loss: 0.52685 Validation Loss: 0.54877\n",
            "Epoch: 7326 Train Loss: 0.52550 Validation Loss: 0.54841\n",
            "Epoch: 7327 Train Loss: 0.52200 Validation Loss: 0.54852\n",
            "Epoch: 7328 Train Loss: 0.52396 Validation Loss: 0.54901\n",
            "Epoch: 7329 Train Loss: 0.52211 Validation Loss: 0.54921\n",
            "Epoch: 7330 Train Loss: 0.52216 Validation Loss: 0.54807\n",
            "Epoch: 7331 Train Loss: 0.52290 Validation Loss: 0.54850\n",
            "Epoch: 7332 Train Loss: 0.52283 Validation Loss: 0.54870\n",
            "Epoch: 7333 Train Loss: 0.52354 Validation Loss: 0.54951\n",
            "Epoch: 7334 Train Loss: 0.52687 Validation Loss: 0.54879\n",
            "Epoch: 7335 Train Loss: 0.52391 Validation Loss: 0.54817\n",
            "Epoch: 7336 Train Loss: 0.52296 Validation Loss: 0.54866\n",
            "Epoch: 7337 Train Loss: 0.52293 Validation Loss: 0.54820\n",
            "Epoch: 7338 Train Loss: 0.52421 Validation Loss: 0.54924\n",
            "Epoch: 7339 Train Loss: 0.52377 Validation Loss: 0.54836\n",
            "Epoch: 7340 Train Loss: 0.52344 Validation Loss: 0.54869\n",
            "Epoch: 7341 Train Loss: 0.52421 Validation Loss: 0.54793\n",
            "Epoch: 7342 Train Loss: 0.52095 Validation Loss: 0.54877\n",
            "Epoch: 7343 Train Loss: 0.52322 Validation Loss: 0.54881\n",
            "Epoch: 7344 Train Loss: 0.52617 Validation Loss: 0.54839\n",
            "Epoch: 7345 Train Loss: 0.52384 Validation Loss: 0.54853\n",
            "Epoch: 7346 Train Loss: 0.52320 Validation Loss: 0.54846\n",
            "Epoch: 7347 Train Loss: 0.52584 Validation Loss: 0.54896\n",
            "Epoch: 7348 Train Loss: 0.52192 Validation Loss: 0.54900\n",
            "Epoch: 7349 Train Loss: 0.52221 Validation Loss: 0.54853\n",
            "Epoch: 7350 Train Loss: 0.51938 Validation Loss: 0.54863\n",
            "Epoch: 7351 Train Loss: 0.52399 Validation Loss: 0.54848\n",
            "Epoch: 7352 Train Loss: 0.52241 Validation Loss: 0.54877\n",
            "Epoch: 7353 Train Loss: 0.52684 Validation Loss: 0.54868\n",
            "Epoch: 7354 Train Loss: 0.52493 Validation Loss: 0.54850\n",
            "Epoch: 7355 Train Loss: 0.52281 Validation Loss: 0.54819\n",
            "Epoch: 7356 Train Loss: 0.52315 Validation Loss: 0.54881\n",
            "Epoch: 7357 Train Loss: 0.52472 Validation Loss: 0.54798\n",
            "Epoch: 7358 Train Loss: 0.52175 Validation Loss: 0.54883\n",
            "Epoch: 7359 Train Loss: 0.52205 Validation Loss: 0.54850\n",
            "Epoch: 7360 Train Loss: 0.52283 Validation Loss: 0.54870\n",
            "Epoch: 7361 Train Loss: 0.52177 Validation Loss: 0.54896\n",
            "Epoch: 7362 Train Loss: 0.52097 Validation Loss: 0.54848\n",
            "Epoch: 7363 Train Loss: 0.52219 Validation Loss: 0.54896\n",
            "Epoch: 7364 Train Loss: 0.52313 Validation Loss: 0.54863\n",
            "Epoch: 7365 Train Loss: 0.52573 Validation Loss: 0.54856\n",
            "Epoch: 7366 Train Loss: 0.52515 Validation Loss: 0.54904\n",
            "Epoch: 7367 Train Loss: 0.52453 Validation Loss: 0.54826\n",
            "Epoch: 7368 Train Loss: 0.52511 Validation Loss: 0.54903\n",
            "Epoch: 7369 Train Loss: 0.52696 Validation Loss: 0.54881\n",
            "Epoch: 7370 Train Loss: 0.52292 Validation Loss: 0.54816\n",
            "Epoch: 7371 Train Loss: 0.52308 Validation Loss: 0.54835\n",
            "Epoch: 7372 Train Loss: 0.52186 Validation Loss: 0.54831\n",
            "Epoch: 7373 Train Loss: 0.52312 Validation Loss: 0.54872\n",
            "Epoch: 7374 Train Loss: 0.52283 Validation Loss: 0.54882\n",
            "Epoch: 7375 Train Loss: 0.52389 Validation Loss: 0.54897\n",
            "Epoch: 7376 Train Loss: 0.52109 Validation Loss: 0.54820\n",
            "Epoch: 7377 Train Loss: 0.52398 Validation Loss: 0.54858\n",
            "Epoch: 7378 Train Loss: 0.52479 Validation Loss: 0.54905\n",
            "Epoch: 7379 Train Loss: 0.52510 Validation Loss: 0.54906\n",
            "Epoch: 7380 Train Loss: 0.52189 Validation Loss: 0.54833\n",
            "Epoch: 7381 Train Loss: 0.52284 Validation Loss: 0.54829\n",
            "Epoch: 7382 Train Loss: 0.52080 Validation Loss: 0.54880\n",
            "Epoch: 7383 Train Loss: 0.52202 Validation Loss: 0.54853\n",
            "Epoch: 7384 Train Loss: 0.52420 Validation Loss: 0.54944\n",
            "Epoch: 7385 Train Loss: 0.52474 Validation Loss: 0.54857\n",
            "Epoch: 7386 Train Loss: 0.52545 Validation Loss: 0.54788\n",
            "Epoch: 7387 Train Loss: 0.52479 Validation Loss: 0.54849\n",
            "Epoch: 7388 Train Loss: 0.52101 Validation Loss: 0.54934\n",
            "Epoch: 7389 Train Loss: 0.52207 Validation Loss: 0.54934\n",
            "Epoch: 7390 Train Loss: 0.52391 Validation Loss: 0.54882\n",
            "Epoch: 7391 Train Loss: 0.52414 Validation Loss: 0.54822\n",
            "Epoch: 7392 Train Loss: 0.52229 Validation Loss: 0.54894\n",
            "Epoch: 7393 Train Loss: 0.52315 Validation Loss: 0.54886\n",
            "Epoch: 7394 Train Loss: 0.52301 Validation Loss: 0.54808\n",
            "Epoch: 7395 Train Loss: 0.52491 Validation Loss: 0.54856\n",
            "Epoch: 7396 Train Loss: 0.52216 Validation Loss: 0.54956\n",
            "Epoch: 7397 Train Loss: 0.52214 Validation Loss: 0.54817\n",
            "Epoch: 7398 Train Loss: 0.52179 Validation Loss: 0.54900\n",
            "Epoch: 7399 Train Loss: 0.52109 Validation Loss: 0.54843\n",
            "Epoch: 7400 Train Loss: 0.52192 Validation Loss: 0.54864\n",
            "Epoch: 7401 Train Loss: 0.52558 Validation Loss: 0.54957\n",
            "Epoch: 7402 Train Loss: 0.52380 Validation Loss: 0.54824\n",
            "Epoch: 7403 Train Loss: 0.52187 Validation Loss: 0.54824\n",
            "Epoch: 7404 Train Loss: 0.52393 Validation Loss: 0.54886\n",
            "Epoch: 7405 Train Loss: 0.52383 Validation Loss: 0.54891\n",
            "Epoch: 7406 Train Loss: 0.51984 Validation Loss: 0.54880\n",
            "Epoch: 7407 Train Loss: 0.52635 Validation Loss: 0.54932\n",
            "Epoch: 7408 Train Loss: 0.52292 Validation Loss: 0.54821\n",
            "Epoch: 7409 Train Loss: 0.52207 Validation Loss: 0.54809\n",
            "Epoch: 7410 Train Loss: 0.52489 Validation Loss: 0.54898\n",
            "Epoch: 7411 Train Loss: 0.52280 Validation Loss: 0.54858\n",
            "Epoch: 7412 Train Loss: 0.52388 Validation Loss: 0.54900\n",
            "Epoch: 7413 Train Loss: 0.52495 Validation Loss: 0.54848\n",
            "Epoch: 7414 Train Loss: 0.52182 Validation Loss: 0.54850\n",
            "Epoch: 7415 Train Loss: 0.52304 Validation Loss: 0.54865\n",
            "Epoch: 7416 Train Loss: 0.52211 Validation Loss: 0.54887\n",
            "Epoch: 7417 Train Loss: 0.52311 Validation Loss: 0.54850\n",
            "Epoch: 7418 Train Loss: 0.52631 Validation Loss: 0.54900\n",
            "Epoch: 7419 Train Loss: 0.52679 Validation Loss: 0.54857\n",
            "Epoch: 7420 Train Loss: 0.52406 Validation Loss: 0.54802\n",
            "Epoch: 7421 Train Loss: 0.52079 Validation Loss: 0.54838\n",
            "Epoch: 7422 Train Loss: 0.52517 Validation Loss: 0.54968\n",
            "Epoch: 7423 Train Loss: 0.52182 Validation Loss: 0.54823\n",
            "Epoch: 7424 Train Loss: 0.52290 Validation Loss: 0.54874\n",
            "Epoch: 7425 Train Loss: 0.52643 Validation Loss: 0.54938\n",
            "Epoch: 7426 Train Loss: 0.52581 Validation Loss: 0.54846\n",
            "Epoch: 7427 Train Loss: 0.52210 Validation Loss: 0.54785\n",
            "Epoch: 7428 Train Loss: 0.52049 Validation Loss: 0.54819\n",
            "Epoch: 7429 Train Loss: 0.52271 Validation Loss: 0.55006\n",
            "Epoch: 7430 Train Loss: 0.52117 Validation Loss: 0.54888\n",
            "Epoch: 7431 Train Loss: 0.52170 Validation Loss: 0.54911\n",
            "Epoch: 7432 Train Loss: 0.52590 Validation Loss: 0.54888\n",
            "Epoch: 7433 Train Loss: 0.52484 Validation Loss: 0.54882\n",
            "Epoch: 7434 Train Loss: 0.52498 Validation Loss: 0.54875\n",
            "Epoch: 7435 Train Loss: 0.52481 Validation Loss: 0.54843\n",
            "Epoch: 7436 Train Loss: 0.52240 Validation Loss: 0.54816\n",
            "Epoch: 7437 Train Loss: 0.52483 Validation Loss: 0.54903\n",
            "Epoch: 7438 Train Loss: 0.52494 Validation Loss: 0.54904\n",
            "Epoch: 7439 Train Loss: 0.52134 Validation Loss: 0.54818\n",
            "Epoch: 7440 Train Loss: 0.52102 Validation Loss: 0.54857\n",
            "Epoch: 7441 Train Loss: 0.52316 Validation Loss: 0.54921\n",
            "Epoch: 7442 Train Loss: 0.52074 Validation Loss: 0.54836\n",
            "Epoch: 7443 Train Loss: 0.52094 Validation Loss: 0.54847\n",
            "Epoch: 7444 Train Loss: 0.52296 Validation Loss: 0.54870\n",
            "Epoch: 7445 Train Loss: 0.51984 Validation Loss: 0.54840\n",
            "Epoch: 7446 Train Loss: 0.52428 Validation Loss: 0.54917\n",
            "Epoch: 7447 Train Loss: 0.52342 Validation Loss: 0.54814\n",
            "Epoch: 7448 Train Loss: 0.51981 Validation Loss: 0.54859\n",
            "Epoch: 7449 Train Loss: 0.52397 Validation Loss: 0.54946\n",
            "Epoch: 7450 Train Loss: 0.52474 Validation Loss: 0.54887\n",
            "Epoch: 7451 Train Loss: 0.52850 Validation Loss: 0.54825\n",
            "Epoch: 7452 Train Loss: 0.52285 Validation Loss: 0.54860\n",
            "Epoch: 7453 Train Loss: 0.52295 Validation Loss: 0.54845\n",
            "Epoch: 7454 Train Loss: 0.52374 Validation Loss: 0.54905\n",
            "Epoch: 7455 Train Loss: 0.52233 Validation Loss: 0.54834\n",
            "Epoch: 7456 Train Loss: 0.52389 Validation Loss: 0.54939\n",
            "Epoch: 7457 Train Loss: 0.52085 Validation Loss: 0.54851\n",
            "Epoch: 7458 Train Loss: 0.52408 Validation Loss: 0.54891\n",
            "Epoch: 7459 Train Loss: 0.52577 Validation Loss: 0.54834\n",
            "Epoch: 7460 Train Loss: 0.52187 Validation Loss: 0.54803\n",
            "Epoch: 7461 Train Loss: 0.52181 Validation Loss: 0.54836\n",
            "Epoch: 7462 Train Loss: 0.52475 Validation Loss: 0.54919\n",
            "Epoch: 7463 Train Loss: 0.52582 Validation Loss: 0.54871\n",
            "Epoch: 7464 Train Loss: 0.52187 Validation Loss: 0.54837\n",
            "Epoch: 7465 Train Loss: 0.52109 Validation Loss: 0.54843\n",
            "Epoch: 7466 Train Loss: 0.52282 Validation Loss: 0.54859\n",
            "Epoch: 7467 Train Loss: 0.52205 Validation Loss: 0.54926\n",
            "Epoch: 7468 Train Loss: 0.52376 Validation Loss: 0.54864\n",
            "Epoch: 7469 Train Loss: 0.52378 Validation Loss: 0.54822\n",
            "Epoch: 7470 Train Loss: 0.52224 Validation Loss: 0.54822\n",
            "Epoch: 7471 Train Loss: 0.52409 Validation Loss: 0.54960\n",
            "Epoch: 7472 Train Loss: 0.52512 Validation Loss: 0.54866\n",
            "Epoch: 7473 Train Loss: 0.52085 Validation Loss: 0.54840\n",
            "Epoch: 7474 Train Loss: 0.52396 Validation Loss: 0.54880\n",
            "Epoch: 7475 Train Loss: 0.52232 Validation Loss: 0.54861\n",
            "Epoch: 7476 Train Loss: 0.52725 Validation Loss: 0.54955\n",
            "Epoch: 7477 Train Loss: 0.52543 Validation Loss: 0.54820\n",
            "Epoch: 7478 Train Loss: 0.52407 Validation Loss: 0.54876\n",
            "Epoch: 7479 Train Loss: 0.52572 Validation Loss: 0.54847\n",
            "Epoch: 7480 Train Loss: 0.52398 Validation Loss: 0.54807\n",
            "Epoch: 7481 Train Loss: 0.52506 Validation Loss: 0.54904\n",
            "Epoch: 7482 Train Loss: 0.52209 Validation Loss: 0.54905\n",
            "Epoch: 7483 Train Loss: 0.52171 Validation Loss: 0.54812\n",
            "Epoch: 7484 Train Loss: 0.52306 Validation Loss: 0.54880\n",
            "Epoch: 7485 Train Loss: 0.52225 Validation Loss: 0.54848\n",
            "Epoch: 7486 Train Loss: 0.52647 Validation Loss: 0.54883\n",
            "Epoch: 7487 Train Loss: 0.52176 Validation Loss: 0.54880\n",
            "Epoch: 7488 Train Loss: 0.52393 Validation Loss: 0.54882\n",
            "Epoch: 7489 Train Loss: 0.52037 Validation Loss: 0.54804\n",
            "Epoch: 7490 Train Loss: 0.52184 Validation Loss: 0.54914\n",
            "Epoch: 7491 Train Loss: 0.52393 Validation Loss: 0.54934\n",
            "Epoch: 7492 Train Loss: 0.52310 Validation Loss: 0.54838\n",
            "Epoch: 7493 Train Loss: 0.52482 Validation Loss: 0.54830\n",
            "Epoch: 7494 Train Loss: 0.52136 Validation Loss: 0.54838\n",
            "Epoch: 7495 Train Loss: 0.52291 Validation Loss: 0.54846\n",
            "Epoch: 7496 Train Loss: 0.52628 Validation Loss: 0.54959\n",
            "Epoch: 7497 Train Loss: 0.52330 Validation Loss: 0.54838\n",
            "Epoch: 7498 Train Loss: 0.52585 Validation Loss: 0.54869\n",
            "Epoch: 7499 Train Loss: 0.52117 Validation Loss: 0.54817\n",
            "Epoch: 7500 Train Loss: 0.52165 Validation Loss: 0.54897\n",
            "Epoch: 7501 Train Loss: 0.52736 Validation Loss: 0.54939\n",
            "Epoch: 7502 Train Loss: 0.52269 Validation Loss: 0.54821\n",
            "Epoch: 7503 Train Loss: 0.52436 Validation Loss: 0.54829\n",
            "Epoch: 7504 Train Loss: 0.51994 Validation Loss: 0.54828\n",
            "Epoch: 7505 Train Loss: 0.52402 Validation Loss: 0.54937\n",
            "Epoch: 7506 Train Loss: 0.52521 Validation Loss: 0.54911\n",
            "Epoch: 7507 Train Loss: 0.52013 Validation Loss: 0.54831\n",
            "Epoch: 7508 Train Loss: 0.52276 Validation Loss: 0.54901\n",
            "Epoch: 7509 Train Loss: 0.52390 Validation Loss: 0.54886\n",
            "Epoch: 7510 Train Loss: 0.52791 Validation Loss: 0.54908\n",
            "Epoch: 7511 Train Loss: 0.52079 Validation Loss: 0.54811\n",
            "Epoch: 7512 Train Loss: 0.52419 Validation Loss: 0.54906\n",
            "Epoch: 7513 Train Loss: 0.52585 Validation Loss: 0.54899\n",
            "Epoch: 7514 Train Loss: 0.52197 Validation Loss: 0.54826\n",
            "Epoch: 7515 Train Loss: 0.52244 Validation Loss: 0.54913\n",
            "Epoch: 7516 Train Loss: 0.52272 Validation Loss: 0.54841\n",
            "Epoch: 7517 Train Loss: 0.52293 Validation Loss: 0.54843\n",
            "Epoch: 7518 Train Loss: 0.52175 Validation Loss: 0.54839\n",
            "Epoch: 7519 Train Loss: 0.52289 Validation Loss: 0.54868\n",
            "Epoch: 7520 Train Loss: 0.52193 Validation Loss: 0.54884\n",
            "Epoch: 7521 Train Loss: 0.52174 Validation Loss: 0.54850\n",
            "Epoch: 7522 Train Loss: 0.52247 Validation Loss: 0.54826\n",
            "Epoch: 7523 Train Loss: 0.52296 Validation Loss: 0.54917\n",
            "Epoch: 7524 Train Loss: 0.52389 Validation Loss: 0.54903\n",
            "Epoch: 7525 Train Loss: 0.52395 Validation Loss: 0.54877\n",
            "Epoch: 7526 Train Loss: 0.52691 Validation Loss: 0.54832\n",
            "Epoch: 7527 Train Loss: 0.52095 Validation Loss: 0.54809\n",
            "Epoch: 7528 Train Loss: 0.52236 Validation Loss: 0.54883\n",
            "Epoch: 7529 Train Loss: 0.52270 Validation Loss: 0.54860\n",
            "Epoch: 7530 Train Loss: 0.52478 Validation Loss: 0.54871\n",
            "Epoch: 7531 Train Loss: 0.52303 Validation Loss: 0.54829\n",
            "Epoch: 7532 Train Loss: 0.52702 Validation Loss: 0.54912\n",
            "Epoch: 7533 Train Loss: 0.52608 Validation Loss: 0.54866\n",
            "Epoch: 7534 Train Loss: 0.52403 Validation Loss: 0.54777\n",
            "Epoch: 7535 Train Loss: 0.52547 Validation Loss: 0.54964\n",
            "Epoch: 7536 Train Loss: 0.52371 Validation Loss: 0.54853\n",
            "Epoch: 7537 Train Loss: 0.52205 Validation Loss: 0.54810\n",
            "Epoch: 7538 Train Loss: 0.52287 Validation Loss: 0.54886\n",
            "Epoch: 7539 Train Loss: 0.52175 Validation Loss: 0.54880\n",
            "Epoch: 7540 Train Loss: 0.52390 Validation Loss: 0.54889\n",
            "Epoch: 7541 Train Loss: 0.52407 Validation Loss: 0.54820\n",
            "Epoch: 7542 Train Loss: 0.52311 Validation Loss: 0.54868\n",
            "Epoch: 7543 Train Loss: 0.52289 Validation Loss: 0.54856\n",
            "Epoch: 7544 Train Loss: 0.52085 Validation Loss: 0.54891\n",
            "Epoch: 7545 Train Loss: 0.52094 Validation Loss: 0.54864\n",
            "Epoch: 7546 Train Loss: 0.52190 Validation Loss: 0.54905\n",
            "Epoch: 7547 Train Loss: 0.52200 Validation Loss: 0.54902\n",
            "Epoch: 7548 Train Loss: 0.52474 Validation Loss: 0.54888\n",
            "Epoch: 7549 Train Loss: 0.52385 Validation Loss: 0.54882\n",
            "Epoch: 7550 Train Loss: 0.52518 Validation Loss: 0.54860\n",
            "Epoch: 7551 Train Loss: 0.52713 Validation Loss: 0.54890\n",
            "Epoch: 7552 Train Loss: 0.52459 Validation Loss: 0.54817\n",
            "Epoch: 7553 Train Loss: 0.52444 Validation Loss: 0.54822\n",
            "Epoch: 7554 Train Loss: 0.52310 Validation Loss: 0.54844\n",
            "Epoch: 7555 Train Loss: 0.52428 Validation Loss: 0.54954\n",
            "Epoch: 7556 Train Loss: 0.52203 Validation Loss: 0.54808\n",
            "Epoch: 7557 Train Loss: 0.52518 Validation Loss: 0.54935\n",
            "Epoch: 7558 Train Loss: 0.52080 Validation Loss: 0.54861\n",
            "Epoch: 7559 Train Loss: 0.52293 Validation Loss: 0.54871\n",
            "Epoch: 7560 Train Loss: 0.52376 Validation Loss: 0.54852\n",
            "Epoch: 7561 Train Loss: 0.52378 Validation Loss: 0.54846\n",
            "Epoch: 7562 Train Loss: 0.52386 Validation Loss: 0.54868\n",
            "Epoch: 7563 Train Loss: 0.52182 Validation Loss: 0.54878\n",
            "Epoch: 7564 Train Loss: 0.52142 Validation Loss: 0.54803\n",
            "Epoch: 7565 Train Loss: 0.52062 Validation Loss: 0.54929\n",
            "Epoch: 7566 Train Loss: 0.52338 Validation Loss: 0.54861\n",
            "Epoch: 7567 Train Loss: 0.52476 Validation Loss: 0.54901\n",
            "Epoch: 7568 Train Loss: 0.52721 Validation Loss: 0.54929\n",
            "Epoch: 7569 Train Loss: 0.52381 Validation Loss: 0.54836\n",
            "Epoch: 7570 Train Loss: 0.52509 Validation Loss: 0.54897\n",
            "Epoch: 7571 Train Loss: 0.52294 Validation Loss: 0.54887\n",
            "Epoch: 7572 Train Loss: 0.52609 Validation Loss: 0.54802\n",
            "Epoch: 7573 Train Loss: 0.52476 Validation Loss: 0.54909\n",
            "Epoch: 7574 Train Loss: 0.52306 Validation Loss: 0.54895\n",
            "Epoch: 7575 Train Loss: 0.52150 Validation Loss: 0.54872\n",
            "Epoch: 7576 Train Loss: 0.52343 Validation Loss: 0.54840\n",
            "Epoch: 7577 Train Loss: 0.52226 Validation Loss: 0.54843\n",
            "Epoch: 7578 Train Loss: 0.52228 Validation Loss: 0.54846\n",
            "Epoch: 7579 Train Loss: 0.52409 Validation Loss: 0.54926\n",
            "Epoch: 7580 Train Loss: 0.52398 Validation Loss: 0.54876\n",
            "Epoch: 7581 Train Loss: 0.52533 Validation Loss: 0.54831\n",
            "Epoch: 7582 Train Loss: 0.52187 Validation Loss: 0.54890\n",
            "Epoch: 7583 Train Loss: 0.52291 Validation Loss: 0.54868\n",
            "Epoch: 7584 Train Loss: 0.52181 Validation Loss: 0.54817\n",
            "Epoch: 7585 Train Loss: 0.52176 Validation Loss: 0.54872\n",
            "Epoch: 7586 Train Loss: 0.52414 Validation Loss: 0.54846\n",
            "Epoch: 7587 Train Loss: 0.52487 Validation Loss: 0.54921\n",
            "Epoch: 7588 Train Loss: 0.52404 Validation Loss: 0.54874\n",
            "Epoch: 7589 Train Loss: 0.52257 Validation Loss: 0.54808\n",
            "Epoch: 7590 Train Loss: 0.52465 Validation Loss: 0.54873\n",
            "Epoch: 7591 Train Loss: 0.52109 Validation Loss: 0.54856\n",
            "Epoch: 7592 Train Loss: 0.52302 Validation Loss: 0.54867\n",
            "Epoch: 7593 Train Loss: 0.52218 Validation Loss: 0.54894\n",
            "Epoch: 7594 Train Loss: 0.52180 Validation Loss: 0.54834\n",
            "Epoch: 7595 Train Loss: 0.52548 Validation Loss: 0.54924\n",
            "Epoch: 7596 Train Loss: 0.52074 Validation Loss: 0.54792\n",
            "Epoch: 7597 Train Loss: 0.52587 Validation Loss: 0.54931\n",
            "Epoch: 7598 Train Loss: 0.52183 Validation Loss: 0.54875\n",
            "Epoch: 7599 Train Loss: 0.52585 Validation Loss: 0.54876\n",
            "Epoch: 7600 Train Loss: 0.52387 Validation Loss: 0.54831\n",
            "Epoch: 7601 Train Loss: 0.52396 Validation Loss: 0.54882\n",
            "Epoch: 7602 Train Loss: 0.52591 Validation Loss: 0.54886\n",
            "Epoch: 7603 Train Loss: 0.52583 Validation Loss: 0.54863\n",
            "Epoch: 7604 Train Loss: 0.52524 Validation Loss: 0.54798\n",
            "Epoch: 7605 Train Loss: 0.52083 Validation Loss: 0.54837\n",
            "Epoch: 7606 Train Loss: 0.51978 Validation Loss: 0.54894\n",
            "Epoch: 7607 Train Loss: 0.52309 Validation Loss: 0.54926\n",
            "Epoch: 7608 Train Loss: 0.52308 Validation Loss: 0.54840\n",
            "Epoch: 7609 Train Loss: 0.52383 Validation Loss: 0.54902\n",
            "Epoch: 7610 Train Loss: 0.52190 Validation Loss: 0.54890\n",
            "Epoch: 7611 Train Loss: 0.52372 Validation Loss: 0.54864\n",
            "Epoch: 7612 Train Loss: 0.52393 Validation Loss: 0.54839\n",
            "Epoch: 7613 Train Loss: 0.52089 Validation Loss: 0.54822\n",
            "Epoch: 7614 Train Loss: 0.52275 Validation Loss: 0.54862\n",
            "Epoch: 7615 Train Loss: 0.52734 Validation Loss: 0.54968\n",
            "Epoch: 7616 Train Loss: 0.52507 Validation Loss: 0.54800\n",
            "Epoch: 7617 Train Loss: 0.52251 Validation Loss: 0.54850\n",
            "Epoch: 7618 Train Loss: 0.52493 Validation Loss: 0.54869\n",
            "Epoch: 7619 Train Loss: 0.52325 Validation Loss: 0.54846\n",
            "Epoch: 7620 Train Loss: 0.52648 Validation Loss: 0.54882\n",
            "Epoch: 7621 Train Loss: 0.52375 Validation Loss: 0.54836\n",
            "Epoch: 7622 Train Loss: 0.52295 Validation Loss: 0.54812\n",
            "Epoch: 7623 Train Loss: 0.52494 Validation Loss: 0.54870\n",
            "Epoch: 7624 Train Loss: 0.52090 Validation Loss: 0.54837\n",
            "Epoch: 7625 Train Loss: 0.52594 Validation Loss: 0.54989\n",
            "Epoch: 7626 Train Loss: 0.52281 Validation Loss: 0.54820\n",
            "Epoch: 7627 Train Loss: 0.52705 Validation Loss: 0.54840\n",
            "Epoch: 7628 Train Loss: 0.52580 Validation Loss: 0.54829\n",
            "Epoch: 7629 Train Loss: 0.52532 Validation Loss: 0.54826\n",
            "Epoch: 7630 Train Loss: 0.52108 Validation Loss: 0.54853\n",
            "Epoch: 7631 Train Loss: 0.52011 Validation Loss: 0.54802\n",
            "Epoch: 7632 Train Loss: 0.52586 Validation Loss: 0.54888\n",
            "Epoch: 7633 Train Loss: 0.52284 Validation Loss: 0.54962\n",
            "Epoch: 7634 Train Loss: 0.52302 Validation Loss: 0.54799\n",
            "Epoch: 7635 Train Loss: 0.52504 Validation Loss: 0.54831\n",
            "Epoch: 7636 Train Loss: 0.52285 Validation Loss: 0.54892\n",
            "Epoch: 7637 Train Loss: 0.52021 Validation Loss: 0.54815\n",
            "Epoch: 7638 Train Loss: 0.52173 Validation Loss: 0.54872\n",
            "Epoch: 7639 Train Loss: 0.52328 Validation Loss: 0.54953\n",
            "Epoch: 7640 Train Loss: 0.52186 Validation Loss: 0.54864\n",
            "Epoch: 7641 Train Loss: 0.52092 Validation Loss: 0.54827\n",
            "Epoch: 7642 Train Loss: 0.52093 Validation Loss: 0.54848\n",
            "Epoch: 7643 Train Loss: 0.51988 Validation Loss: 0.54860\n",
            "Epoch: 7644 Train Loss: 0.52113 Validation Loss: 0.54844\n",
            "Epoch: 7645 Train Loss: 0.52182 Validation Loss: 0.54883\n",
            "Epoch: 7646 Train Loss: 0.52392 Validation Loss: 0.54907\n",
            "Epoch: 7647 Train Loss: 0.52527 Validation Loss: 0.54920\n",
            "Epoch: 7648 Train Loss: 0.52275 Validation Loss: 0.54853\n",
            "Epoch: 7649 Train Loss: 0.51984 Validation Loss: 0.54847\n",
            "Epoch: 7650 Train Loss: 0.52387 Validation Loss: 0.54857\n",
            "Epoch: 7651 Train Loss: 0.52275 Validation Loss: 0.54895\n",
            "Epoch: 7652 Train Loss: 0.52439 Validation Loss: 0.54849\n",
            "Epoch: 7653 Train Loss: 0.52283 Validation Loss: 0.54884\n",
            "Epoch: 7654 Train Loss: 0.52185 Validation Loss: 0.54911\n",
            "Epoch: 7655 Train Loss: 0.52304 Validation Loss: 0.54840\n",
            "Epoch: 7656 Train Loss: 0.52183 Validation Loss: 0.54836\n",
            "Epoch: 7657 Train Loss: 0.52204 Validation Loss: 0.54894\n",
            "Epoch: 7658 Train Loss: 0.52303 Validation Loss: 0.54895\n",
            "Epoch: 7659 Train Loss: 0.52399 Validation Loss: 0.54886\n",
            "Epoch: 7660 Train Loss: 0.52121 Validation Loss: 0.54828\n",
            "Epoch: 7661 Train Loss: 0.52178 Validation Loss: 0.54876\n",
            "Epoch: 7662 Train Loss: 0.52515 Validation Loss: 0.54916\n",
            "Epoch: 7663 Train Loss: 0.52383 Validation Loss: 0.54858\n",
            "Epoch: 7664 Train Loss: 0.52427 Validation Loss: 0.54821\n",
            "Epoch: 7665 Train Loss: 0.52177 Validation Loss: 0.54886\n",
            "Epoch: 7666 Train Loss: 0.52606 Validation Loss: 0.54938\n",
            "Epoch: 7667 Train Loss: 0.52363 Validation Loss: 0.54823\n",
            "Epoch: 7668 Train Loss: 0.52189 Validation Loss: 0.54844\n",
            "Epoch: 7669 Train Loss: 0.52510 Validation Loss: 0.54854\n",
            "Epoch: 7670 Train Loss: 0.52303 Validation Loss: 0.54807\n",
            "Epoch: 7671 Train Loss: 0.52180 Validation Loss: 0.54839\n",
            "Epoch: 7672 Train Loss: 0.52386 Validation Loss: 0.54916\n",
            "Epoch: 7673 Train Loss: 0.52605 Validation Loss: 0.54909\n",
            "Epoch: 7674 Train Loss: 0.52125 Validation Loss: 0.54797\n",
            "Epoch: 7675 Train Loss: 0.52095 Validation Loss: 0.54894\n",
            "Epoch: 7676 Train Loss: 0.52212 Validation Loss: 0.54933\n",
            "Epoch: 7677 Train Loss: 0.52220 Validation Loss: 0.54836\n",
            "Epoch: 7678 Train Loss: 0.52416 Validation Loss: 0.54851\n",
            "Epoch: 7679 Train Loss: 0.52101 Validation Loss: 0.54897\n",
            "Epoch: 7680 Train Loss: 0.52291 Validation Loss: 0.54885\n",
            "Epoch: 7681 Train Loss: 0.52135 Validation Loss: 0.54826\n",
            "Epoch: 7682 Train Loss: 0.52334 Validation Loss: 0.54952\n",
            "Epoch: 7683 Train Loss: 0.52475 Validation Loss: 0.54873\n",
            "Epoch: 7684 Train Loss: 0.52280 Validation Loss: 0.54839\n",
            "Epoch: 7685 Train Loss: 0.52420 Validation Loss: 0.54894\n",
            "Epoch: 7686 Train Loss: 0.52304 Validation Loss: 0.54820\n",
            "Epoch: 7687 Train Loss: 0.52376 Validation Loss: 0.54860\n",
            "Epoch: 7688 Train Loss: 0.52378 Validation Loss: 0.54876\n",
            "Epoch: 7689 Train Loss: 0.52092 Validation Loss: 0.54847\n",
            "Epoch: 7690 Train Loss: 0.52432 Validation Loss: 0.54930\n",
            "Epoch: 7691 Train Loss: 0.52403 Validation Loss: 0.54827\n",
            "Epoch: 7692 Train Loss: 0.52191 Validation Loss: 0.54855\n",
            "Epoch: 7693 Train Loss: 0.52418 Validation Loss: 0.54852\n",
            "Epoch: 7694 Train Loss: 0.52175 Validation Loss: 0.54900\n",
            "Epoch: 7695 Train Loss: 0.52696 Validation Loss: 0.54897\n",
            "Epoch: 7696 Train Loss: 0.52196 Validation Loss: 0.54874\n",
            "Epoch: 7697 Train Loss: 0.52187 Validation Loss: 0.54875\n",
            "Epoch: 7698 Train Loss: 0.52278 Validation Loss: 0.54854\n",
            "Epoch: 7699 Train Loss: 0.52106 Validation Loss: 0.54862\n",
            "Epoch: 7700 Train Loss: 0.52192 Validation Loss: 0.54839\n",
            "Epoch: 7701 Train Loss: 0.52380 Validation Loss: 0.54853\n",
            "Epoch: 7702 Train Loss: 0.52090 Validation Loss: 0.54891\n",
            "Epoch: 7703 Train Loss: 0.52397 Validation Loss: 0.54898\n",
            "Epoch: 7704 Train Loss: 0.52274 Validation Loss: 0.54868\n",
            "Epoch: 7705 Train Loss: 0.52316 Validation Loss: 0.54860\n",
            "Epoch: 7706 Train Loss: 0.52179 Validation Loss: 0.54873\n",
            "Epoch: 7707 Train Loss: 0.52500 Validation Loss: 0.54937\n",
            "Epoch: 7708 Train Loss: 0.52316 Validation Loss: 0.54843\n",
            "Epoch: 7709 Train Loss: 0.52382 Validation Loss: 0.54899\n",
            "Epoch: 7710 Train Loss: 0.52587 Validation Loss: 0.54876\n",
            "Epoch: 7711 Train Loss: 0.52294 Validation Loss: 0.54845\n",
            "Epoch: 7712 Train Loss: 0.52216 Validation Loss: 0.54875\n",
            "Epoch: 7713 Train Loss: 0.52677 Validation Loss: 0.54849\n",
            "Epoch: 7714 Train Loss: 0.52106 Validation Loss: 0.54797\n",
            "Epoch: 7715 Train Loss: 0.52473 Validation Loss: 0.54867\n",
            "Epoch: 7716 Train Loss: 0.52114 Validation Loss: 0.54848\n",
            "Epoch: 7717 Train Loss: 0.52383 Validation Loss: 0.54880\n",
            "Epoch: 7718 Train Loss: 0.52121 Validation Loss: 0.54908\n",
            "Epoch: 7719 Train Loss: 0.52176 Validation Loss: 0.54844\n",
            "Epoch: 7720 Train Loss: 0.52295 Validation Loss: 0.54850\n",
            "Epoch: 7721 Train Loss: 0.52287 Validation Loss: 0.54845\n",
            "Epoch: 7722 Train Loss: 0.52298 Validation Loss: 0.54890\n",
            "Epoch: 7723 Train Loss: 0.52729 Validation Loss: 0.54923\n",
            "Epoch: 7724 Train Loss: 0.52378 Validation Loss: 0.54824\n",
            "Epoch: 7725 Train Loss: 0.52395 Validation Loss: 0.54815\n",
            "Epoch: 7726 Train Loss: 0.52290 Validation Loss: 0.54898\n",
            "Epoch: 7727 Train Loss: 0.52307 Validation Loss: 0.54896\n",
            "Epoch: 7728 Train Loss: 0.52279 Validation Loss: 0.54848\n",
            "Epoch: 7729 Train Loss: 0.52381 Validation Loss: 0.54855\n",
            "Epoch: 7730 Train Loss: 0.52478 Validation Loss: 0.54855\n",
            "Epoch: 7731 Train Loss: 0.51982 Validation Loss: 0.54846\n",
            "Epoch: 7732 Train Loss: 0.52374 Validation Loss: 0.54867\n",
            "Epoch: 7733 Train Loss: 0.52510 Validation Loss: 0.54891\n",
            "Epoch: 7734 Train Loss: 0.52102 Validation Loss: 0.54816\n",
            "Epoch: 7735 Train Loss: 0.52185 Validation Loss: 0.54861\n",
            "Epoch: 7736 Train Loss: 0.52593 Validation Loss: 0.54916\n",
            "Epoch: 7737 Train Loss: 0.52296 Validation Loss: 0.54842\n",
            "Epoch: 7738 Train Loss: 0.52201 Validation Loss: 0.54887\n",
            "Epoch: 7739 Train Loss: 0.52213 Validation Loss: 0.54883\n",
            "Epoch: 7740 Train Loss: 0.52496 Validation Loss: 0.54831\n",
            "Epoch: 7741 Train Loss: 0.52312 Validation Loss: 0.54833\n",
            "Epoch: 7742 Train Loss: 0.52484 Validation Loss: 0.54924\n",
            "Epoch: 7743 Train Loss: 0.52077 Validation Loss: 0.54891\n",
            "Epoch: 7744 Train Loss: 0.52180 Validation Loss: 0.54867\n",
            "Epoch: 7745 Train Loss: 0.52209 Validation Loss: 0.54842\n",
            "Epoch: 7746 Train Loss: 0.52510 Validation Loss: 0.54838\n",
            "Epoch: 7747 Train Loss: 0.52504 Validation Loss: 0.54959\n",
            "Epoch: 7748 Train Loss: 0.52484 Validation Loss: 0.54879\n",
            "Epoch: 7749 Train Loss: 0.52493 Validation Loss: 0.54860\n",
            "Epoch: 7750 Train Loss: 0.52599 Validation Loss: 0.54850\n",
            "Epoch: 7751 Train Loss: 0.52374 Validation Loss: 0.54842\n",
            "Epoch: 7752 Train Loss: 0.52697 Validation Loss: 0.54861\n",
            "Epoch: 7753 Train Loss: 0.52476 Validation Loss: 0.54804\n",
            "Epoch: 7754 Train Loss: 0.52391 Validation Loss: 0.54856\n",
            "Epoch: 7755 Train Loss: 0.52180 Validation Loss: 0.54852\n",
            "Epoch: 7756 Train Loss: 0.52085 Validation Loss: 0.54836\n",
            "Epoch: 7757 Train Loss: 0.52231 Validation Loss: 0.54834\n",
            "Epoch: 7758 Train Loss: 0.52329 Validation Loss: 0.54962\n",
            "Epoch: 7759 Train Loss: 0.52204 Validation Loss: 0.54876\n",
            "Epoch: 7760 Train Loss: 0.52105 Validation Loss: 0.54841\n",
            "Epoch: 7761 Train Loss: 0.52002 Validation Loss: 0.54864\n",
            "Epoch: 7762 Train Loss: 0.52388 Validation Loss: 0.54871\n",
            "Epoch: 7763 Train Loss: 0.52288 Validation Loss: 0.54858\n",
            "Epoch: 7764 Train Loss: 0.52745 Validation Loss: 0.54923\n",
            "Epoch: 7765 Train Loss: 0.52193 Validation Loss: 0.54836\n",
            "Epoch: 7766 Train Loss: 0.52182 Validation Loss: 0.54849\n",
            "Epoch: 7767 Train Loss: 0.52503 Validation Loss: 0.54888\n",
            "Epoch: 7768 Train Loss: 0.52910 Validation Loss: 0.54858\n",
            "Epoch: 7769 Train Loss: 0.52537 Validation Loss: 0.54813\n",
            "Epoch: 7770 Train Loss: 0.52762 Validation Loss: 0.54917\n",
            "Epoch: 7771 Train Loss: 0.52287 Validation Loss: 0.54800\n",
            "Epoch: 7772 Train Loss: 0.52096 Validation Loss: 0.54828\n",
            "Epoch: 7773 Train Loss: 0.52376 Validation Loss: 0.54919\n",
            "Epoch: 7774 Train Loss: 0.52498 Validation Loss: 0.54865\n",
            "Epoch: 7775 Train Loss: 0.52210 Validation Loss: 0.54841\n",
            "Epoch: 7776 Train Loss: 0.52064 Validation Loss: 0.54893\n",
            "Epoch: 7777 Train Loss: 0.51990 Validation Loss: 0.54895\n",
            "Epoch: 7778 Train Loss: 0.52701 Validation Loss: 0.54889\n",
            "Epoch: 7779 Train Loss: 0.52183 Validation Loss: 0.54844\n",
            "Epoch: 7780 Train Loss: 0.52377 Validation Loss: 0.54839\n",
            "Epoch: 7781 Train Loss: 0.52311 Validation Loss: 0.54921\n",
            "Epoch: 7782 Train Loss: 0.51907 Validation Loss: 0.54812\n",
            "Epoch: 7783 Train Loss: 0.52524 Validation Loss: 0.54896\n",
            "Epoch: 7784 Train Loss: 0.52497 Validation Loss: 0.54847\n",
            "Epoch: 7785 Train Loss: 0.52381 Validation Loss: 0.54937\n",
            "Epoch: 7786 Train Loss: 0.52404 Validation Loss: 0.54791\n",
            "Epoch: 7787 Train Loss: 0.52233 Validation Loss: 0.54932\n",
            "Epoch: 7788 Train Loss: 0.52415 Validation Loss: 0.54821\n",
            "Epoch: 7789 Train Loss: 0.52081 Validation Loss: 0.54858\n",
            "Epoch: 7790 Train Loss: 0.52593 Validation Loss: 0.54955\n",
            "Epoch: 7791 Train Loss: 0.52077 Validation Loss: 0.54826\n",
            "Epoch: 7792 Train Loss: 0.52498 Validation Loss: 0.54860\n",
            "Epoch: 7793 Train Loss: 0.52088 Validation Loss: 0.54845\n",
            "Epoch: 7794 Train Loss: 0.52278 Validation Loss: 0.54876\n",
            "Epoch: 7795 Train Loss: 0.52295 Validation Loss: 0.54861\n",
            "Epoch: 7796 Train Loss: 0.52488 Validation Loss: 0.54903\n",
            "Epoch: 7797 Train Loss: 0.52945 Validation Loss: 0.54944\n",
            "Epoch: 7798 Train Loss: 0.52238 Validation Loss: 0.54796\n",
            "Epoch: 7799 Train Loss: 0.52430 Validation Loss: 0.54847\n",
            "Epoch: 7800 Train Loss: 0.52397 Validation Loss: 0.54801\n",
            "Epoch: 7801 Train Loss: 0.52184 Validation Loss: 0.54851\n",
            "Epoch: 7802 Train Loss: 0.52408 Validation Loss: 0.54835\n",
            "Epoch: 7803 Train Loss: 0.52441 Validation Loss: 0.54855\n",
            "Epoch: 7804 Train Loss: 0.52338 Validation Loss: 0.54995\n",
            "Epoch: 7805 Train Loss: 0.52160 Validation Loss: 0.54815\n",
            "Epoch: 7806 Train Loss: 0.52265 Validation Loss: 0.54978\n",
            "Epoch: 7807 Train Loss: 0.52407 Validation Loss: 0.54853\n",
            "Epoch: 7808 Train Loss: 0.52174 Validation Loss: 0.54840\n",
            "Epoch: 7809 Train Loss: 0.52119 Validation Loss: 0.54834\n",
            "Epoch: 7810 Train Loss: 0.52193 Validation Loss: 0.54944\n",
            "Epoch: 7811 Train Loss: 0.52291 Validation Loss: 0.54976\n",
            "Epoch: 7812 Train Loss: 0.52398 Validation Loss: 0.54901\n",
            "Epoch: 7813 Train Loss: 0.52487 Validation Loss: 0.54855\n",
            "Epoch: 7814 Train Loss: 0.52482 Validation Loss: 0.54826\n",
            "Epoch: 7815 Train Loss: 0.52391 Validation Loss: 0.54843\n",
            "Epoch: 7816 Train Loss: 0.52377 Validation Loss: 0.54871\n",
            "Epoch: 7817 Train Loss: 0.52601 Validation Loss: 0.54911\n",
            "Epoch: 7818 Train Loss: 0.52517 Validation Loss: 0.54850\n",
            "Epoch: 7819 Train Loss: 0.52294 Validation Loss: 0.54869\n",
            "Epoch: 7820 Train Loss: 0.52182 Validation Loss: 0.54839\n",
            "Epoch: 7821 Train Loss: 0.52491 Validation Loss: 0.54880\n",
            "Epoch: 7822 Train Loss: 0.52200 Validation Loss: 0.54829\n",
            "Epoch: 7823 Train Loss: 0.52286 Validation Loss: 0.54937\n",
            "Epoch: 7824 Train Loss: 0.52509 Validation Loss: 0.54892\n",
            "Epoch: 7825 Train Loss: 0.52391 Validation Loss: 0.54858\n",
            "Epoch: 7826 Train Loss: 0.52282 Validation Loss: 0.54830\n",
            "Epoch: 7827 Train Loss: 0.52938 Validation Loss: 0.54868\n",
            "Epoch: 7828 Train Loss: 0.52488 Validation Loss: 0.54836\n",
            "Epoch: 7829 Train Loss: 0.52381 Validation Loss: 0.54817\n",
            "Epoch: 7830 Train Loss: 0.52194 Validation Loss: 0.54802\n",
            "Epoch: 7831 Train Loss: 0.52372 Validation Loss: 0.54857\n",
            "Epoch: 7832 Train Loss: 0.52281 Validation Loss: 0.54915\n",
            "Epoch: 7833 Train Loss: 0.52298 Validation Loss: 0.54878\n",
            "Epoch: 7834 Train Loss: 0.52640 Validation Loss: 0.54844\n",
            "Epoch: 7835 Train Loss: 0.52303 Validation Loss: 0.54882\n",
            "Epoch: 7836 Train Loss: 0.52564 Validation Loss: 0.54829\n",
            "Epoch: 7837 Train Loss: 0.52220 Validation Loss: 0.54866\n",
            "Epoch: 7838 Train Loss: 0.52343 Validation Loss: 0.54893\n",
            "Epoch: 7839 Train Loss: 0.52200 Validation Loss: 0.54898\n",
            "Epoch: 7840 Train Loss: 0.52376 Validation Loss: 0.54853\n",
            "Epoch: 7841 Train Loss: 0.51900 Validation Loss: 0.54822\n",
            "Epoch: 7842 Train Loss: 0.52409 Validation Loss: 0.54920\n",
            "Epoch: 7843 Train Loss: 0.52383 Validation Loss: 0.54855\n",
            "Epoch: 7844 Train Loss: 0.52031 Validation Loss: 0.54811\n",
            "Epoch: 7845 Train Loss: 0.52531 Validation Loss: 0.54971\n",
            "Epoch: 7846 Train Loss: 0.52015 Validation Loss: 0.54810\n",
            "Epoch: 7847 Train Loss: 0.52604 Validation Loss: 0.54945\n",
            "Epoch: 7848 Train Loss: 0.52109 Validation Loss: 0.54797\n",
            "Epoch: 7849 Train Loss: 0.52274 Validation Loss: 0.54878\n",
            "Epoch: 7850 Train Loss: 0.52478 Validation Loss: 0.54876\n",
            "Epoch: 7851 Train Loss: 0.52391 Validation Loss: 0.54845\n",
            "Epoch: 7852 Train Loss: 0.52288 Validation Loss: 0.54876\n",
            "Epoch: 7853 Train Loss: 0.52290 Validation Loss: 0.54840\n",
            "Epoch: 7854 Train Loss: 0.52234 Validation Loss: 0.54854\n",
            "Epoch: 7855 Train Loss: 0.51994 Validation Loss: 0.54873\n",
            "Epoch: 7856 Train Loss: 0.52491 Validation Loss: 0.54900\n",
            "Epoch: 7857 Train Loss: 0.52627 Validation Loss: 0.54894\n",
            "Epoch: 7858 Train Loss: 0.52216 Validation Loss: 0.54805\n",
            "Epoch: 7859 Train Loss: 0.52291 Validation Loss: 0.54858\n",
            "Epoch: 7860 Train Loss: 0.52100 Validation Loss: 0.54852\n",
            "Epoch: 7861 Train Loss: 0.52611 Validation Loss: 0.54929\n",
            "Epoch: 7862 Train Loss: 0.52206 Validation Loss: 0.54878\n",
            "Epoch: 7863 Train Loss: 0.52282 Validation Loss: 0.54838\n",
            "Epoch: 7864 Train Loss: 0.52477 Validation Loss: 0.54851\n",
            "Epoch: 7865 Train Loss: 0.52520 Validation Loss: 0.54923\n",
            "Epoch: 7866 Train Loss: 0.52243 Validation Loss: 0.54794\n",
            "Epoch: 7867 Train Loss: 0.52292 Validation Loss: 0.54843\n",
            "Epoch: 7868 Train Loss: 0.52389 Validation Loss: 0.54946\n",
            "Epoch: 7869 Train Loss: 0.52397 Validation Loss: 0.54909\n",
            "Epoch: 7870 Train Loss: 0.52530 Validation Loss: 0.54811\n",
            "Epoch: 7871 Train Loss: 0.52196 Validation Loss: 0.54846\n",
            "Epoch: 7872 Train Loss: 0.51913 Validation Loss: 0.54836\n",
            "Epoch: 7873 Train Loss: 0.52236 Validation Loss: 0.54822\n",
            "Epoch: 7874 Train Loss: 0.52376 Validation Loss: 0.54887\n",
            "Epoch: 7875 Train Loss: 0.52289 Validation Loss: 0.54891\n",
            "Epoch: 7876 Train Loss: 0.52402 Validation Loss: 0.54852\n",
            "Epoch: 7877 Train Loss: 0.52285 Validation Loss: 0.54865\n",
            "Epoch: 7878 Train Loss: 0.52377 Validation Loss: 0.54851\n",
            "Epoch: 7879 Train Loss: 0.52410 Validation Loss: 0.54865\n",
            "Epoch: 7880 Train Loss: 0.52085 Validation Loss: 0.54828\n",
            "Epoch: 7881 Train Loss: 0.52649 Validation Loss: 0.54967\n",
            "Epoch: 7882 Train Loss: 0.52494 Validation Loss: 0.54814\n",
            "Epoch: 7883 Train Loss: 0.52218 Validation Loss: 0.54861\n",
            "Epoch: 7884 Train Loss: 0.52278 Validation Loss: 0.54840\n",
            "Epoch: 7885 Train Loss: 0.52218 Validation Loss: 0.54824\n",
            "Epoch: 7886 Train Loss: 0.52379 Validation Loss: 0.54872\n",
            "Epoch: 7887 Train Loss: 0.52277 Validation Loss: 0.54862\n",
            "Epoch: 7888 Train Loss: 0.52399 Validation Loss: 0.54900\n",
            "Epoch: 7889 Train Loss: 0.52409 Validation Loss: 0.54915\n",
            "Epoch: 7890 Train Loss: 0.52511 Validation Loss: 0.54896\n",
            "Epoch: 7891 Train Loss: 0.52579 Validation Loss: 0.54812\n",
            "Epoch: 7892 Train Loss: 0.52383 Validation Loss: 0.54865\n",
            "Epoch: 7893 Train Loss: 0.52128 Validation Loss: 0.54809\n",
            "Epoch: 7894 Train Loss: 0.52277 Validation Loss: 0.54915\n",
            "Epoch: 7895 Train Loss: 0.52433 Validation Loss: 0.54853\n",
            "Epoch: 7896 Train Loss: 0.52178 Validation Loss: 0.54873\n",
            "Epoch: 7897 Train Loss: 0.52754 Validation Loss: 0.54891\n",
            "Epoch: 7898 Train Loss: 0.52307 Validation Loss: 0.54892\n",
            "Epoch: 7899 Train Loss: 0.52508 Validation Loss: 0.54886\n",
            "Epoch: 7900 Train Loss: 0.52112 Validation Loss: 0.54802\n",
            "Epoch: 7901 Train Loss: 0.52195 Validation Loss: 0.54904\n",
            "Epoch: 7902 Train Loss: 0.52488 Validation Loss: 0.54868\n",
            "Epoch: 7903 Train Loss: 0.52282 Validation Loss: 0.54841\n",
            "Epoch: 7904 Train Loss: 0.52194 Validation Loss: 0.54834\n",
            "Epoch: 7905 Train Loss: 0.52360 Validation Loss: 0.54956\n",
            "Epoch: 7906 Train Loss: 0.52322 Validation Loss: 0.54861\n",
            "Epoch: 7907 Train Loss: 0.52480 Validation Loss: 0.54838\n",
            "Epoch: 7908 Train Loss: 0.52393 Validation Loss: 0.54842\n",
            "Epoch: 7909 Train Loss: 0.52372 Validation Loss: 0.54885\n",
            "Epoch: 7910 Train Loss: 0.52289 Validation Loss: 0.54891\n",
            "Epoch: 7911 Train Loss: 0.52250 Validation Loss: 0.54816\n",
            "Epoch: 7912 Train Loss: 0.52361 Validation Loss: 0.54914\n",
            "Epoch: 7913 Train Loss: 0.52316 Validation Loss: 0.54957\n",
            "Epoch: 7914 Train Loss: 0.52324 Validation Loss: 0.54803\n",
            "Epoch: 7915 Train Loss: 0.52221 Validation Loss: 0.54890\n",
            "Epoch: 7916 Train Loss: 0.52510 Validation Loss: 0.54845\n",
            "Epoch: 7917 Train Loss: 0.52484 Validation Loss: 0.54895\n",
            "Epoch: 7918 Train Loss: 0.52290 Validation Loss: 0.54840\n",
            "Epoch: 7919 Train Loss: 0.52477 Validation Loss: 0.54891\n",
            "Epoch: 7920 Train Loss: 0.52327 Validation Loss: 0.54853\n",
            "Epoch: 7921 Train Loss: 0.52174 Validation Loss: 0.54897\n",
            "Epoch: 7922 Train Loss: 0.52212 Validation Loss: 0.54852\n",
            "Epoch: 7923 Train Loss: 0.52027 Validation Loss: 0.54841\n",
            "Epoch: 7924 Train Loss: 0.52358 Validation Loss: 0.55017\n",
            "Epoch: 7925 Train Loss: 0.52291 Validation Loss: 0.54840\n",
            "Epoch: 7926 Train Loss: 0.52581 Validation Loss: 0.54864\n",
            "Epoch: 7927 Train Loss: 0.52185 Validation Loss: 0.54858\n",
            "Epoch: 7928 Train Loss: 0.52495 Validation Loss: 0.54921\n",
            "Epoch: 7929 Train Loss: 0.52401 Validation Loss: 0.54825\n",
            "Epoch: 7930 Train Loss: 0.51993 Validation Loss: 0.54821\n",
            "Epoch: 7931 Train Loss: 0.52390 Validation Loss: 0.54880\n",
            "Epoch: 7932 Train Loss: 0.52287 Validation Loss: 0.54901\n",
            "Epoch: 7933 Train Loss: 0.52092 Validation Loss: 0.54834\n",
            "Epoch: 7934 Train Loss: 0.52493 Validation Loss: 0.54853\n",
            "Epoch: 7935 Train Loss: 0.52338 Validation Loss: 0.54833\n",
            "Epoch: 7936 Train Loss: 0.52371 Validation Loss: 0.54934\n",
            "Epoch: 7937 Train Loss: 0.52304 Validation Loss: 0.54897\n",
            "Epoch: 7938 Train Loss: 0.52407 Validation Loss: 0.54843\n",
            "Epoch: 7939 Train Loss: 0.52171 Validation Loss: 0.54850\n",
            "Epoch: 7940 Train Loss: 0.52388 Validation Loss: 0.54937\n",
            "Epoch: 7941 Train Loss: 0.52098 Validation Loss: 0.54866\n",
            "Epoch: 7942 Train Loss: 0.52348 Validation Loss: 0.54853\n",
            "Epoch: 7943 Train Loss: 0.52247 Validation Loss: 0.54870\n",
            "Epoch: 7944 Train Loss: 0.52188 Validation Loss: 0.54904\n",
            "Epoch: 7945 Train Loss: 0.52376 Validation Loss: 0.54870\n",
            "Epoch: 7946 Train Loss: 0.52444 Validation Loss: 0.54919\n",
            "Epoch: 7947 Train Loss: 0.52300 Validation Loss: 0.54868\n",
            "Epoch: 7948 Train Loss: 0.52183 Validation Loss: 0.54844\n",
            "Epoch: 7949 Train Loss: 0.52400 Validation Loss: 0.54832\n",
            "Epoch: 7950 Train Loss: 0.52216 Validation Loss: 0.54862\n",
            "Epoch: 7951 Train Loss: 0.52080 Validation Loss: 0.54934\n",
            "Epoch: 7952 Train Loss: 0.52620 Validation Loss: 0.54908\n",
            "Epoch: 7953 Train Loss: 0.52294 Validation Loss: 0.54857\n",
            "Epoch: 7954 Train Loss: 0.52293 Validation Loss: 0.54866\n",
            "Epoch: 7955 Train Loss: 0.52807 Validation Loss: 0.54934\n",
            "Epoch: 7956 Train Loss: 0.52493 Validation Loss: 0.54825\n",
            "Epoch: 7957 Train Loss: 0.52302 Validation Loss: 0.54848\n",
            "Epoch: 7958 Train Loss: 0.51976 Validation Loss: 0.54845\n",
            "Epoch: 7959 Train Loss: 0.52372 Validation Loss: 0.54892\n",
            "Epoch: 7960 Train Loss: 0.52413 Validation Loss: 0.54933\n",
            "Epoch: 7961 Train Loss: 0.52600 Validation Loss: 0.54902\n",
            "Epoch: 7962 Train Loss: 0.52769 Validation Loss: 0.54844\n",
            "Epoch: 7963 Train Loss: 0.52316 Validation Loss: 0.54782\n",
            "Epoch: 7964 Train Loss: 0.52269 Validation Loss: 0.54859\n",
            "Epoch: 7965 Train Loss: 0.52258 Validation Loss: 0.54812\n",
            "Epoch: 7966 Train Loss: 0.52325 Validation Loss: 0.54972\n",
            "Epoch: 7967 Train Loss: 0.52403 Validation Loss: 0.54880\n",
            "Epoch: 7968 Train Loss: 0.52088 Validation Loss: 0.54838\n",
            "Epoch: 7969 Train Loss: 0.52306 Validation Loss: 0.54898\n",
            "Epoch: 7970 Train Loss: 0.52210 Validation Loss: 0.54818\n",
            "Epoch: 7971 Train Loss: 0.52635 Validation Loss: 0.54965\n",
            "Epoch: 7972 Train Loss: 0.52694 Validation Loss: 0.54876\n",
            "Epoch: 7973 Train Loss: 0.52364 Validation Loss: 0.54809\n",
            "Epoch: 7974 Train Loss: 0.52192 Validation Loss: 0.54857\n",
            "Epoch: 7975 Train Loss: 0.52502 Validation Loss: 0.54879\n",
            "Epoch: 7976 Train Loss: 0.52181 Validation Loss: 0.54900\n",
            "Epoch: 7977 Train Loss: 0.52092 Validation Loss: 0.54876\n",
            "Epoch: 7978 Train Loss: 0.52332 Validation Loss: 0.54906\n",
            "Epoch: 7979 Train Loss: 0.52223 Validation Loss: 0.54804\n",
            "Epoch: 7980 Train Loss: 0.52396 Validation Loss: 0.54902\n",
            "Epoch: 7981 Train Loss: 0.52301 Validation Loss: 0.54924\n",
            "Epoch: 7982 Train Loss: 0.52275 Validation Loss: 0.54859\n",
            "Epoch: 7983 Train Loss: 0.52182 Validation Loss: 0.54838\n",
            "Epoch: 7984 Train Loss: 0.52280 Validation Loss: 0.54860\n",
            "Epoch: 7985 Train Loss: 0.52376 Validation Loss: 0.54864\n",
            "Epoch: 7986 Train Loss: 0.52272 Validation Loss: 0.54867\n",
            "Epoch: 7987 Train Loss: 0.52610 Validation Loss: 0.54924\n",
            "Epoch: 7988 Train Loss: 0.52638 Validation Loss: 0.54857\n",
            "Epoch: 7989 Train Loss: 0.52206 Validation Loss: 0.54812\n",
            "Epoch: 7990 Train Loss: 0.52375 Validation Loss: 0.54860\n",
            "Epoch: 7991 Train Loss: 0.52389 Validation Loss: 0.54884\n",
            "Epoch: 7992 Train Loss: 0.52379 Validation Loss: 0.54844\n",
            "Epoch: 7993 Train Loss: 0.52492 Validation Loss: 0.54873\n",
            "Epoch: 7994 Train Loss: 0.52415 Validation Loss: 0.54834\n",
            "Epoch: 7995 Train Loss: 0.52300 Validation Loss: 0.54810\n",
            "Epoch: 7996 Train Loss: 0.52390 Validation Loss: 0.54865\n",
            "Epoch: 7997 Train Loss: 0.52176 Validation Loss: 0.54903\n",
            "Epoch: 7998 Train Loss: 0.52199 Validation Loss: 0.54920\n",
            "Epoch: 7999 Train Loss: 0.52490 Validation Loss: 0.54886\n",
            "Epoch: 8000 Train Loss: 0.52492 Validation Loss: 0.54851\n",
            "Epoch: 8001 Train Loss: 0.52379 Validation Loss: 0.54839\n",
            "Epoch: 8002 Train Loss: 0.52620 Validation Loss: 0.54871\n",
            "Epoch: 8003 Train Loss: 0.52093 Validation Loss: 0.54835\n",
            "Epoch: 8004 Train Loss: 0.52083 Validation Loss: 0.54839\n",
            "Epoch: 8005 Train Loss: 0.52405 Validation Loss: 0.54901\n",
            "Epoch: 8006 Train Loss: 0.52227 Validation Loss: 0.54809\n",
            "Epoch: 8007 Train Loss: 0.52403 Validation Loss: 0.54868\n",
            "Epoch: 8008 Train Loss: 0.51993 Validation Loss: 0.54831\n",
            "Epoch: 8009 Train Loss: 0.52524 Validation Loss: 0.54907\n",
            "Epoch: 8010 Train Loss: 0.52724 Validation Loss: 0.54836\n",
            "Epoch: 8011 Train Loss: 0.52594 Validation Loss: 0.54834\n",
            "Epoch: 8012 Train Loss: 0.52588 Validation Loss: 0.54876\n",
            "Epoch: 8013 Train Loss: 0.52816 Validation Loss: 0.54881\n",
            "Epoch: 8014 Train Loss: 0.52482 Validation Loss: 0.54857\n",
            "Epoch: 8015 Train Loss: 0.52386 Validation Loss: 0.54821\n",
            "Epoch: 8016 Train Loss: 0.52283 Validation Loss: 0.54829\n",
            "Epoch: 8017 Train Loss: 0.52305 Validation Loss: 0.54862\n",
            "Epoch: 8018 Train Loss: 0.52118 Validation Loss: 0.54832\n",
            "Epoch: 8019 Train Loss: 0.52297 Validation Loss: 0.54980\n",
            "Epoch: 8020 Train Loss: 0.52509 Validation Loss: 0.54818\n",
            "Epoch: 8021 Train Loss: 0.52224 Validation Loss: 0.54905\n",
            "Epoch: 8022 Train Loss: 0.52407 Validation Loss: 0.54882\n",
            "Epoch: 8023 Train Loss: 0.52331 Validation Loss: 0.54837\n",
            "Epoch: 8024 Train Loss: 0.52709 Validation Loss: 0.54865\n",
            "Epoch: 8025 Train Loss: 0.52446 Validation Loss: 0.54982\n",
            "Epoch: 8026 Train Loss: 0.52109 Validation Loss: 0.54808\n",
            "Epoch: 8027 Train Loss: 0.52226 Validation Loss: 0.54956\n",
            "Epoch: 8028 Train Loss: 0.52492 Validation Loss: 0.54853\n",
            "Epoch: 8029 Train Loss: 0.51998 Validation Loss: 0.54810\n",
            "Epoch: 8030 Train Loss: 0.52280 Validation Loss: 0.54855\n",
            "Epoch: 8031 Train Loss: 0.52381 Validation Loss: 0.54877\n",
            "Epoch: 8032 Train Loss: 0.52221 Validation Loss: 0.54964\n",
            "Epoch: 8033 Train Loss: 0.52494 Validation Loss: 0.54900\n",
            "Epoch: 8034 Train Loss: 0.52302 Validation Loss: 0.54811\n",
            "Epoch: 8035 Train Loss: 0.52495 Validation Loss: 0.54909\n",
            "Epoch: 8036 Train Loss: 0.52322 Validation Loss: 0.54812\n",
            "Epoch: 8037 Train Loss: 0.52796 Validation Loss: 0.54949\n",
            "Epoch: 8038 Train Loss: 0.52612 Validation Loss: 0.54776\n",
            "Epoch: 8039 Train Loss: 0.52386 Validation Loss: 0.54866\n",
            "Epoch: 8040 Train Loss: 0.52497 Validation Loss: 0.54875\n",
            "Epoch: 8041 Train Loss: 0.52314 Validation Loss: 0.54909\n",
            "Epoch: 8042 Train Loss: 0.52468 Validation Loss: 0.54838\n",
            "Epoch: 8043 Train Loss: 0.52412 Validation Loss: 0.54893\n",
            "Epoch: 8044 Train Loss: 0.52086 Validation Loss: 0.54810\n",
            "Epoch: 8045 Train Loss: 0.52294 Validation Loss: 0.54836\n",
            "Epoch: 8046 Train Loss: 0.52708 Validation Loss: 0.54848\n",
            "Epoch: 8047 Train Loss: 0.52389 Validation Loss: 0.54897\n",
            "Epoch: 8048 Train Loss: 0.52177 Validation Loss: 0.54866\n",
            "Epoch: 8049 Train Loss: 0.51978 Validation Loss: 0.54833\n",
            "Epoch: 8050 Train Loss: 0.52492 Validation Loss: 0.54902\n",
            "Epoch: 8051 Train Loss: 0.52277 Validation Loss: 0.54822\n",
            "Epoch: 8052 Train Loss: 0.52517 Validation Loss: 0.54848\n",
            "Epoch: 8053 Train Loss: 0.52274 Validation Loss: 0.54830\n",
            "Epoch: 8054 Train Loss: 0.52177 Validation Loss: 0.54849\n",
            "Epoch: 8055 Train Loss: 0.52471 Validation Loss: 0.54893\n",
            "Epoch: 8056 Train Loss: 0.52275 Validation Loss: 0.54875\n",
            "Epoch: 8057 Train Loss: 0.52292 Validation Loss: 0.54837\n",
            "Epoch: 8058 Train Loss: 0.52394 Validation Loss: 0.54823\n",
            "Epoch: 8059 Train Loss: 0.52179 Validation Loss: 0.54876\n",
            "Epoch: 8060 Train Loss: 0.52526 Validation Loss: 0.54900\n",
            "Epoch: 8061 Train Loss: 0.52282 Validation Loss: 0.54848\n",
            "Epoch: 8062 Train Loss: 0.52183 Validation Loss: 0.54805\n",
            "Epoch: 8063 Train Loss: 0.52222 Validation Loss: 0.54932\n",
            "Epoch: 8064 Train Loss: 0.52113 Validation Loss: 0.54883\n",
            "Epoch: 8065 Train Loss: 0.52279 Validation Loss: 0.54840\n",
            "Epoch: 8066 Train Loss: 0.52491 Validation Loss: 0.54880\n",
            "Epoch: 8067 Train Loss: 0.52200 Validation Loss: 0.54833\n",
            "Epoch: 8068 Train Loss: 0.52413 Validation Loss: 0.54906\n",
            "Epoch: 8069 Train Loss: 0.52483 Validation Loss: 0.54844\n",
            "Epoch: 8070 Train Loss: 0.52303 Validation Loss: 0.54832\n",
            "Epoch: 8071 Train Loss: 0.52187 Validation Loss: 0.54889\n",
            "Epoch: 8072 Train Loss: 0.52100 Validation Loss: 0.54838\n",
            "Epoch: 8073 Train Loss: 0.52282 Validation Loss: 0.54868\n",
            "Epoch: 8074 Train Loss: 0.52827 Validation Loss: 0.54985\n",
            "Epoch: 8075 Train Loss: 0.52358 Validation Loss: 0.54811\n",
            "Epoch: 8076 Train Loss: 0.52377 Validation Loss: 0.54867\n",
            "Epoch: 8077 Train Loss: 0.52482 Validation Loss: 0.54885\n",
            "Epoch: 8078 Train Loss: 0.52301 Validation Loss: 0.54861\n",
            "Epoch: 8079 Train Loss: 0.52377 Validation Loss: 0.54846\n",
            "Epoch: 8080 Train Loss: 0.52284 Validation Loss: 0.54871\n",
            "Epoch: 8081 Train Loss: 0.52503 Validation Loss: 0.54875\n",
            "Epoch: 8082 Train Loss: 0.52253 Validation Loss: 0.54792\n",
            "Epoch: 8083 Train Loss: 0.52289 Validation Loss: 0.54985\n",
            "Epoch: 8084 Train Loss: 0.52315 Validation Loss: 0.54829\n",
            "Epoch: 8085 Train Loss: 0.52429 Validation Loss: 0.54843\n",
            "Epoch: 8086 Train Loss: 0.52239 Validation Loss: 0.54873\n",
            "Epoch: 8087 Train Loss: 0.52417 Validation Loss: 0.54877\n",
            "Epoch: 8088 Train Loss: 0.52119 Validation Loss: 0.54817\n",
            "Epoch: 8089 Train Loss: 0.52758 Validation Loss: 0.54956\n",
            "Epoch: 8090 Train Loss: 0.52587 Validation Loss: 0.54826\n",
            "Epoch: 8091 Train Loss: 0.52191 Validation Loss: 0.54816\n",
            "Epoch: 8092 Train Loss: 0.52807 Validation Loss: 0.54859\n",
            "Epoch: 8093 Train Loss: 0.52426 Validation Loss: 0.54804\n",
            "Epoch: 8094 Train Loss: 0.52486 Validation Loss: 0.54962\n",
            "Epoch: 8095 Train Loss: 0.52671 Validation Loss: 0.54849\n",
            "Epoch: 8096 Train Loss: 0.52423 Validation Loss: 0.54791\n",
            "Epoch: 8097 Train Loss: 0.52214 Validation Loss: 0.54884\n",
            "Epoch: 8098 Train Loss: 0.52517 Validation Loss: 0.54888\n",
            "Epoch: 8099 Train Loss: 0.52312 Validation Loss: 0.54835\n",
            "Epoch: 8100 Train Loss: 0.52091 Validation Loss: 0.54824\n",
            "Epoch: 8101 Train Loss: 0.52365 Validation Loss: 0.54910\n",
            "Epoch: 8102 Train Loss: 0.52500 Validation Loss: 0.54858\n",
            "Epoch: 8103 Train Loss: 0.52601 Validation Loss: 0.54868\n",
            "Epoch: 8104 Train Loss: 0.52489 Validation Loss: 0.54815\n",
            "Epoch: 8105 Train Loss: 0.52610 Validation Loss: 0.54821\n",
            "Epoch: 8106 Train Loss: 0.52485 Validation Loss: 0.54887\n",
            "Epoch: 8107 Train Loss: 0.52902 Validation Loss: 0.54882\n",
            "Epoch: 8108 Train Loss: 0.52459 Validation Loss: 0.54810\n",
            "Epoch: 8109 Train Loss: 0.52388 Validation Loss: 0.54801\n",
            "Epoch: 8110 Train Loss: 0.52528 Validation Loss: 0.54959\n",
            "Epoch: 8111 Train Loss: 0.52097 Validation Loss: 0.54818\n",
            "Epoch: 8112 Train Loss: 0.52169 Validation Loss: 0.54878\n",
            "Epoch: 8113 Train Loss: 0.52586 Validation Loss: 0.54916\n",
            "Epoch: 8114 Train Loss: 0.52186 Validation Loss: 0.54862\n",
            "Epoch: 8115 Train Loss: 0.52192 Validation Loss: 0.54857\n",
            "Epoch: 8116 Train Loss: 0.52384 Validation Loss: 0.54890\n",
            "Epoch: 8117 Train Loss: 0.52029 Validation Loss: 0.54841\n",
            "Epoch: 8118 Train Loss: 0.51989 Validation Loss: 0.54879\n",
            "Epoch: 8119 Train Loss: 0.52311 Validation Loss: 0.54919\n",
            "Epoch: 8120 Train Loss: 0.52485 Validation Loss: 0.54892\n",
            "Epoch: 8121 Train Loss: 0.52616 Validation Loss: 0.54854\n",
            "Epoch: 8122 Train Loss: 0.52384 Validation Loss: 0.54808\n",
            "Epoch: 8123 Train Loss: 0.52629 Validation Loss: 0.54889\n",
            "Epoch: 8124 Train Loss: 0.52269 Validation Loss: 0.54828\n",
            "Epoch: 8125 Train Loss: 0.52399 Validation Loss: 0.54885\n",
            "Epoch: 8126 Train Loss: 0.52602 Validation Loss: 0.54846\n",
            "Epoch: 8127 Train Loss: 0.52417 Validation Loss: 0.54859\n",
            "Epoch: 8128 Train Loss: 0.52389 Validation Loss: 0.54850\n",
            "Epoch: 8129 Train Loss: 0.52602 Validation Loss: 0.54827\n",
            "Epoch: 8130 Train Loss: 0.52194 Validation Loss: 0.54891\n",
            "Epoch: 8131 Train Loss: 0.52286 Validation Loss: 0.54867\n",
            "Epoch: 8132 Train Loss: 0.52304 Validation Loss: 0.54873\n",
            "Epoch: 8133 Train Loss: 0.52175 Validation Loss: 0.54860\n",
            "Epoch: 8134 Train Loss: 0.52489 Validation Loss: 0.54864\n",
            "Epoch: 8135 Train Loss: 0.52206 Validation Loss: 0.54817\n",
            "Epoch: 8136 Train Loss: 0.52740 Validation Loss: 0.54899\n",
            "Epoch: 8137 Train Loss: 0.52279 Validation Loss: 0.54829\n",
            "Epoch: 8138 Train Loss: 0.52295 Validation Loss: 0.54848\n",
            "Epoch: 8139 Train Loss: 0.52472 Validation Loss: 0.54852\n",
            "Epoch: 8140 Train Loss: 0.52283 Validation Loss: 0.54841\n",
            "Epoch: 8141 Train Loss: 0.52080 Validation Loss: 0.54840\n",
            "Epoch: 8142 Train Loss: 0.52409 Validation Loss: 0.54960\n",
            "Epoch: 8143 Train Loss: 0.52095 Validation Loss: 0.54850\n",
            "Epoch: 8144 Train Loss: 0.52379 Validation Loss: 0.54863\n",
            "Epoch: 8145 Train Loss: 0.52491 Validation Loss: 0.54845\n",
            "Epoch: 8146 Train Loss: 0.52483 Validation Loss: 0.54849\n",
            "Epoch: 8147 Train Loss: 0.52227 Validation Loss: 0.54846\n",
            "Epoch: 8148 Train Loss: 0.52387 Validation Loss: 0.54822\n",
            "Epoch: 8149 Train Loss: 0.52090 Validation Loss: 0.54869\n",
            "Epoch: 8150 Train Loss: 0.52278 Validation Loss: 0.54858\n",
            "Epoch: 8151 Train Loss: 0.52408 Validation Loss: 0.54919\n",
            "Epoch: 8152 Train Loss: 0.52240 Validation Loss: 0.54811\n",
            "Epoch: 8153 Train Loss: 0.52206 Validation Loss: 0.54900\n",
            "Epoch: 8154 Train Loss: 0.52191 Validation Loss: 0.54882\n",
            "Epoch: 8155 Train Loss: 0.52316 Validation Loss: 0.54825\n",
            "Epoch: 8156 Train Loss: 0.52284 Validation Loss: 0.54870\n",
            "Epoch: 8157 Train Loss: 0.52291 Validation Loss: 0.54882\n",
            "Epoch: 8158 Train Loss: 0.52395 Validation Loss: 0.54864\n",
            "Epoch: 8159 Train Loss: 0.52396 Validation Loss: 0.54916\n",
            "Epoch: 8160 Train Loss: 0.52301 Validation Loss: 0.54831\n",
            "Epoch: 8161 Train Loss: 0.52397 Validation Loss: 0.54867\n",
            "Epoch: 8162 Train Loss: 0.52080 Validation Loss: 0.54848\n",
            "Epoch: 8163 Train Loss: 0.52196 Validation Loss: 0.54912\n",
            "Epoch: 8164 Train Loss: 0.52368 Validation Loss: 0.54847\n",
            "Epoch: 8165 Train Loss: 0.52283 Validation Loss: 0.54926\n",
            "Epoch: 8166 Train Loss: 0.52480 Validation Loss: 0.54889\n",
            "Epoch: 8167 Train Loss: 0.52323 Validation Loss: 0.54907\n",
            "Epoch: 8168 Train Loss: 0.52508 Validation Loss: 0.54837\n",
            "Epoch: 8169 Train Loss: 0.52317 Validation Loss: 0.54821\n",
            "Epoch: 8170 Train Loss: 0.52435 Validation Loss: 0.54807\n",
            "Epoch: 8171 Train Loss: 0.52730 Validation Loss: 0.54930\n",
            "Epoch: 8172 Train Loss: 0.52289 Validation Loss: 0.54828\n",
            "Epoch: 8173 Train Loss: 0.52281 Validation Loss: 0.54862\n",
            "Epoch: 8174 Train Loss: 0.52554 Validation Loss: 0.54946\n",
            "Epoch: 8175 Train Loss: 0.52376 Validation Loss: 0.54813\n",
            "Epoch: 8176 Train Loss: 0.52495 Validation Loss: 0.54830\n",
            "Epoch: 8177 Train Loss: 0.52112 Validation Loss: 0.54836\n",
            "Epoch: 8178 Train Loss: 0.52480 Validation Loss: 0.54872\n",
            "Epoch: 8179 Train Loss: 0.52488 Validation Loss: 0.54893\n",
            "Epoch: 8180 Train Loss: 0.52284 Validation Loss: 0.54813\n",
            "Epoch: 8181 Train Loss: 0.52567 Validation Loss: 0.54880\n",
            "Epoch: 8182 Train Loss: 0.51982 Validation Loss: 0.54822\n",
            "Epoch: 8183 Train Loss: 0.52399 Validation Loss: 0.54878\n",
            "Epoch: 8184 Train Loss: 0.52104 Validation Loss: 0.54824\n",
            "Epoch: 8185 Train Loss: 0.52096 Validation Loss: 0.54878\n",
            "Epoch: 8186 Train Loss: 0.52519 Validation Loss: 0.54910\n",
            "Epoch: 8187 Train Loss: 0.52023 Validation Loss: 0.54809\n",
            "Epoch: 8188 Train Loss: 0.52286 Validation Loss: 0.54864\n",
            "Epoch: 8189 Train Loss: 0.52496 Validation Loss: 0.54892\n",
            "Epoch: 8190 Train Loss: 0.52415 Validation Loss: 0.54884\n",
            "Epoch: 8191 Train Loss: 0.52582 Validation Loss: 0.54847\n",
            "Epoch: 8192 Train Loss: 0.52389 Validation Loss: 0.54815\n",
            "Epoch: 8193 Train Loss: 0.52181 Validation Loss: 0.54851\n",
            "Epoch: 8194 Train Loss: 0.52185 Validation Loss: 0.54857\n",
            "Epoch: 8195 Train Loss: 0.52563 Validation Loss: 0.54916\n",
            "Epoch: 8196 Train Loss: 0.52166 Validation Loss: 0.54850\n",
            "Epoch: 8197 Train Loss: 0.52081 Validation Loss: 0.54840\n",
            "Epoch: 8198 Train Loss: 0.52249 Validation Loss: 0.54813\n",
            "Epoch: 8199 Train Loss: 0.52226 Validation Loss: 0.54918\n",
            "Epoch: 8200 Train Loss: 0.52376 Validation Loss: 0.54871\n",
            "Epoch: 8201 Train Loss: 0.52313 Validation Loss: 0.54810\n",
            "Epoch: 8202 Train Loss: 0.52400 Validation Loss: 0.54872\n",
            "Epoch: 8203 Train Loss: 0.52399 Validation Loss: 0.54880\n",
            "Epoch: 8204 Train Loss: 0.52519 Validation Loss: 0.54822\n",
            "Epoch: 8205 Train Loss: 0.52478 Validation Loss: 0.54865\n",
            "Epoch: 8206 Train Loss: 0.52117 Validation Loss: 0.54836\n",
            "Epoch: 8207 Train Loss: 0.52287 Validation Loss: 0.54926\n",
            "Epoch: 8208 Train Loss: 0.52493 Validation Loss: 0.54892\n",
            "Epoch: 8209 Train Loss: 0.52122 Validation Loss: 0.54822\n",
            "Epoch: 8210 Train Loss: 0.52683 Validation Loss: 0.54889\n",
            "Epoch: 8211 Train Loss: 0.53055 Validation Loss: 0.54966\n",
            "Epoch: 8212 Train Loss: 0.52223 Validation Loss: 0.54776\n",
            "Epoch: 8213 Train Loss: 0.52463 Validation Loss: 0.54875\n",
            "Epoch: 8214 Train Loss: 0.52219 Validation Loss: 0.54835\n",
            "Epoch: 8215 Train Loss: 0.52407 Validation Loss: 0.54940\n",
            "Epoch: 8216 Train Loss: 0.52183 Validation Loss: 0.54876\n",
            "Epoch: 8217 Train Loss: 0.52197 Validation Loss: 0.54819\n",
            "Epoch: 8218 Train Loss: 0.52190 Validation Loss: 0.54853\n",
            "Epoch: 8219 Train Loss: 0.52306 Validation Loss: 0.54935\n",
            "Epoch: 8220 Train Loss: 0.52509 Validation Loss: 0.54878\n",
            "Epoch: 8221 Train Loss: 0.52700 Validation Loss: 0.54908\n",
            "Epoch: 8222 Train Loss: 0.52050 Validation Loss: 0.54788\n",
            "Epoch: 8223 Train Loss: 0.52374 Validation Loss: 0.54899\n",
            "Epoch: 8224 Train Loss: 0.52404 Validation Loss: 0.54947\n",
            "Epoch: 8225 Train Loss: 0.52421 Validation Loss: 0.54884\n",
            "Epoch: 8226 Train Loss: 0.52325 Validation Loss: 0.54781\n",
            "Epoch: 8227 Train Loss: 0.52400 Validation Loss: 0.54882\n",
            "Epoch: 8228 Train Loss: 0.52296 Validation Loss: 0.54829\n",
            "Epoch: 8229 Train Loss: 0.52277 Validation Loss: 0.54861\n",
            "Epoch: 8230 Train Loss: 0.52610 Validation Loss: 0.54851\n",
            "Epoch: 8231 Train Loss: 0.52180 Validation Loss: 0.54892\n",
            "Epoch: 8232 Train Loss: 0.52284 Validation Loss: 0.54872\n",
            "Epoch: 8233 Train Loss: 0.52390 Validation Loss: 0.54844\n",
            "Epoch: 8234 Train Loss: 0.52507 Validation Loss: 0.54884\n",
            "Epoch: 8235 Train Loss: 0.52311 Validation Loss: 0.54807\n",
            "Epoch: 8236 Train Loss: 0.52208 Validation Loss: 0.54920\n",
            "Epoch: 8237 Train Loss: 0.51997 Validation Loss: 0.54902\n",
            "Epoch: 8238 Train Loss: 0.52543 Validation Loss: 0.54833\n",
            "Epoch: 8239 Train Loss: 0.52391 Validation Loss: 0.54854\n",
            "Epoch: 8240 Train Loss: 0.52840 Validation Loss: 0.54971\n",
            "Epoch: 8241 Train Loss: 0.52466 Validation Loss: 0.54805\n",
            "Epoch: 8242 Train Loss: 0.52402 Validation Loss: 0.54841\n",
            "Epoch: 8243 Train Loss: 0.51980 Validation Loss: 0.54834\n",
            "Epoch: 8244 Train Loss: 0.52486 Validation Loss: 0.54850\n",
            "Epoch: 8245 Train Loss: 0.52282 Validation Loss: 0.54870\n",
            "Epoch: 8246 Train Loss: 0.52382 Validation Loss: 0.54872\n",
            "Epoch: 8247 Train Loss: 0.51991 Validation Loss: 0.54823\n",
            "Epoch: 8248 Train Loss: 0.52204 Validation Loss: 0.54848\n",
            "Epoch: 8249 Train Loss: 0.52093 Validation Loss: 0.54882\n",
            "Epoch: 8250 Train Loss: 0.52524 Validation Loss: 0.55002\n",
            "Epoch: 8251 Train Loss: 0.52354 Validation Loss: 0.54818\n",
            "Epoch: 8252 Train Loss: 0.52279 Validation Loss: 0.54872\n",
            "Epoch: 8253 Train Loss: 0.52194 Validation Loss: 0.54865\n",
            "Epoch: 8254 Train Loss: 0.52285 Validation Loss: 0.54913\n",
            "Epoch: 8255 Train Loss: 0.52513 Validation Loss: 0.54909\n",
            "Epoch: 8256 Train Loss: 0.52674 Validation Loss: 0.54848\n",
            "Epoch: 8257 Train Loss: 0.52241 Validation Loss: 0.54787\n",
            "Epoch: 8258 Train Loss: 0.52144 Validation Loss: 0.54829\n",
            "Epoch: 8259 Train Loss: 0.52342 Validation Loss: 0.54825\n",
            "Epoch: 8260 Train Loss: 0.52479 Validation Loss: 0.54924\n",
            "Epoch: 8261 Train Loss: 0.52299 Validation Loss: 0.54890\n",
            "Epoch: 8262 Train Loss: 0.52276 Validation Loss: 0.54834\n",
            "Epoch: 8263 Train Loss: 0.52043 Validation Loss: 0.54826\n",
            "Epoch: 8264 Train Loss: 0.52292 Validation Loss: 0.54930\n",
            "Epoch: 8265 Train Loss: 0.52361 Validation Loss: 0.54834\n",
            "Epoch: 8266 Train Loss: 0.52740 Validation Loss: 0.54995\n",
            "Epoch: 8267 Train Loss: 0.51998 Validation Loss: 0.54826\n",
            "Epoch: 8268 Train Loss: 0.52182 Validation Loss: 0.54832\n",
            "Epoch: 8269 Train Loss: 0.52177 Validation Loss: 0.54858\n",
            "Epoch: 8270 Train Loss: 0.52178 Validation Loss: 0.54886\n",
            "Epoch: 8271 Train Loss: 0.52424 Validation Loss: 0.54933\n",
            "Epoch: 8272 Train Loss: 0.52273 Validation Loss: 0.54827\n",
            "Epoch: 8273 Train Loss: 0.52481 Validation Loss: 0.54842\n",
            "Epoch: 8274 Train Loss: 0.52327 Validation Loss: 0.54875\n",
            "Epoch: 8275 Train Loss: 0.52278 Validation Loss: 0.54818\n",
            "Epoch: 8276 Train Loss: 0.52194 Validation Loss: 0.54803\n",
            "Epoch: 8277 Train Loss: 0.52403 Validation Loss: 0.54925\n",
            "Epoch: 8278 Train Loss: 0.52475 Validation Loss: 0.54872\n",
            "Epoch: 8279 Train Loss: 0.52178 Validation Loss: 0.54832\n",
            "Epoch: 8280 Train Loss: 0.52501 Validation Loss: 0.54893\n",
            "Epoch: 8281 Train Loss: 0.52654 Validation Loss: 0.54910\n",
            "Epoch: 8282 Train Loss: 0.52183 Validation Loss: 0.54857\n",
            "Epoch: 8283 Train Loss: 0.52430 Validation Loss: 0.54843\n",
            "Epoch: 8284 Train Loss: 0.52206 Validation Loss: 0.54830\n",
            "Epoch: 8285 Train Loss: 0.52284 Validation Loss: 0.54932\n",
            "Epoch: 8286 Train Loss: 0.52295 Validation Loss: 0.54870\n",
            "Epoch: 8287 Train Loss: 0.52572 Validation Loss: 0.54895\n",
            "Epoch: 8288 Train Loss: 0.52185 Validation Loss: 0.54869\n",
            "Epoch: 8289 Train Loss: 0.52292 Validation Loss: 0.54856\n",
            "Epoch: 8290 Train Loss: 0.52697 Validation Loss: 0.54848\n",
            "Epoch: 8291 Train Loss: 0.52387 Validation Loss: 0.54847\n",
            "Epoch: 8292 Train Loss: 0.52595 Validation Loss: 0.54916\n",
            "Epoch: 8293 Train Loss: 0.52388 Validation Loss: 0.54831\n",
            "Epoch: 8294 Train Loss: 0.52193 Validation Loss: 0.54825\n",
            "Epoch: 8295 Train Loss: 0.52305 Validation Loss: 0.54889\n",
            "Epoch: 8296 Train Loss: 0.52595 Validation Loss: 0.54852\n",
            "Epoch: 8297 Train Loss: 0.52296 Validation Loss: 0.54822\n",
            "Epoch: 8298 Train Loss: 0.52003 Validation Loss: 0.54836\n",
            "Epoch: 8299 Train Loss: 0.52315 Validation Loss: 0.54969\n",
            "Epoch: 8300 Train Loss: 0.52291 Validation Loss: 0.54881\n",
            "Epoch: 8301 Train Loss: 0.52134 Validation Loss: 0.54851\n",
            "Epoch: 8302 Train Loss: 0.52341 Validation Loss: 0.54862\n",
            "Epoch: 8303 Train Loss: 0.52656 Validation Loss: 0.54884\n",
            "Epoch: 8304 Train Loss: 0.52174 Validation Loss: 0.54795\n",
            "Epoch: 8305 Train Loss: 0.52185 Validation Loss: 0.54849\n",
            "Epoch: 8306 Train Loss: 0.52297 Validation Loss: 0.54940\n",
            "Epoch: 8307 Train Loss: 0.52398 Validation Loss: 0.54909\n",
            "Epoch: 8308 Train Loss: 0.52280 Validation Loss: 0.54824\n",
            "Epoch: 8309 Train Loss: 0.52113 Validation Loss: 0.54839\n",
            "Epoch: 8310 Train Loss: 0.52287 Validation Loss: 0.54903\n",
            "Epoch: 8311 Train Loss: 0.52179 Validation Loss: 0.54869\n",
            "Epoch: 8312 Train Loss: 0.52183 Validation Loss: 0.54915\n",
            "Epoch: 8313 Train Loss: 0.52479 Validation Loss: 0.54893\n",
            "Epoch: 8314 Train Loss: 0.52632 Validation Loss: 0.54822\n",
            "Epoch: 8315 Train Loss: 0.52320 Validation Loss: 0.54835\n",
            "Epoch: 8316 Train Loss: 0.52302 Validation Loss: 0.54886\n",
            "Epoch: 8317 Train Loss: 0.52395 Validation Loss: 0.54891\n",
            "Epoch: 8318 Train Loss: 0.52279 Validation Loss: 0.54862\n",
            "Epoch: 8319 Train Loss: 0.52484 Validation Loss: 0.54878\n",
            "Epoch: 8320 Train Loss: 0.52435 Validation Loss: 0.54892\n",
            "Epoch: 8321 Train Loss: 0.52080 Validation Loss: 0.54825\n",
            "Epoch: 8322 Train Loss: 0.52280 Validation Loss: 0.54880\n",
            "Epoch: 8323 Train Loss: 0.52276 Validation Loss: 0.54878\n",
            "Epoch: 8324 Train Loss: 0.52277 Validation Loss: 0.54872\n",
            "Epoch: 8325 Train Loss: 0.52291 Validation Loss: 0.54828\n",
            "Epoch: 8326 Train Loss: 0.52274 Validation Loss: 0.54855\n",
            "Epoch: 8327 Train Loss: 0.52387 Validation Loss: 0.54829\n",
            "Epoch: 8328 Train Loss: 0.52475 Validation Loss: 0.54887\n",
            "Epoch: 8329 Train Loss: 0.52338 Validation Loss: 0.54969\n",
            "Epoch: 8330 Train Loss: 0.52275 Validation Loss: 0.54873\n",
            "Epoch: 8331 Train Loss: 0.52299 Validation Loss: 0.54804\n",
            "Epoch: 8332 Train Loss: 0.52286 Validation Loss: 0.54891\n",
            "Epoch: 8333 Train Loss: 0.52145 Validation Loss: 0.54786\n",
            "Epoch: 8334 Train Loss: 0.52258 Validation Loss: 0.54983\n",
            "Epoch: 8335 Train Loss: 0.52322 Validation Loss: 0.55007\n",
            "Epoch: 8336 Train Loss: 0.52276 Validation Loss: 0.54838\n",
            "Epoch: 8337 Train Loss: 0.52244 Validation Loss: 0.54803\n",
            "Epoch: 8338 Train Loss: 0.52407 Validation Loss: 0.54913\n",
            "Epoch: 8339 Train Loss: 0.52539 Validation Loss: 0.54880\n",
            "Epoch: 8340 Train Loss: 0.52492 Validation Loss: 0.54860\n",
            "Epoch: 8341 Train Loss: 0.52540 Validation Loss: 0.54811\n",
            "Epoch: 8342 Train Loss: 0.52290 Validation Loss: 0.54883\n",
            "Epoch: 8343 Train Loss: 0.52453 Validation Loss: 0.54936\n",
            "Epoch: 8344 Train Loss: 0.52352 Validation Loss: 0.54825\n",
            "Epoch: 8345 Train Loss: 0.52457 Validation Loss: 0.54782\n",
            "Epoch: 8346 Train Loss: 0.52593 Validation Loss: 0.54858\n",
            "Epoch: 8347 Train Loss: 0.52378 Validation Loss: 0.54828\n",
            "Epoch: 8348 Train Loss: 0.52391 Validation Loss: 0.54839\n",
            "Epoch: 8349 Train Loss: 0.52276 Validation Loss: 0.54880\n",
            "Epoch: 8350 Train Loss: 0.52197 Validation Loss: 0.54872\n",
            "Epoch: 8351 Train Loss: 0.52513 Validation Loss: 0.54914\n",
            "Epoch: 8352 Train Loss: 0.52307 Validation Loss: 0.54802\n",
            "Epoch: 8353 Train Loss: 0.52387 Validation Loss: 0.54910\n",
            "Epoch: 8354 Train Loss: 0.52500 Validation Loss: 0.54906\n",
            "Epoch: 8355 Train Loss: 0.52069 Validation Loss: 0.54850\n",
            "Epoch: 8356 Train Loss: 0.51992 Validation Loss: 0.54850\n",
            "Epoch: 8357 Train Loss: 0.52293 Validation Loss: 0.54840\n",
            "Epoch: 8358 Train Loss: 0.52095 Validation Loss: 0.54914\n",
            "Epoch: 8359 Train Loss: 0.52387 Validation Loss: 0.54927\n",
            "Epoch: 8360 Train Loss: 0.52411 Validation Loss: 0.54849\n",
            "Epoch: 8361 Train Loss: 0.52088 Validation Loss: 0.54855\n",
            "Epoch: 8362 Train Loss: 0.52226 Validation Loss: 0.54889\n",
            "Epoch: 8363 Train Loss: 0.52479 Validation Loss: 0.54879\n",
            "Epoch: 8364 Train Loss: 0.52304 Validation Loss: 0.54872\n",
            "Epoch: 8365 Train Loss: 0.52401 Validation Loss: 0.54797\n",
            "Epoch: 8366 Train Loss: 0.52356 Validation Loss: 0.54901\n",
            "Epoch: 8367 Train Loss: 0.52123 Validation Loss: 0.54823\n",
            "Epoch: 8368 Train Loss: 0.52483 Validation Loss: 0.54893\n",
            "Epoch: 8369 Train Loss: 0.52201 Validation Loss: 0.54840\n",
            "Epoch: 8370 Train Loss: 0.52094 Validation Loss: 0.54904\n",
            "Epoch: 8371 Train Loss: 0.52362 Validation Loss: 0.54895\n",
            "Epoch: 8372 Train Loss: 0.52092 Validation Loss: 0.54837\n",
            "Epoch: 8373 Train Loss: 0.52292 Validation Loss: 0.54895\n",
            "Epoch: 8374 Train Loss: 0.52281 Validation Loss: 0.54879\n",
            "Epoch: 8375 Train Loss: 0.52085 Validation Loss: 0.54839\n",
            "Epoch: 8376 Train Loss: 0.52623 Validation Loss: 0.54870\n",
            "Epoch: 8377 Train Loss: 0.52298 Validation Loss: 0.54839\n",
            "Epoch: 8378 Train Loss: 0.52491 Validation Loss: 0.54848\n",
            "Epoch: 8379 Train Loss: 0.52190 Validation Loss: 0.54809\n",
            "Epoch: 8380 Train Loss: 0.52600 Validation Loss: 0.54871\n",
            "Epoch: 8381 Train Loss: 0.52383 Validation Loss: 0.54832\n",
            "Epoch: 8382 Train Loss: 0.52710 Validation Loss: 0.54893\n",
            "Epoch: 8383 Train Loss: 0.52307 Validation Loss: 0.54797\n",
            "Epoch: 8384 Train Loss: 0.52596 Validation Loss: 0.54911\n",
            "Epoch: 8385 Train Loss: 0.52384 Validation Loss: 0.54891\n",
            "Epoch: 8386 Train Loss: 0.52391 Validation Loss: 0.54846\n",
            "Epoch: 8387 Train Loss: 0.52410 Validation Loss: 0.54903\n",
            "Epoch: 8388 Train Loss: 0.52371 Validation Loss: 0.54830\n",
            "Epoch: 8389 Train Loss: 0.52440 Validation Loss: 0.54790\n",
            "Epoch: 8390 Train Loss: 0.52453 Validation Loss: 0.54963\n",
            "Epoch: 8391 Train Loss: 0.52503 Validation Loss: 0.54942\n",
            "Epoch: 8392 Train Loss: 0.52280 Validation Loss: 0.54837\n",
            "Epoch: 8393 Train Loss: 0.52008 Validation Loss: 0.54809\n",
            "Epoch: 8394 Train Loss: 0.52241 Validation Loss: 0.54875\n",
            "Epoch: 8395 Train Loss: 0.52176 Validation Loss: 0.54914\n",
            "Epoch: 8396 Train Loss: 0.52082 Validation Loss: 0.54887\n",
            "Epoch: 8397 Train Loss: 0.52184 Validation Loss: 0.54885\n",
            "Epoch: 8398 Train Loss: 0.52281 Validation Loss: 0.54841\n",
            "Epoch: 8399 Train Loss: 0.52393 Validation Loss: 0.54838\n",
            "Epoch: 8400 Train Loss: 0.52491 Validation Loss: 0.54875\n",
            "Epoch: 8401 Train Loss: 0.52344 Validation Loss: 0.54934\n",
            "Epoch: 8402 Train Loss: 0.52687 Validation Loss: 0.54853\n",
            "Epoch: 8403 Train Loss: 0.52332 Validation Loss: 0.54797\n",
            "Epoch: 8404 Train Loss: 0.52199 Validation Loss: 0.54896\n",
            "Epoch: 8405 Train Loss: 0.52207 Validation Loss: 0.54881\n",
            "Epoch: 8406 Train Loss: 0.52294 Validation Loss: 0.54835\n",
            "Epoch: 8407 Train Loss: 0.52586 Validation Loss: 0.54892\n",
            "Epoch: 8408 Train Loss: 0.52418 Validation Loss: 0.54797\n",
            "Epoch: 8409 Train Loss: 0.52390 Validation Loss: 0.54838\n",
            "Epoch: 8410 Train Loss: 0.52242 Validation Loss: 0.55000\n",
            "Epoch: 8411 Train Loss: 0.52456 Validation Loss: 0.54817\n",
            "Epoch: 8412 Train Loss: 0.52093 Validation Loss: 0.54845\n",
            "Epoch: 8413 Train Loss: 0.52487 Validation Loss: 0.54952\n",
            "Epoch: 8414 Train Loss: 0.52187 Validation Loss: 0.54886\n",
            "Epoch: 8415 Train Loss: 0.52097 Validation Loss: 0.54863\n",
            "Epoch: 8416 Train Loss: 0.52289 Validation Loss: 0.54926\n",
            "Epoch: 8417 Train Loss: 0.52389 Validation Loss: 0.54895\n",
            "Epoch: 8418 Train Loss: 0.52477 Validation Loss: 0.54859\n",
            "Epoch: 8419 Train Loss: 0.52195 Validation Loss: 0.54818\n",
            "Epoch: 8420 Train Loss: 0.52431 Validation Loss: 0.54857\n",
            "Epoch: 8421 Train Loss: 0.52196 Validation Loss: 0.54887\n",
            "Epoch: 8422 Train Loss: 0.52593 Validation Loss: 0.54913\n",
            "Epoch: 8423 Train Loss: 0.51990 Validation Loss: 0.54843\n",
            "Epoch: 8424 Train Loss: 0.52214 Validation Loss: 0.54950\n",
            "Epoch: 8425 Train Loss: 0.52325 Validation Loss: 0.54880\n",
            "Epoch: 8426 Train Loss: 0.52797 Validation Loss: 0.54858\n",
            "Epoch: 8427 Train Loss: 0.52288 Validation Loss: 0.54815\n",
            "Epoch: 8428 Train Loss: 0.52432 Validation Loss: 0.54802\n",
            "Epoch: 8429 Train Loss: 0.52244 Validation Loss: 0.54841\n",
            "Epoch: 8430 Train Loss: 0.52046 Validation Loss: 0.54952\n",
            "Epoch: 8431 Train Loss: 0.52082 Validation Loss: 0.54829\n",
            "Epoch: 8432 Train Loss: 0.52412 Validation Loss: 0.54903\n",
            "Epoch: 8433 Train Loss: 0.52583 Validation Loss: 0.54881\n",
            "Epoch: 8434 Train Loss: 0.52309 Validation Loss: 0.54822\n",
            "Epoch: 8435 Train Loss: 0.52279 Validation Loss: 0.54842\n",
            "Epoch: 8436 Train Loss: 0.52310 Validation Loss: 0.54894\n",
            "Epoch: 8437 Train Loss: 0.52088 Validation Loss: 0.54837\n",
            "Epoch: 8438 Train Loss: 0.52371 Validation Loss: 0.54880\n",
            "Epoch: 8439 Train Loss: 0.52606 Validation Loss: 0.54870\n",
            "Epoch: 8440 Train Loss: 0.52005 Validation Loss: 0.54855\n",
            "Epoch: 8441 Train Loss: 0.52390 Validation Loss: 0.54917\n",
            "Epoch: 8442 Train Loss: 0.52493 Validation Loss: 0.54891\n",
            "Epoch: 8443 Train Loss: 0.52416 Validation Loss: 0.54875\n",
            "Epoch: 8444 Train Loss: 0.52282 Validation Loss: 0.54825\n",
            "Epoch: 8445 Train Loss: 0.52216 Validation Loss: 0.54806\n",
            "Epoch: 8446 Train Loss: 0.52655 Validation Loss: 0.54947\n",
            "Epoch: 8447 Train Loss: 0.52639 Validation Loss: 0.54826\n",
            "Epoch: 8448 Train Loss: 0.52550 Validation Loss: 0.54912\n",
            "Epoch: 8449 Train Loss: 0.52011 Validation Loss: 0.54796\n",
            "Epoch: 8450 Train Loss: 0.52314 Validation Loss: 0.54811\n",
            "Epoch: 8451 Train Loss: 0.52597 Validation Loss: 0.54913\n",
            "Epoch: 8452 Train Loss: 0.51994 Validation Loss: 0.54867\n",
            "Epoch: 8453 Train Loss: 0.52179 Validation Loss: 0.54884\n",
            "Epoch: 8454 Train Loss: 0.52340 Validation Loss: 0.54969\n",
            "Epoch: 8455 Train Loss: 0.52464 Validation Loss: 0.54834\n",
            "Epoch: 8456 Train Loss: 0.52510 Validation Loss: 0.54840\n",
            "Epoch: 8457 Train Loss: 0.52819 Validation Loss: 0.54892\n",
            "Epoch: 8458 Train Loss: 0.52327 Validation Loss: 0.54798\n",
            "Epoch: 8459 Train Loss: 0.52205 Validation Loss: 0.54887\n",
            "Epoch: 8460 Train Loss: 0.52097 Validation Loss: 0.54842\n",
            "Epoch: 8461 Train Loss: 0.52278 Validation Loss: 0.54874\n",
            "Epoch: 8462 Train Loss: 0.52596 Validation Loss: 0.54900\n",
            "Epoch: 8463 Train Loss: 0.52193 Validation Loss: 0.54855\n",
            "Epoch: 8464 Train Loss: 0.52389 Validation Loss: 0.54846\n",
            "Epoch: 8465 Train Loss: 0.52283 Validation Loss: 0.54865\n",
            "Epoch: 8466 Train Loss: 0.52297 Validation Loss: 0.54819\n",
            "Epoch: 8467 Train Loss: 0.52150 Validation Loss: 0.54929\n",
            "Epoch: 8468 Train Loss: 0.52490 Validation Loss: 0.54861\n",
            "Epoch: 8469 Train Loss: 0.52492 Validation Loss: 0.54897\n",
            "Epoch: 8470 Train Loss: 0.52184 Validation Loss: 0.54854\n",
            "Epoch: 8471 Train Loss: 0.52270 Validation Loss: 0.54847\n",
            "Epoch: 8472 Train Loss: 0.52095 Validation Loss: 0.54844\n",
            "Epoch: 8473 Train Loss: 0.52088 Validation Loss: 0.54850\n",
            "Epoch: 8474 Train Loss: 0.52677 Validation Loss: 0.54932\n",
            "Epoch: 8475 Train Loss: 0.52363 Validation Loss: 0.54826\n",
            "Epoch: 8476 Train Loss: 0.52080 Validation Loss: 0.54882\n",
            "Epoch: 8477 Train Loss: 0.52677 Validation Loss: 0.55010\n",
            "Epoch: 8478 Train Loss: 0.52168 Validation Loss: 0.54826\n",
            "Epoch: 8479 Train Loss: 0.52303 Validation Loss: 0.54808\n",
            "Epoch: 8480 Train Loss: 0.52287 Validation Loss: 0.54857\n",
            "Epoch: 8481 Train Loss: 0.52280 Validation Loss: 0.54864\n",
            "Epoch: 8482 Train Loss: 0.52125 Validation Loss: 0.54931\n",
            "Epoch: 8483 Train Loss: 0.52579 Validation Loss: 0.54855\n",
            "Epoch: 8484 Train Loss: 0.52284 Validation Loss: 0.54838\n",
            "Epoch: 8485 Train Loss: 0.52286 Validation Loss: 0.54886\n",
            "Epoch: 8486 Train Loss: 0.52210 Validation Loss: 0.54805\n",
            "Epoch: 8487 Train Loss: 0.52381 Validation Loss: 0.54886\n",
            "Epoch: 8488 Train Loss: 0.52383 Validation Loss: 0.54896\n",
            "Epoch: 8489 Train Loss: 0.52489 Validation Loss: 0.54865\n",
            "Epoch: 8490 Train Loss: 0.52384 Validation Loss: 0.54819\n",
            "Epoch: 8491 Train Loss: 0.52411 Validation Loss: 0.54815\n",
            "Epoch: 8492 Train Loss: 0.52294 Validation Loss: 0.54890\n",
            "Epoch: 8493 Train Loss: 0.52286 Validation Loss: 0.54819\n",
            "Epoch: 8494 Train Loss: 0.52284 Validation Loss: 0.54874\n",
            "Epoch: 8495 Train Loss: 0.52483 Validation Loss: 0.54900\n",
            "Epoch: 8496 Train Loss: 0.52174 Validation Loss: 0.54854\n",
            "Epoch: 8497 Train Loss: 0.52203 Validation Loss: 0.54836\n",
            "Epoch: 8498 Train Loss: 0.52341 Validation Loss: 0.54867\n",
            "Epoch: 8499 Train Loss: 0.52210 Validation Loss: 0.54871\n",
            "Epoch: 8500 Train Loss: 0.52434 Validation Loss: 0.54876\n",
            "Epoch: 8501 Train Loss: 0.52322 Validation Loss: 0.54813\n",
            "Epoch: 8502 Train Loss: 0.52184 Validation Loss: 0.54919\n",
            "Epoch: 8503 Train Loss: 0.52389 Validation Loss: 0.54916\n",
            "Epoch: 8504 Train Loss: 0.52609 Validation Loss: 0.54918\n",
            "Epoch: 8505 Train Loss: 0.52652 Validation Loss: 0.54908\n",
            "Epoch: 8506 Train Loss: 0.52199 Validation Loss: 0.54849\n",
            "Epoch: 8507 Train Loss: 0.52189 Validation Loss: 0.54880\n",
            "Epoch: 8508 Train Loss: 0.52287 Validation Loss: 0.54831\n",
            "Epoch: 8509 Train Loss: 0.52480 Validation Loss: 0.54889\n",
            "Epoch: 8510 Train Loss: 0.52396 Validation Loss: 0.54885\n",
            "Epoch: 8511 Train Loss: 0.52481 Validation Loss: 0.54847\n",
            "Epoch: 8512 Train Loss: 0.52576 Validation Loss: 0.54855\n",
            "Epoch: 8513 Train Loss: 0.52278 Validation Loss: 0.54828\n",
            "Epoch: 8514 Train Loss: 0.52513 Validation Loss: 0.54869\n",
            "Epoch: 8515 Train Loss: 0.52255 Validation Loss: 0.54817\n",
            "Epoch: 8516 Train Loss: 0.52289 Validation Loss: 0.54928\n",
            "Epoch: 8517 Train Loss: 0.52311 Validation Loss: 0.54885\n",
            "Epoch: 8518 Train Loss: 0.52016 Validation Loss: 0.54865\n",
            "Epoch: 8519 Train Loss: 0.52086 Validation Loss: 0.54895\n",
            "Epoch: 8520 Train Loss: 0.52618 Validation Loss: 0.54936\n",
            "Epoch: 8521 Train Loss: 0.52380 Validation Loss: 0.54859\n",
            "Epoch: 8522 Train Loss: 0.52445 Validation Loss: 0.54789\n",
            "Epoch: 8523 Train Loss: 0.52405 Validation Loss: 0.54932\n",
            "Epoch: 8524 Train Loss: 0.52279 Validation Loss: 0.54799\n",
            "Epoch: 8525 Train Loss: 0.52301 Validation Loss: 0.54981\n",
            "Epoch: 8526 Train Loss: 0.52238 Validation Loss: 0.54861\n",
            "Epoch: 8527 Train Loss: 0.52309 Validation Loss: 0.54873\n",
            "Epoch: 8528 Train Loss: 0.52186 Validation Loss: 0.54885\n",
            "Epoch: 8529 Train Loss: 0.52482 Validation Loss: 0.54891\n",
            "Epoch: 8530 Train Loss: 0.52276 Validation Loss: 0.54873\n",
            "Epoch: 8531 Train Loss: 0.51912 Validation Loss: 0.54823\n",
            "Epoch: 8532 Train Loss: 0.52174 Validation Loss: 0.54920\n",
            "Epoch: 8533 Train Loss: 0.52563 Validation Loss: 0.54903\n",
            "Epoch: 8534 Train Loss: 0.52097 Validation Loss: 0.54927\n",
            "Epoch: 8535 Train Loss: 0.52401 Validation Loss: 0.54848\n",
            "Epoch: 8536 Train Loss: 0.52380 Validation Loss: 0.54896\n",
            "Epoch: 8537 Train Loss: 0.52379 Validation Loss: 0.54871\n",
            "Epoch: 8538 Train Loss: 0.52091 Validation Loss: 0.54842\n",
            "Epoch: 8539 Train Loss: 0.52285 Validation Loss: 0.54878\n",
            "Epoch: 8540 Train Loss: 0.52588 Validation Loss: 0.54889\n",
            "Epoch: 8541 Train Loss: 0.52272 Validation Loss: 0.54848\n",
            "Epoch: 8542 Train Loss: 0.52294 Validation Loss: 0.54864\n",
            "Epoch: 8543 Train Loss: 0.52276 Validation Loss: 0.54855\n",
            "Epoch: 8544 Train Loss: 0.52104 Validation Loss: 0.54831\n",
            "Epoch: 8545 Train Loss: 0.52514 Validation Loss: 0.54889\n",
            "Epoch: 8546 Train Loss: 0.52411 Validation Loss: 0.54914\n",
            "Epoch: 8547 Train Loss: 0.52277 Validation Loss: 0.54856\n",
            "Epoch: 8548 Train Loss: 0.52093 Validation Loss: 0.54801\n",
            "Epoch: 8549 Train Loss: 0.52100 Validation Loss: 0.54883\n",
            "Epoch: 8550 Train Loss: 0.52486 Validation Loss: 0.54902\n",
            "Epoch: 8551 Train Loss: 0.52291 Validation Loss: 0.54852\n",
            "Epoch: 8552 Train Loss: 0.52388 Validation Loss: 0.54839\n",
            "Epoch: 8553 Train Loss: 0.52582 Validation Loss: 0.54842\n",
            "Epoch: 8554 Train Loss: 0.52178 Validation Loss: 0.54842\n",
            "Epoch: 8555 Train Loss: 0.52085 Validation Loss: 0.54850\n",
            "Epoch: 8556 Train Loss: 0.52275 Validation Loss: 0.54892\n",
            "Epoch: 8557 Train Loss: 0.52430 Validation Loss: 0.54873\n",
            "Epoch: 8558 Train Loss: 0.52285 Validation Loss: 0.54878\n",
            "Epoch: 8559 Train Loss: 0.52097 Validation Loss: 0.54850\n",
            "Epoch: 8560 Train Loss: 0.52076 Validation Loss: 0.54880\n",
            "Epoch: 8561 Train Loss: 0.52303 Validation Loss: 0.54880\n",
            "Epoch: 8562 Train Loss: 0.52531 Validation Loss: 0.54956\n",
            "Epoch: 8563 Train Loss: 0.52209 Validation Loss: 0.54857\n",
            "Epoch: 8564 Train Loss: 0.52259 Validation Loss: 0.54844\n",
            "Epoch: 8565 Train Loss: 0.52513 Validation Loss: 0.54857\n",
            "Epoch: 8566 Train Loss: 0.52484 Validation Loss: 0.54882\n",
            "Epoch: 8567 Train Loss: 0.52199 Validation Loss: 0.54811\n",
            "Epoch: 8568 Train Loss: 0.52272 Validation Loss: 0.54866\n",
            "Epoch: 8569 Train Loss: 0.52299 Validation Loss: 0.54887\n",
            "Epoch: 8570 Train Loss: 0.52551 Validation Loss: 0.54940\n",
            "Epoch: 8571 Train Loss: 0.52388 Validation Loss: 0.54829\n",
            "Epoch: 8572 Train Loss: 0.52387 Validation Loss: 0.54863\n",
            "Epoch: 8573 Train Loss: 0.52550 Validation Loss: 0.54902\n",
            "Epoch: 8574 Train Loss: 0.52276 Validation Loss: 0.54846\n",
            "Epoch: 8575 Train Loss: 0.52090 Validation Loss: 0.54816\n",
            "Epoch: 8576 Train Loss: 0.52266 Validation Loss: 0.54879\n",
            "Epoch: 8577 Train Loss: 0.52405 Validation Loss: 0.54886\n",
            "Epoch: 8578 Train Loss: 0.52284 Validation Loss: 0.54862\n",
            "Epoch: 8579 Train Loss: 0.52479 Validation Loss: 0.54864\n",
            "Epoch: 8580 Train Loss: 0.52623 Validation Loss: 0.54818\n",
            "Epoch: 8581 Train Loss: 0.52222 Validation Loss: 0.54841\n",
            "Epoch: 8582 Train Loss: 0.52277 Validation Loss: 0.54883\n",
            "Epoch: 8583 Train Loss: 0.52590 Validation Loss: 0.54913\n",
            "Epoch: 8584 Train Loss: 0.52475 Validation Loss: 0.54852\n",
            "Epoch: 8585 Train Loss: 0.52208 Validation Loss: 0.54818\n",
            "Epoch: 8586 Train Loss: 0.52793 Validation Loss: 0.54873\n",
            "Epoch: 8587 Train Loss: 0.52187 Validation Loss: 0.54825\n",
            "Epoch: 8588 Train Loss: 0.52687 Validation Loss: 0.54897\n",
            "Epoch: 8589 Train Loss: 0.52499 Validation Loss: 0.54854\n",
            "Epoch: 8590 Train Loss: 0.52294 Validation Loss: 0.54869\n",
            "Epoch: 8591 Train Loss: 0.52393 Validation Loss: 0.54862\n",
            "Epoch: 8592 Train Loss: 0.52405 Validation Loss: 0.54916\n",
            "Epoch: 8593 Train Loss: 0.52303 Validation Loss: 0.54849\n",
            "Epoch: 8594 Train Loss: 0.52189 Validation Loss: 0.54841\n",
            "Epoch: 8595 Train Loss: 0.52394 Validation Loss: 0.54876\n",
            "Epoch: 8596 Train Loss: 0.52488 Validation Loss: 0.54870\n",
            "Epoch: 8597 Train Loss: 0.52296 Validation Loss: 0.54809\n",
            "Epoch: 8598 Train Loss: 0.51980 Validation Loss: 0.54856\n",
            "Epoch: 8599 Train Loss: 0.52015 Validation Loss: 0.54843\n",
            "Epoch: 8600 Train Loss: 0.52394 Validation Loss: 0.55026\n",
            "Epoch: 8601 Train Loss: 0.52082 Validation Loss: 0.54841\n",
            "Epoch: 8602 Train Loss: 0.52393 Validation Loss: 0.54831\n",
            "Epoch: 8603 Train Loss: 0.52298 Validation Loss: 0.54929\n",
            "Epoch: 8604 Train Loss: 0.52207 Validation Loss: 0.54855\n",
            "Epoch: 8605 Train Loss: 0.52498 Validation Loss: 0.54855\n",
            "Epoch: 8606 Train Loss: 0.52241 Validation Loss: 0.54846\n",
            "Epoch: 8607 Train Loss: 0.52346 Validation Loss: 0.54931\n",
            "Epoch: 8608 Train Loss: 0.52426 Validation Loss: 0.54901\n",
            "Epoch: 8609 Train Loss: 0.52379 Validation Loss: 0.54869\n",
            "Epoch: 8610 Train Loss: 0.52097 Validation Loss: 0.54827\n",
            "Epoch: 8611 Train Loss: 0.52214 Validation Loss: 0.54944\n",
            "Epoch: 8612 Train Loss: 0.52393 Validation Loss: 0.54868\n",
            "Epoch: 8613 Train Loss: 0.52476 Validation Loss: 0.54843\n",
            "Epoch: 8614 Train Loss: 0.52430 Validation Loss: 0.54918\n",
            "Epoch: 8615 Train Loss: 0.52475 Validation Loss: 0.54820\n",
            "Epoch: 8616 Train Loss: 0.52279 Validation Loss: 0.54829\n",
            "Epoch: 8617 Train Loss: 0.52302 Validation Loss: 0.54856\n",
            "Epoch: 8618 Train Loss: 0.51989 Validation Loss: 0.54838\n",
            "Epoch: 8619 Train Loss: 0.52179 Validation Loss: 0.54891\n",
            "Epoch: 8620 Train Loss: 0.52180 Validation Loss: 0.54880\n",
            "Epoch: 8621 Train Loss: 0.52018 Validation Loss: 0.54836\n",
            "Epoch: 8622 Train Loss: 0.52289 Validation Loss: 0.54857\n",
            "Epoch: 8623 Train Loss: 0.52599 Validation Loss: 0.54961\n",
            "Epoch: 8624 Train Loss: 0.52281 Validation Loss: 0.54835\n",
            "Epoch: 8625 Train Loss: 0.52000 Validation Loss: 0.54793\n",
            "Epoch: 8626 Train Loss: 0.52402 Validation Loss: 0.54845\n",
            "Epoch: 8627 Train Loss: 0.52317 Validation Loss: 0.54852\n",
            "Epoch: 8628 Train Loss: 0.52560 Validation Loss: 0.54948\n",
            "Epoch: 8629 Train Loss: 0.52216 Validation Loss: 0.54793\n",
            "Epoch: 8630 Train Loss: 0.52387 Validation Loss: 0.54852\n",
            "Epoch: 8631 Train Loss: 0.52432 Validation Loss: 0.54880\n",
            "Epoch: 8632 Train Loss: 0.52195 Validation Loss: 0.54839\n",
            "Epoch: 8633 Train Loss: 0.52423 Validation Loss: 0.54927\n",
            "Epoch: 8634 Train Loss: 0.52392 Validation Loss: 0.54864\n",
            "Epoch: 8635 Train Loss: 0.52389 Validation Loss: 0.54817\n",
            "Epoch: 8636 Train Loss: 0.52388 Validation Loss: 0.54859\n",
            "Epoch: 8637 Train Loss: 0.52576 Validation Loss: 0.54858\n",
            "Epoch: 8638 Train Loss: 0.52373 Validation Loss: 0.54838\n",
            "Epoch: 8639 Train Loss: 0.52186 Validation Loss: 0.54845\n",
            "Epoch: 8640 Train Loss: 0.52387 Validation Loss: 0.54917\n",
            "Epoch: 8641 Train Loss: 0.52180 Validation Loss: 0.54854\n",
            "Epoch: 8642 Train Loss: 0.52276 Validation Loss: 0.54868\n",
            "Epoch: 8643 Train Loss: 0.52555 Validation Loss: 0.54823\n",
            "Epoch: 8644 Train Loss: 0.52087 Validation Loss: 0.54841\n",
            "Epoch: 8645 Train Loss: 0.52012 Validation Loss: 0.54896\n",
            "Epoch: 8646 Train Loss: 0.52613 Validation Loss: 0.54919\n",
            "Epoch: 8647 Train Loss: 0.52568 Validation Loss: 0.54810\n",
            "Epoch: 8648 Train Loss: 0.52923 Validation Loss: 0.54858\n",
            "Epoch: 8649 Train Loss: 0.52366 Validation Loss: 0.54806\n",
            "Epoch: 8650 Train Loss: 0.52401 Validation Loss: 0.54853\n",
            "Epoch: 8651 Train Loss: 0.52215 Validation Loss: 0.54840\n",
            "Epoch: 8652 Train Loss: 0.51979 Validation Loss: 0.54853\n",
            "Epoch: 8653 Train Loss: 0.51992 Validation Loss: 0.54856\n",
            "Epoch: 8654 Train Loss: 0.52213 Validation Loss: 0.54916\n",
            "Epoch: 8655 Train Loss: 0.52166 Validation Loss: 0.54822\n",
            "Epoch: 8656 Train Loss: 0.52481 Validation Loss: 0.54916\n",
            "Epoch: 8657 Train Loss: 0.52196 Validation Loss: 0.54911\n",
            "Epoch: 8658 Train Loss: 0.52514 Validation Loss: 0.54886\n",
            "Epoch: 8659 Train Loss: 0.52190 Validation Loss: 0.54844\n",
            "Epoch: 8660 Train Loss: 0.52221 Validation Loss: 0.54831\n",
            "Epoch: 8661 Train Loss: 0.52285 Validation Loss: 0.54952\n",
            "Epoch: 8662 Train Loss: 0.52384 Validation Loss: 0.54882\n",
            "Epoch: 8663 Train Loss: 0.52171 Validation Loss: 0.54794\n",
            "Epoch: 8664 Train Loss: 0.51986 Validation Loss: 0.54957\n",
            "Epoch: 8665 Train Loss: 0.52495 Validation Loss: 0.54905\n",
            "Epoch: 8666 Train Loss: 0.52432 Validation Loss: 0.54819\n",
            "Epoch: 8667 Train Loss: 0.52510 Validation Loss: 0.54878\n",
            "Epoch: 8668 Train Loss: 0.52475 Validation Loss: 0.54840\n",
            "Epoch: 8669 Train Loss: 0.52421 Validation Loss: 0.54802\n",
            "Epoch: 8670 Train Loss: 0.52679 Validation Loss: 0.54870\n",
            "Epoch: 8671 Train Loss: 0.52080 Validation Loss: 0.54890\n",
            "Epoch: 8672 Train Loss: 0.52381 Validation Loss: 0.54872\n",
            "Epoch: 8673 Train Loss: 0.52177 Validation Loss: 0.54864\n",
            "Epoch: 8674 Train Loss: 0.52688 Validation Loss: 0.54875\n",
            "Epoch: 8675 Train Loss: 0.52298 Validation Loss: 0.54824\n",
            "Epoch: 8676 Train Loss: 0.52103 Validation Loss: 0.54820\n",
            "Epoch: 8677 Train Loss: 0.52400 Validation Loss: 0.54878\n",
            "Epoch: 8678 Train Loss: 0.52188 Validation Loss: 0.54866\n",
            "Epoch: 8679 Train Loss: 0.52802 Validation Loss: 0.54908\n",
            "Epoch: 8680 Train Loss: 0.52184 Validation Loss: 0.54842\n",
            "Epoch: 8681 Train Loss: 0.52699 Validation Loss: 0.54849\n",
            "Epoch: 8682 Train Loss: 0.52389 Validation Loss: 0.54824\n",
            "Epoch: 8683 Train Loss: 0.52059 Validation Loss: 0.54803\n",
            "Epoch: 8684 Train Loss: 0.52146 Validation Loss: 0.54913\n",
            "Epoch: 8685 Train Loss: 0.52082 Validation Loss: 0.54929\n",
            "Epoch: 8686 Train Loss: 0.52109 Validation Loss: 0.54875\n",
            "Epoch: 8687 Train Loss: 0.52320 Validation Loss: 0.54912\n",
            "Epoch: 8688 Train Loss: 0.52204 Validation Loss: 0.54813\n",
            "Epoch: 8689 Train Loss: 0.51935 Validation Loss: 0.54809\n",
            "Epoch: 8690 Train Loss: 0.52147 Validation Loss: 0.54908\n",
            "Epoch: 8691 Train Loss: 0.52095 Validation Loss: 0.54926\n",
            "Epoch: 8692 Train Loss: 0.52591 Validation Loss: 0.54898\n",
            "Epoch: 8693 Train Loss: 0.52182 Validation Loss: 0.54837\n",
            "Epoch: 8694 Train Loss: 0.52533 Validation Loss: 0.54939\n",
            "Epoch: 8695 Train Loss: 0.52382 Validation Loss: 0.54883\n",
            "Epoch: 8696 Train Loss: 0.52280 Validation Loss: 0.54800\n",
            "Epoch: 8697 Train Loss: 0.52285 Validation Loss: 0.54857\n",
            "Epoch: 8698 Train Loss: 0.52498 Validation Loss: 0.54865\n",
            "Epoch: 8699 Train Loss: 0.52188 Validation Loss: 0.54859\n",
            "Epoch: 8700 Train Loss: 0.52397 Validation Loss: 0.54843\n",
            "Epoch: 8701 Train Loss: 0.52301 Validation Loss: 0.54887\n",
            "Epoch: 8702 Train Loss: 0.52106 Validation Loss: 0.54838\n",
            "Epoch: 8703 Train Loss: 0.52175 Validation Loss: 0.54884\n",
            "Epoch: 8704 Train Loss: 0.52087 Validation Loss: 0.54853\n",
            "Epoch: 8705 Train Loss: 0.52293 Validation Loss: 0.54906\n",
            "Epoch: 8706 Train Loss: 0.52729 Validation Loss: 0.54873\n",
            "Epoch: 8707 Train Loss: 0.52275 Validation Loss: 0.54808\n",
            "Epoch: 8708 Train Loss: 0.52589 Validation Loss: 0.54855\n",
            "Epoch: 8709 Train Loss: 0.52181 Validation Loss: 0.54870\n",
            "Epoch: 8710 Train Loss: 0.52408 Validation Loss: 0.54872\n",
            "Epoch: 8711 Train Loss: 0.52928 Validation Loss: 0.54967\n",
            "Epoch: 8712 Train Loss: 0.52339 Validation Loss: 0.54795\n",
            "Epoch: 8713 Train Loss: 0.52396 Validation Loss: 0.54823\n",
            "Epoch: 8714 Train Loss: 0.52482 Validation Loss: 0.54848\n",
            "Epoch: 8715 Train Loss: 0.52507 Validation Loss: 0.54870\n",
            "Epoch: 8716 Train Loss: 0.52414 Validation Loss: 0.54793\n",
            "Epoch: 8717 Train Loss: 0.52188 Validation Loss: 0.54839\n",
            "Epoch: 8718 Train Loss: 0.52400 Validation Loss: 0.54850\n",
            "Epoch: 8719 Train Loss: 0.52478 Validation Loss: 0.54895\n",
            "Epoch: 8720 Train Loss: 0.52313 Validation Loss: 0.54847\n",
            "Epoch: 8721 Train Loss: 0.52441 Validation Loss: 0.54864\n",
            "Epoch: 8722 Train Loss: 0.52202 Validation Loss: 0.54863\n",
            "Epoch: 8723 Train Loss: 0.52475 Validation Loss: 0.54870\n",
            "Epoch: 8724 Train Loss: 0.52286 Validation Loss: 0.54881\n",
            "Epoch: 8725 Train Loss: 0.52301 Validation Loss: 0.54883\n",
            "Epoch: 8726 Train Loss: 0.52488 Validation Loss: 0.54879\n",
            "Epoch: 8727 Train Loss: 0.52526 Validation Loss: 0.54849\n",
            "Epoch: 8728 Train Loss: 0.52405 Validation Loss: 0.54861\n",
            "Epoch: 8729 Train Loss: 0.52498 Validation Loss: 0.54883\n",
            "Epoch: 8730 Train Loss: 0.52378 Validation Loss: 0.54814\n",
            "Epoch: 8731 Train Loss: 0.52108 Validation Loss: 0.54838\n",
            "Epoch: 8732 Train Loss: 0.52084 Validation Loss: 0.54867\n",
            "Epoch: 8733 Train Loss: 0.52418 Validation Loss: 0.54941\n",
            "Epoch: 8734 Train Loss: 0.52397 Validation Loss: 0.54867\n",
            "Epoch: 8735 Train Loss: 0.52317 Validation Loss: 0.54828\n",
            "Epoch: 8736 Train Loss: 0.52493 Validation Loss: 0.54917\n",
            "Epoch: 8737 Train Loss: 0.52388 Validation Loss: 0.54875\n",
            "Epoch: 8738 Train Loss: 0.52208 Validation Loss: 0.54821\n",
            "Epoch: 8739 Train Loss: 0.51937 Validation Loss: 0.54822\n",
            "Epoch: 8740 Train Loss: 0.52416 Validation Loss: 0.54957\n",
            "Epoch: 8741 Train Loss: 0.52399 Validation Loss: 0.54862\n",
            "Epoch: 8742 Train Loss: 0.51976 Validation Loss: 0.54845\n",
            "Epoch: 8743 Train Loss: 0.52112 Validation Loss: 0.54901\n",
            "Epoch: 8744 Train Loss: 0.52186 Validation Loss: 0.54810\n",
            "Epoch: 8745 Train Loss: 0.52697 Validation Loss: 0.54990\n",
            "Epoch: 8746 Train Loss: 0.52403 Validation Loss: 0.54829\n",
            "Epoch: 8747 Train Loss: 0.52210 Validation Loss: 0.54837\n",
            "Epoch: 8748 Train Loss: 0.52397 Validation Loss: 0.54866\n",
            "Epoch: 8749 Train Loss: 0.52291 Validation Loss: 0.54852\n",
            "Epoch: 8750 Train Loss: 0.52312 Validation Loss: 0.54913\n",
            "Epoch: 8751 Train Loss: 0.52100 Validation Loss: 0.54829\n",
            "Epoch: 8752 Train Loss: 0.52175 Validation Loss: 0.54874\n",
            "Epoch: 8753 Train Loss: 0.52401 Validation Loss: 0.54915\n",
            "Epoch: 8754 Train Loss: 0.52500 Validation Loss: 0.54913\n",
            "Epoch: 8755 Train Loss: 0.52597 Validation Loss: 0.54841\n",
            "Epoch: 8756 Train Loss: 0.52211 Validation Loss: 0.54806\n",
            "Epoch: 8757 Train Loss: 0.52282 Validation Loss: 0.54903\n",
            "Epoch: 8758 Train Loss: 0.51961 Validation Loss: 0.54801\n",
            "Epoch: 8759 Train Loss: 0.52394 Validation Loss: 0.54952\n",
            "Epoch: 8760 Train Loss: 0.52606 Validation Loss: 0.54880\n",
            "Epoch: 8761 Train Loss: 0.52594 Validation Loss: 0.54869\n",
            "Epoch: 8762 Train Loss: 0.52125 Validation Loss: 0.54794\n",
            "Epoch: 8763 Train Loss: 0.52500 Validation Loss: 0.54893\n",
            "Epoch: 8764 Train Loss: 0.52180 Validation Loss: 0.54865\n",
            "Epoch: 8765 Train Loss: 0.52479 Validation Loss: 0.54873\n",
            "Epoch: 8766 Train Loss: 0.52299 Validation Loss: 0.54837\n",
            "Epoch: 8767 Train Loss: 0.52391 Validation Loss: 0.54882\n",
            "Epoch: 8768 Train Loss: 0.52701 Validation Loss: 0.54886\n",
            "Epoch: 8769 Train Loss: 0.52189 Validation Loss: 0.54846\n",
            "Epoch: 8770 Train Loss: 0.52481 Validation Loss: 0.54840\n",
            "Epoch: 8771 Train Loss: 0.52287 Validation Loss: 0.54874\n",
            "Epoch: 8772 Train Loss: 0.52378 Validation Loss: 0.54848\n",
            "Epoch: 8773 Train Loss: 0.52312 Validation Loss: 0.54898\n",
            "Epoch: 8774 Train Loss: 0.52302 Validation Loss: 0.54890\n",
            "Epoch: 8775 Train Loss: 0.52290 Validation Loss: 0.54829\n",
            "Epoch: 8776 Train Loss: 0.52574 Validation Loss: 0.54926\n",
            "Epoch: 8777 Train Loss: 0.52436 Validation Loss: 0.54820\n",
            "Epoch: 8778 Train Loss: 0.52497 Validation Loss: 0.54911\n",
            "Epoch: 8779 Train Loss: 0.52609 Validation Loss: 0.54860\n",
            "Epoch: 8780 Train Loss: 0.52318 Validation Loss: 0.54808\n",
            "Epoch: 8781 Train Loss: 0.52125 Validation Loss: 0.54883\n",
            "Epoch: 8782 Train Loss: 0.52491 Validation Loss: 0.54851\n",
            "Epoch: 8783 Train Loss: 0.52193 Validation Loss: 0.54870\n",
            "Epoch: 8784 Train Loss: 0.52478 Validation Loss: 0.54846\n",
            "Epoch: 8785 Train Loss: 0.52306 Validation Loss: 0.54816\n",
            "Epoch: 8786 Train Loss: 0.52236 Validation Loss: 0.54936\n",
            "Epoch: 8787 Train Loss: 0.52198 Validation Loss: 0.54817\n",
            "Epoch: 8788 Train Loss: 0.52227 Validation Loss: 0.54944\n",
            "Epoch: 8789 Train Loss: 0.52269 Validation Loss: 0.54787\n",
            "Epoch: 8790 Train Loss: 0.52329 Validation Loss: 0.54913\n",
            "Epoch: 8791 Train Loss: 0.52184 Validation Loss: 0.54878\n",
            "Epoch: 8792 Train Loss: 0.52190 Validation Loss: 0.54850\n",
            "Epoch: 8793 Train Loss: 0.52092 Validation Loss: 0.54874\n",
            "Epoch: 8794 Train Loss: 0.52300 Validation Loss: 0.54824\n",
            "Epoch: 8795 Train Loss: 0.52593 Validation Loss: 0.54921\n",
            "Epoch: 8796 Train Loss: 0.52384 Validation Loss: 0.54881\n",
            "Epoch: 8797 Train Loss: 0.52409 Validation Loss: 0.54809\n",
            "Epoch: 8798 Train Loss: 0.52082 Validation Loss: 0.54842\n",
            "Epoch: 8799 Train Loss: 0.52637 Validation Loss: 0.54912\n",
            "Epoch: 8800 Train Loss: 0.52202 Validation Loss: 0.54813\n",
            "Epoch: 8801 Train Loss: 0.52403 Validation Loss: 0.54939\n",
            "Epoch: 8802 Train Loss: 0.52610 Validation Loss: 0.54852\n",
            "Epoch: 8803 Train Loss: 0.51885 Validation Loss: 0.54844\n",
            "Epoch: 8804 Train Loss: 0.52242 Validation Loss: 0.54875\n",
            "Epoch: 8805 Train Loss: 0.52334 Validation Loss: 0.54841\n",
            "Epoch: 8806 Train Loss: 0.52352 Validation Loss: 0.54966\n",
            "Epoch: 8807 Train Loss: 0.52476 Validation Loss: 0.54878\n",
            "Epoch: 8808 Train Loss: 0.52198 Validation Loss: 0.54891\n",
            "Epoch: 8809 Train Loss: 0.52288 Validation Loss: 0.54849\n",
            "Epoch: 8810 Train Loss: 0.52185 Validation Loss: 0.54832\n",
            "Epoch: 8811 Train Loss: 0.52292 Validation Loss: 0.54819\n",
            "Epoch: 8812 Train Loss: 0.52316 Validation Loss: 0.54920\n",
            "Epoch: 8813 Train Loss: 0.52410 Validation Loss: 0.54867\n",
            "Epoch: 8814 Train Loss: 0.52597 Validation Loss: 0.54887\n",
            "Epoch: 8815 Train Loss: 0.52320 Validation Loss: 0.54864\n",
            "Epoch: 8816 Train Loss: 0.52287 Validation Loss: 0.54857\n",
            "Epoch: 8817 Train Loss: 0.52279 Validation Loss: 0.54828\n",
            "Epoch: 8818 Train Loss: 0.52387 Validation Loss: 0.54847\n",
            "Epoch: 8819 Train Loss: 0.52401 Validation Loss: 0.54904\n",
            "Epoch: 8820 Train Loss: 0.52573 Validation Loss: 0.54865\n",
            "Epoch: 8821 Train Loss: 0.52310 Validation Loss: 0.54806\n",
            "Epoch: 8822 Train Loss: 0.52208 Validation Loss: 0.54821\n",
            "Epoch: 8823 Train Loss: 0.52072 Validation Loss: 0.54908\n",
            "Epoch: 8824 Train Loss: 0.52575 Validation Loss: 0.54949\n",
            "Epoch: 8825 Train Loss: 0.52225 Validation Loss: 0.54805\n",
            "Epoch: 8826 Train Loss: 0.52265 Validation Loss: 0.54915\n",
            "Epoch: 8827 Train Loss: 0.52422 Validation Loss: 0.54819\n",
            "Epoch: 8828 Train Loss: 0.52196 Validation Loss: 0.54871\n",
            "Epoch: 8829 Train Loss: 0.52294 Validation Loss: 0.54882\n",
            "Epoch: 8830 Train Loss: 0.52208 Validation Loss: 0.54880\n",
            "Epoch: 8831 Train Loss: 0.52484 Validation Loss: 0.54890\n",
            "Epoch: 8832 Train Loss: 0.52599 Validation Loss: 0.54848\n",
            "Epoch: 8833 Train Loss: 0.52297 Validation Loss: 0.54822\n",
            "Epoch: 8834 Train Loss: 0.52413 Validation Loss: 0.54895\n",
            "Epoch: 8835 Train Loss: 0.52094 Validation Loss: 0.54823\n",
            "Epoch: 8836 Train Loss: 0.52273 Validation Loss: 0.54866\n",
            "Epoch: 8837 Train Loss: 0.52390 Validation Loss: 0.54926\n",
            "Epoch: 8838 Train Loss: 0.52451 Validation Loss: 0.54891\n",
            "Epoch: 8839 Train Loss: 0.52279 Validation Loss: 0.54853\n",
            "Epoch: 8840 Train Loss: 0.52611 Validation Loss: 0.54932\n",
            "Epoch: 8841 Train Loss: 0.51976 Validation Loss: 0.54823\n",
            "Epoch: 8842 Train Loss: 0.52490 Validation Loss: 0.54838\n",
            "Epoch: 8843 Train Loss: 0.52781 Validation Loss: 0.54926\n",
            "Epoch: 8844 Train Loss: 0.52165 Validation Loss: 0.54816\n",
            "Epoch: 8845 Train Loss: 0.52508 Validation Loss: 0.54848\n",
            "Epoch: 8846 Train Loss: 0.52092 Validation Loss: 0.54801\n",
            "Epoch: 8847 Train Loss: 0.52271 Validation Loss: 0.54852\n",
            "Epoch: 8848 Train Loss: 0.52411 Validation Loss: 0.54898\n",
            "Epoch: 8849 Train Loss: 0.52642 Validation Loss: 0.54850\n",
            "Epoch: 8850 Train Loss: 0.52514 Validation Loss: 0.54929\n",
            "Epoch: 8851 Train Loss: 0.52134 Validation Loss: 0.54809\n",
            "Epoch: 8852 Train Loss: 0.52600 Validation Loss: 0.54929\n",
            "Epoch: 8853 Train Loss: 0.52047 Validation Loss: 0.54805\n",
            "Epoch: 8854 Train Loss: 0.52269 Validation Loss: 0.54906\n",
            "Epoch: 8855 Train Loss: 0.52479 Validation Loss: 0.54887\n",
            "Epoch: 8856 Train Loss: 0.52226 Validation Loss: 0.54851\n",
            "Epoch: 8857 Train Loss: 0.52272 Validation Loss: 0.54919\n",
            "Epoch: 8858 Train Loss: 0.52118 Validation Loss: 0.54828\n",
            "Epoch: 8859 Train Loss: 0.52173 Validation Loss: 0.54939\n",
            "Epoch: 8860 Train Loss: 0.52314 Validation Loss: 0.54863\n",
            "Epoch: 8861 Train Loss: 0.52230 Validation Loss: 0.54833\n",
            "Epoch: 8862 Train Loss: 0.52166 Validation Loss: 0.54893\n",
            "Epoch: 8863 Train Loss: 0.52292 Validation Loss: 0.54878\n",
            "Epoch: 8864 Train Loss: 0.52091 Validation Loss: 0.54883\n",
            "Epoch: 8865 Train Loss: 0.52395 Validation Loss: 0.54908\n",
            "Epoch: 8866 Train Loss: 0.52189 Validation Loss: 0.54850\n",
            "Epoch: 8867 Train Loss: 0.52185 Validation Loss: 0.54871\n",
            "Epoch: 8868 Train Loss: 0.52296 Validation Loss: 0.54863\n",
            "Epoch: 8869 Train Loss: 0.52400 Validation Loss: 0.54803\n",
            "Epoch: 8870 Train Loss: 0.52090 Validation Loss: 0.54855\n",
            "Epoch: 8871 Train Loss: 0.52438 Validation Loss: 0.54959\n",
            "Epoch: 8872 Train Loss: 0.52411 Validation Loss: 0.54847\n",
            "Epoch: 8873 Train Loss: 0.52200 Validation Loss: 0.54868\n",
            "Epoch: 8874 Train Loss: 0.52612 Validation Loss: 0.54893\n",
            "Epoch: 8875 Train Loss: 0.52202 Validation Loss: 0.54806\n",
            "Epoch: 8876 Train Loss: 0.52288 Validation Loss: 0.54841\n",
            "Epoch: 8877 Train Loss: 0.52387 Validation Loss: 0.54860\n",
            "Epoch: 8878 Train Loss: 0.52191 Validation Loss: 0.54859\n",
            "Epoch: 8879 Train Loss: 0.52310 Validation Loss: 0.54834\n",
            "Epoch: 8880 Train Loss: 0.52270 Validation Loss: 0.54899\n",
            "Epoch: 8881 Train Loss: 0.52398 Validation Loss: 0.54946\n",
            "Epoch: 8882 Train Loss: 0.52352 Validation Loss: 0.54846\n",
            "Epoch: 8883 Train Loss: 0.52239 Validation Loss: 0.54990\n",
            "Epoch: 8884 Train Loss: 0.52391 Validation Loss: 0.54795\n",
            "Epoch: 8885 Train Loss: 0.52335 Validation Loss: 0.54806\n",
            "Epoch: 8886 Train Loss: 0.52479 Validation Loss: 0.54929\n",
            "Epoch: 8887 Train Loss: 0.52428 Validation Loss: 0.54794\n",
            "Epoch: 8888 Train Loss: 0.52270 Validation Loss: 0.54858\n",
            "Epoch: 8889 Train Loss: 0.52473 Validation Loss: 0.54886\n",
            "Epoch: 8890 Train Loss: 0.52493 Validation Loss: 0.54913\n",
            "Epoch: 8891 Train Loss: 0.52417 Validation Loss: 0.54861\n",
            "Epoch: 8892 Train Loss: 0.52195 Validation Loss: 0.54820\n",
            "Epoch: 8893 Train Loss: 0.52622 Validation Loss: 0.54895\n",
            "Epoch: 8894 Train Loss: 0.52017 Validation Loss: 0.54803\n",
            "Epoch: 8895 Train Loss: 0.52288 Validation Loss: 0.54896\n",
            "Epoch: 8896 Train Loss: 0.52510 Validation Loss: 0.54851\n",
            "Epoch: 8897 Train Loss: 0.52686 Validation Loss: 0.54865\n",
            "Epoch: 8898 Train Loss: 0.52389 Validation Loss: 0.54889\n",
            "Epoch: 8899 Train Loss: 0.52481 Validation Loss: 0.54812\n",
            "Epoch: 8900 Train Loss: 0.52382 Validation Loss: 0.54904\n",
            "Epoch: 8901 Train Loss: 0.52383 Validation Loss: 0.54860\n",
            "Epoch: 8902 Train Loss: 0.52411 Validation Loss: 0.54915\n",
            "Epoch: 8903 Train Loss: 0.52593 Validation Loss: 0.54884\n",
            "Epoch: 8904 Train Loss: 0.52285 Validation Loss: 0.54853\n",
            "Epoch: 8905 Train Loss: 0.52564 Validation Loss: 0.54814\n",
            "Epoch: 8906 Train Loss: 0.52006 Validation Loss: 0.54812\n",
            "Epoch: 8907 Train Loss: 0.52185 Validation Loss: 0.54892\n",
            "Epoch: 8908 Train Loss: 0.52388 Validation Loss: 0.54897\n",
            "Epoch: 8909 Train Loss: 0.52089 Validation Loss: 0.54848\n",
            "Epoch: 8910 Train Loss: 0.52501 Validation Loss: 0.54905\n",
            "Epoch: 8911 Train Loss: 0.52183 Validation Loss: 0.54875\n",
            "Epoch: 8912 Train Loss: 0.52381 Validation Loss: 0.54833\n",
            "Epoch: 8913 Train Loss: 0.52173 Validation Loss: 0.54847\n",
            "Epoch: 8914 Train Loss: 0.52285 Validation Loss: 0.54863\n",
            "Epoch: 8915 Train Loss: 0.51974 Validation Loss: 0.54904\n",
            "Epoch: 8916 Train Loss: 0.52226 Validation Loss: 0.54886\n",
            "Epoch: 8917 Train Loss: 0.52383 Validation Loss: 0.54869\n",
            "Epoch: 8918 Train Loss: 0.52076 Validation Loss: 0.54860\n",
            "Epoch: 8919 Train Loss: 0.52190 Validation Loss: 0.54861\n",
            "Epoch: 8920 Train Loss: 0.52274 Validation Loss: 0.54936\n",
            "Epoch: 8921 Train Loss: 0.52164 Validation Loss: 0.54980\n",
            "Epoch: 8922 Train Loss: 0.52170 Validation Loss: 0.54851\n",
            "Epoch: 8923 Train Loss: 0.52297 Validation Loss: 0.54808\n",
            "Epoch: 8924 Train Loss: 0.52171 Validation Loss: 0.54863\n",
            "Epoch: 8925 Train Loss: 0.52226 Validation Loss: 0.54853\n",
            "Epoch: 8926 Train Loss: 0.52296 Validation Loss: 0.54966\n",
            "Epoch: 8927 Train Loss: 0.51983 Validation Loss: 0.54860\n",
            "Epoch: 8928 Train Loss: 0.52507 Validation Loss: 0.54853\n",
            "Epoch: 8929 Train Loss: 0.52479 Validation Loss: 0.54878\n",
            "Epoch: 8930 Train Loss: 0.52295 Validation Loss: 0.54854\n",
            "Epoch: 8931 Train Loss: 0.52511 Validation Loss: 0.54914\n",
            "Epoch: 8932 Train Loss: 0.52513 Validation Loss: 0.54822\n",
            "Epoch: 8933 Train Loss: 0.52122 Validation Loss: 0.54848\n",
            "Epoch: 8934 Train Loss: 0.52492 Validation Loss: 0.54902\n",
            "Epoch: 8935 Train Loss: 0.52568 Validation Loss: 0.54827\n",
            "Epoch: 8936 Train Loss: 0.52084 Validation Loss: 0.54831\n",
            "Epoch: 8937 Train Loss: 0.52278 Validation Loss: 0.54895\n",
            "Epoch: 8938 Train Loss: 0.52303 Validation Loss: 0.54861\n",
            "Epoch: 8939 Train Loss: 0.52378 Validation Loss: 0.54892\n",
            "Epoch: 8940 Train Loss: 0.52404 Validation Loss: 0.54860\n",
            "Epoch: 8941 Train Loss: 0.52194 Validation Loss: 0.54867\n",
            "Epoch: 8942 Train Loss: 0.52087 Validation Loss: 0.54835\n",
            "Epoch: 8943 Train Loss: 0.52192 Validation Loss: 0.54898\n",
            "Epoch: 8944 Train Loss: 0.52679 Validation Loss: 0.54871\n",
            "Epoch: 8945 Train Loss: 0.52392 Validation Loss: 0.54851\n",
            "Epoch: 8946 Train Loss: 0.52387 Validation Loss: 0.54871\n",
            "Epoch: 8947 Train Loss: 0.52191 Validation Loss: 0.54868\n",
            "Epoch: 8948 Train Loss: 0.52199 Validation Loss: 0.54874\n",
            "Epoch: 8949 Train Loss: 0.52328 Validation Loss: 0.54892\n",
            "Epoch: 8950 Train Loss: 0.52566 Validation Loss: 0.54902\n",
            "Epoch: 8951 Train Loss: 0.52284 Validation Loss: 0.54757\n",
            "Epoch: 8952 Train Loss: 0.52323 Validation Loss: 0.54924\n",
            "Epoch: 8953 Train Loss: 0.51990 Validation Loss: 0.54855\n",
            "Epoch: 8954 Train Loss: 0.52321 Validation Loss: 0.54910\n",
            "Epoch: 8955 Train Loss: 0.52485 Validation Loss: 0.54838\n",
            "Epoch: 8956 Train Loss: 0.52100 Validation Loss: 0.54834\n",
            "Epoch: 8957 Train Loss: 0.52398 Validation Loss: 0.54853\n",
            "Epoch: 8958 Train Loss: 0.52483 Validation Loss: 0.54874\n",
            "Epoch: 8959 Train Loss: 0.52538 Validation Loss: 0.54902\n",
            "Epoch: 8960 Train Loss: 0.52322 Validation Loss: 0.54780\n",
            "Epoch: 8961 Train Loss: 0.52308 Validation Loss: 0.54847\n",
            "Epoch: 8962 Train Loss: 0.52390 Validation Loss: 0.54925\n",
            "Epoch: 8963 Train Loss: 0.52383 Validation Loss: 0.54858\n",
            "Epoch: 8964 Train Loss: 0.52295 Validation Loss: 0.54824\n",
            "Epoch: 8965 Train Loss: 0.52376 Validation Loss: 0.54855\n",
            "Epoch: 8966 Train Loss: 0.52079 Validation Loss: 0.54853\n",
            "Epoch: 8967 Train Loss: 0.52182 Validation Loss: 0.54841\n",
            "Epoch: 8968 Train Loss: 0.52454 Validation Loss: 0.54998\n",
            "Epoch: 8969 Train Loss: 0.52254 Validation Loss: 0.54851\n",
            "Epoch: 8970 Train Loss: 0.52511 Validation Loss: 0.54847\n",
            "Epoch: 8971 Train Loss: 0.52511 Validation Loss: 0.54833\n",
            "Epoch: 8972 Train Loss: 0.52666 Validation Loss: 0.54949\n",
            "Epoch: 8973 Train Loss: 0.52383 Validation Loss: 0.54834\n",
            "Epoch: 8974 Train Loss: 0.52212 Validation Loss: 0.54803\n",
            "Epoch: 8975 Train Loss: 0.52188 Validation Loss: 0.54830\n",
            "Epoch: 8976 Train Loss: 0.52103 Validation Loss: 0.54858\n",
            "Epoch: 8977 Train Loss: 0.52191 Validation Loss: 0.54900\n",
            "Epoch: 8978 Train Loss: 0.52304 Validation Loss: 0.54842\n",
            "Epoch: 8979 Train Loss: 0.52603 Validation Loss: 0.54869\n",
            "Epoch: 8980 Train Loss: 0.52477 Validation Loss: 0.54866\n",
            "Epoch: 8981 Train Loss: 0.52095 Validation Loss: 0.54879\n",
            "Epoch: 8982 Train Loss: 0.52686 Validation Loss: 0.54848\n",
            "Epoch: 8983 Train Loss: 0.52181 Validation Loss: 0.54819\n",
            "Epoch: 8984 Train Loss: 0.52581 Validation Loss: 0.54845\n",
            "Epoch: 8985 Train Loss: 0.52378 Validation Loss: 0.54861\n",
            "Epoch: 8986 Train Loss: 0.52378 Validation Loss: 0.54856\n",
            "Epoch: 8987 Train Loss: 0.52386 Validation Loss: 0.54831\n",
            "Epoch: 8988 Train Loss: 0.52302 Validation Loss: 0.54875\n",
            "Epoch: 8989 Train Loss: 0.52410 Validation Loss: 0.54904\n",
            "Epoch: 8990 Train Loss: 0.52705 Validation Loss: 0.54898\n",
            "Epoch: 8991 Train Loss: 0.52595 Validation Loss: 0.54802\n",
            "Epoch: 8992 Train Loss: 0.52327 Validation Loss: 0.54825\n",
            "Epoch: 8993 Train Loss: 0.52098 Validation Loss: 0.54875\n",
            "Epoch: 8994 Train Loss: 0.52330 Validation Loss: 0.54858\n",
            "Epoch: 8995 Train Loss: 0.52662 Validation Loss: 0.54940\n",
            "Epoch: 8996 Train Loss: 0.52670 Validation Loss: 0.54830\n",
            "Epoch: 8997 Train Loss: 0.52302 Validation Loss: 0.54791\n",
            "Epoch: 8998 Train Loss: 0.52588 Validation Loss: 0.54885\n",
            "Epoch: 8999 Train Loss: 0.52495 Validation Loss: 0.54914\n",
            "Epoch: 9000 Train Loss: 0.52813 Validation Loss: 0.54828\n",
            "Epoch: 9001 Train Loss: 0.52440 Validation Loss: 0.54854\n",
            "Epoch: 9002 Train Loss: 0.52484 Validation Loss: 0.54900\n",
            "Epoch: 9003 Train Loss: 0.52310 Validation Loss: 0.54838\n",
            "Epoch: 9004 Train Loss: 0.52292 Validation Loss: 0.54917\n",
            "Epoch: 9005 Train Loss: 0.52389 Validation Loss: 0.54894\n",
            "Epoch: 9006 Train Loss: 0.52574 Validation Loss: 0.54866\n",
            "Epoch: 9007 Train Loss: 0.52481 Validation Loss: 0.54840\n",
            "Epoch: 9008 Train Loss: 0.52509 Validation Loss: 0.54838\n",
            "Epoch: 9009 Train Loss: 0.52153 Validation Loss: 0.54794\n",
            "Epoch: 9010 Train Loss: 0.52602 Validation Loss: 0.54941\n",
            "Epoch: 9011 Train Loss: 0.52080 Validation Loss: 0.54853\n",
            "Epoch: 9012 Train Loss: 0.52677 Validation Loss: 0.54865\n",
            "Epoch: 9013 Train Loss: 0.52321 Validation Loss: 0.54896\n",
            "Epoch: 9014 Train Loss: 0.52178 Validation Loss: 0.54826\n",
            "Epoch: 9015 Train Loss: 0.52358 Validation Loss: 0.54811\n",
            "Epoch: 9016 Train Loss: 0.52577 Validation Loss: 0.54983\n",
            "Epoch: 9017 Train Loss: 0.52129 Validation Loss: 0.54851\n",
            "Epoch: 9018 Train Loss: 0.52094 Validation Loss: 0.54915\n",
            "Epoch: 9019 Train Loss: 0.52815 Validation Loss: 0.54902\n",
            "Epoch: 9020 Train Loss: 0.52305 Validation Loss: 0.54854\n",
            "Epoch: 9021 Train Loss: 0.52526 Validation Loss: 0.54892\n",
            "Epoch: 9022 Train Loss: 0.52265 Validation Loss: 0.54806\n",
            "Epoch: 9023 Train Loss: 0.52283 Validation Loss: 0.54902\n",
            "Epoch: 9024 Train Loss: 0.52085 Validation Loss: 0.54855\n",
            "Epoch: 9025 Train Loss: 0.52374 Validation Loss: 0.54880\n",
            "Epoch: 9026 Train Loss: 0.52176 Validation Loss: 0.54863\n",
            "Epoch: 9027 Train Loss: 0.52093 Validation Loss: 0.54835\n",
            "Epoch: 9028 Train Loss: 0.52103 Validation Loss: 0.54844\n",
            "Epoch: 9029 Train Loss: 0.52585 Validation Loss: 0.54953\n",
            "Epoch: 9030 Train Loss: 0.52499 Validation Loss: 0.54917\n",
            "Epoch: 9031 Train Loss: 0.52798 Validation Loss: 0.54889\n",
            "Epoch: 9032 Train Loss: 0.52373 Validation Loss: 0.54827\n",
            "Epoch: 9033 Train Loss: 0.52606 Validation Loss: 0.54876\n",
            "Epoch: 9034 Train Loss: 0.52810 Validation Loss: 0.54859\n",
            "Epoch: 9035 Train Loss: 0.52181 Validation Loss: 0.54760\n",
            "Epoch: 9036 Train Loss: 0.52801 Validation Loss: 0.54892\n",
            "Epoch: 9037 Train Loss: 0.52079 Validation Loss: 0.54841\n",
            "Epoch: 9038 Train Loss: 0.52697 Validation Loss: 0.54867\n",
            "Epoch: 9039 Train Loss: 0.52104 Validation Loss: 0.54833\n",
            "Epoch: 9040 Train Loss: 0.52705 Validation Loss: 0.54905\n",
            "Epoch: 9041 Train Loss: 0.52511 Validation Loss: 0.54823\n",
            "Epoch: 9042 Train Loss: 0.52187 Validation Loss: 0.54855\n",
            "Epoch: 9043 Train Loss: 0.52138 Validation Loss: 0.54824\n",
            "Epoch: 9044 Train Loss: 0.52330 Validation Loss: 0.55003\n",
            "Epoch: 9045 Train Loss: 0.52097 Validation Loss: 0.54842\n",
            "Epoch: 9046 Train Loss: 0.52611 Validation Loss: 0.54922\n",
            "Epoch: 9047 Train Loss: 0.52395 Validation Loss: 0.54816\n",
            "Epoch: 9048 Train Loss: 0.52488 Validation Loss: 0.54838\n",
            "Epoch: 9049 Train Loss: 0.52074 Validation Loss: 0.54863\n",
            "Epoch: 9050 Train Loss: 0.52249 Validation Loss: 0.54817\n",
            "Epoch: 9051 Train Loss: 0.52302 Validation Loss: 0.54941\n",
            "Epoch: 9052 Train Loss: 0.52433 Validation Loss: 0.54847\n",
            "Epoch: 9053 Train Loss: 0.52427 Validation Loss: 0.54923\n",
            "Epoch: 9054 Train Loss: 0.52012 Validation Loss: 0.54816\n",
            "Epoch: 9055 Train Loss: 0.52395 Validation Loss: 0.54902\n",
            "Epoch: 9056 Train Loss: 0.52283 Validation Loss: 0.54858\n",
            "Epoch: 9057 Train Loss: 0.52094 Validation Loss: 0.54853\n",
            "Epoch: 9058 Train Loss: 0.52089 Validation Loss: 0.54900\n",
            "Epoch: 9059 Train Loss: 0.52395 Validation Loss: 0.54852\n",
            "Epoch: 9060 Train Loss: 0.52801 Validation Loss: 0.54903\n",
            "Epoch: 9061 Train Loss: 0.52505 Validation Loss: 0.54859\n",
            "Epoch: 9062 Train Loss: 0.52253 Validation Loss: 0.54816\n",
            "Epoch: 9063 Train Loss: 0.52200 Validation Loss: 0.54912\n",
            "Epoch: 9064 Train Loss: 0.52519 Validation Loss: 0.54849\n",
            "Epoch: 9065 Train Loss: 0.52496 Validation Loss: 0.54893\n",
            "Epoch: 9066 Train Loss: 0.52383 Validation Loss: 0.54838\n",
            "Epoch: 9067 Train Loss: 0.52081 Validation Loss: 0.54829\n",
            "Epoch: 9068 Train Loss: 0.52280 Validation Loss: 0.54871\n",
            "Epoch: 9069 Train Loss: 0.52181 Validation Loss: 0.54869\n",
            "Epoch: 9070 Train Loss: 0.52740 Validation Loss: 0.54935\n",
            "Epoch: 9071 Train Loss: 0.52373 Validation Loss: 0.54794\n",
            "Epoch: 9072 Train Loss: 0.52495 Validation Loss: 0.54854\n",
            "Epoch: 9073 Train Loss: 0.52191 Validation Loss: 0.54882\n",
            "Epoch: 9074 Train Loss: 0.52293 Validation Loss: 0.54861\n",
            "Epoch: 9075 Train Loss: 0.52601 Validation Loss: 0.54851\n",
            "Epoch: 9076 Train Loss: 0.52205 Validation Loss: 0.54838\n",
            "Epoch: 9077 Train Loss: 0.52583 Validation Loss: 0.54913\n",
            "Epoch: 9078 Train Loss: 0.52239 Validation Loss: 0.54814\n",
            "Epoch: 9079 Train Loss: 0.52269 Validation Loss: 0.54868\n",
            "Epoch: 9080 Train Loss: 0.52492 Validation Loss: 0.54920\n",
            "Epoch: 9081 Train Loss: 0.52376 Validation Loss: 0.54876\n",
            "Epoch: 9082 Train Loss: 0.52016 Validation Loss: 0.54818\n",
            "Epoch: 9083 Train Loss: 0.52264 Validation Loss: 0.54900\n",
            "Epoch: 9084 Train Loss: 0.52844 Validation Loss: 0.54976\n",
            "Epoch: 9085 Train Loss: 0.52279 Validation Loss: 0.54848\n",
            "Epoch: 9086 Train Loss: 0.52352 Validation Loss: 0.54830\n",
            "Epoch: 9087 Train Loss: 0.52331 Validation Loss: 0.54880\n",
            "Epoch: 9088 Train Loss: 0.52378 Validation Loss: 0.54852\n",
            "Epoch: 9089 Train Loss: 0.51992 Validation Loss: 0.54811\n",
            "Epoch: 9090 Train Loss: 0.52592 Validation Loss: 0.54877\n",
            "Epoch: 9091 Train Loss: 0.52281 Validation Loss: 0.54886\n",
            "Epoch: 9092 Train Loss: 0.52399 Validation Loss: 0.54881\n",
            "Epoch: 9093 Train Loss: 0.52381 Validation Loss: 0.54860\n",
            "Epoch: 9094 Train Loss: 0.52266 Validation Loss: 0.54816\n",
            "Epoch: 9095 Train Loss: 0.52397 Validation Loss: 0.54880\n",
            "Epoch: 9096 Train Loss: 0.51967 Validation Loss: 0.54904\n",
            "Epoch: 9097 Train Loss: 0.52406 Validation Loss: 0.54922\n",
            "Epoch: 9098 Train Loss: 0.52381 Validation Loss: 0.54865\n",
            "Epoch: 9099 Train Loss: 0.52582 Validation Loss: 0.54827\n",
            "Epoch: 9100 Train Loss: 0.52203 Validation Loss: 0.54857\n",
            "Epoch: 9101 Train Loss: 0.52294 Validation Loss: 0.54841\n",
            "Epoch: 9102 Train Loss: 0.52449 Validation Loss: 0.54879\n",
            "Epoch: 9103 Train Loss: 0.52308 Validation Loss: 0.54909\n",
            "Epoch: 9104 Train Loss: 0.52085 Validation Loss: 0.54910\n",
            "Epoch: 9105 Train Loss: 0.52718 Validation Loss: 0.54941\n",
            "Epoch: 9106 Train Loss: 0.52468 Validation Loss: 0.54850\n",
            "Epoch: 9107 Train Loss: 0.52620 Validation Loss: 0.54826\n",
            "Epoch: 9108 Train Loss: 0.52276 Validation Loss: 0.54867\n",
            "Epoch: 9109 Train Loss: 0.52575 Validation Loss: 0.54874\n",
            "Epoch: 9110 Train Loss: 0.52394 Validation Loss: 0.54866\n",
            "Epoch: 9111 Train Loss: 0.52573 Validation Loss: 0.54838\n",
            "Epoch: 9112 Train Loss: 0.52097 Validation Loss: 0.54818\n",
            "Epoch: 9113 Train Loss: 0.52308 Validation Loss: 0.54867\n",
            "Epoch: 9114 Train Loss: 0.52315 Validation Loss: 0.54842\n",
            "Epoch: 9115 Train Loss: 0.52193 Validation Loss: 0.54851\n",
            "Epoch: 9116 Train Loss: 0.52275 Validation Loss: 0.54942\n",
            "Epoch: 9117 Train Loss: 0.52350 Validation Loss: 0.54849\n",
            "Epoch: 9118 Train Loss: 0.52094 Validation Loss: 0.54845\n",
            "Epoch: 9119 Train Loss: 0.52480 Validation Loss: 0.54905\n",
            "Epoch: 9120 Train Loss: 0.52444 Validation Loss: 0.54831\n",
            "Epoch: 9121 Train Loss: 0.52093 Validation Loss: 0.54904\n",
            "Epoch: 9122 Train Loss: 0.52190 Validation Loss: 0.54862\n",
            "Epoch: 9123 Train Loss: 0.52283 Validation Loss: 0.54902\n",
            "Epoch: 9124 Train Loss: 0.52206 Validation Loss: 0.54827\n",
            "Epoch: 9125 Train Loss: 0.52227 Validation Loss: 0.54878\n",
            "Epoch: 9126 Train Loss: 0.52165 Validation Loss: 0.54815\n",
            "Epoch: 9127 Train Loss: 0.52287 Validation Loss: 0.54858\n",
            "Epoch: 9128 Train Loss: 0.52338 Validation Loss: 0.54959\n",
            "Epoch: 9129 Train Loss: 0.52292 Validation Loss: 0.54832\n",
            "Epoch: 9130 Train Loss: 0.52408 Validation Loss: 0.54898\n",
            "Epoch: 9131 Train Loss: 0.52287 Validation Loss: 0.54897\n",
            "Epoch: 9132 Train Loss: 0.52282 Validation Loss: 0.54843\n",
            "Epoch: 9133 Train Loss: 0.52285 Validation Loss: 0.54852\n",
            "Epoch: 9134 Train Loss: 0.52201 Validation Loss: 0.54861\n",
            "Epoch: 9135 Train Loss: 0.52285 Validation Loss: 0.54851\n",
            "Epoch: 9136 Train Loss: 0.52418 Validation Loss: 0.54902\n",
            "Epoch: 9137 Train Loss: 0.52396 Validation Loss: 0.54885\n",
            "Epoch: 9138 Train Loss: 0.52184 Validation Loss: 0.54795\n",
            "Epoch: 9139 Train Loss: 0.52578 Validation Loss: 0.54849\n",
            "Epoch: 9140 Train Loss: 0.52288 Validation Loss: 0.54859\n",
            "Epoch: 9141 Train Loss: 0.52139 Validation Loss: 0.54850\n",
            "Epoch: 9142 Train Loss: 0.52463 Validation Loss: 0.55014\n",
            "Epoch: 9143 Train Loss: 0.52296 Validation Loss: 0.54831\n",
            "Epoch: 9144 Train Loss: 0.52410 Validation Loss: 0.54905\n",
            "Epoch: 9145 Train Loss: 0.52389 Validation Loss: 0.54813\n",
            "Epoch: 9146 Train Loss: 0.52399 Validation Loss: 0.54832\n",
            "Epoch: 9147 Train Loss: 0.52979 Validation Loss: 0.55003\n",
            "Epoch: 9148 Train Loss: 0.52227 Validation Loss: 0.54796\n",
            "Epoch: 9149 Train Loss: 0.52390 Validation Loss: 0.54887\n",
            "Epoch: 9150 Train Loss: 0.52209 Validation Loss: 0.54839\n",
            "Epoch: 9151 Train Loss: 0.52188 Validation Loss: 0.54923\n",
            "Epoch: 9152 Train Loss: 0.52405 Validation Loss: 0.54854\n",
            "Epoch: 9153 Train Loss: 0.52289 Validation Loss: 0.54899\n",
            "Epoch: 9154 Train Loss: 0.52286 Validation Loss: 0.54887\n",
            "Epoch: 9155 Train Loss: 0.52374 Validation Loss: 0.54830\n",
            "Epoch: 9156 Train Loss: 0.52384 Validation Loss: 0.54863\n",
            "Epoch: 9157 Train Loss: 0.52379 Validation Loss: 0.54835\n",
            "Epoch: 9158 Train Loss: 0.52273 Validation Loss: 0.54856\n",
            "Epoch: 9159 Train Loss: 0.52705 Validation Loss: 0.54925\n",
            "Epoch: 9160 Train Loss: 0.52461 Validation Loss: 0.54821\n",
            "Epoch: 9161 Train Loss: 0.52507 Validation Loss: 0.54925\n",
            "Epoch: 9162 Train Loss: 0.52209 Validation Loss: 0.54887\n",
            "Epoch: 9163 Train Loss: 0.52274 Validation Loss: 0.54794\n",
            "Epoch: 9164 Train Loss: 0.52678 Validation Loss: 0.54916\n",
            "Epoch: 9165 Train Loss: 0.52191 Validation Loss: 0.54777\n",
            "Epoch: 9166 Train Loss: 0.52391 Validation Loss: 0.54906\n",
            "Epoch: 9167 Train Loss: 0.52394 Validation Loss: 0.54852\n",
            "Epoch: 9168 Train Loss: 0.52281 Validation Loss: 0.54862\n",
            "Epoch: 9169 Train Loss: 0.52471 Validation Loss: 0.54800\n",
            "Epoch: 9170 Train Loss: 0.52708 Validation Loss: 0.54893\n",
            "Epoch: 9171 Train Loss: 0.52097 Validation Loss: 0.54829\n",
            "Epoch: 9172 Train Loss: 0.52374 Validation Loss: 0.54861\n",
            "Epoch: 9173 Train Loss: 0.52299 Validation Loss: 0.54858\n",
            "Epoch: 9174 Train Loss: 0.52286 Validation Loss: 0.54860\n",
            "Epoch: 9175 Train Loss: 0.52391 Validation Loss: 0.54849\n",
            "Epoch: 9176 Train Loss: 0.52308 Validation Loss: 0.54886\n",
            "Epoch: 9177 Train Loss: 0.52494 Validation Loss: 0.54887\n",
            "Epoch: 9178 Train Loss: 0.52288 Validation Loss: 0.54843\n",
            "Epoch: 9179 Train Loss: 0.52306 Validation Loss: 0.54879\n",
            "Epoch: 9180 Train Loss: 0.52100 Validation Loss: 0.54842\n",
            "Epoch: 9181 Train Loss: 0.52279 Validation Loss: 0.54854\n",
            "Epoch: 9182 Train Loss: 0.52301 Validation Loss: 0.54863\n",
            "Epoch: 9183 Train Loss: 0.52181 Validation Loss: 0.54831\n",
            "Epoch: 9184 Train Loss: 0.52206 Validation Loss: 0.54820\n",
            "Epoch: 9185 Train Loss: 0.52475 Validation Loss: 0.54933\n",
            "Epoch: 9186 Train Loss: 0.52736 Validation Loss: 0.54919\n",
            "Epoch: 9187 Train Loss: 0.52429 Validation Loss: 0.54767\n",
            "Epoch: 9188 Train Loss: 0.52396 Validation Loss: 0.54932\n",
            "Epoch: 9189 Train Loss: 0.52196 Validation Loss: 0.54829\n",
            "Epoch: 9190 Train Loss: 0.52285 Validation Loss: 0.54918\n",
            "Epoch: 9191 Train Loss: 0.52389 Validation Loss: 0.54896\n",
            "Epoch: 9192 Train Loss: 0.52315 Validation Loss: 0.54783\n",
            "Epoch: 9193 Train Loss: 0.52076 Validation Loss: 0.54897\n",
            "Epoch: 9194 Train Loss: 0.52185 Validation Loss: 0.54902\n",
            "Epoch: 9195 Train Loss: 0.52403 Validation Loss: 0.54913\n",
            "Epoch: 9196 Train Loss: 0.52307 Validation Loss: 0.54871\n",
            "Epoch: 9197 Train Loss: 0.52197 Validation Loss: 0.54825\n",
            "Epoch: 9198 Train Loss: 0.52192 Validation Loss: 0.54868\n",
            "Epoch: 9199 Train Loss: 0.52621 Validation Loss: 0.54853\n",
            "Epoch: 9200 Train Loss: 0.52400 Validation Loss: 0.54853\n",
            "Epoch: 9201 Train Loss: 0.52534 Validation Loss: 0.54857\n",
            "Epoch: 9202 Train Loss: 0.52590 Validation Loss: 0.54869\n",
            "Epoch: 9203 Train Loss: 0.52280 Validation Loss: 0.54839\n",
            "Epoch: 9204 Train Loss: 0.52417 Validation Loss: 0.54833\n",
            "Epoch: 9205 Train Loss: 0.52276 Validation Loss: 0.54852\n",
            "Epoch: 9206 Train Loss: 0.52317 Validation Loss: 0.54911\n",
            "Epoch: 9207 Train Loss: 0.52380 Validation Loss: 0.54795\n",
            "Epoch: 9208 Train Loss: 0.52380 Validation Loss: 0.54908\n",
            "Epoch: 9209 Train Loss: 0.52374 Validation Loss: 0.54848\n",
            "Epoch: 9210 Train Loss: 0.52296 Validation Loss: 0.54863\n",
            "Epoch: 9211 Train Loss: 0.52508 Validation Loss: 0.54916\n",
            "Epoch: 9212 Train Loss: 0.52113 Validation Loss: 0.54825\n",
            "Epoch: 9213 Train Loss: 0.52130 Validation Loss: 0.54839\n",
            "Epoch: 9214 Train Loss: 0.52499 Validation Loss: 0.54975\n",
            "Epoch: 9215 Train Loss: 0.52435 Validation Loss: 0.54802\n",
            "Epoch: 9216 Train Loss: 0.52068 Validation Loss: 0.54861\n",
            "Epoch: 9217 Train Loss: 0.52064 Validation Loss: 0.54919\n",
            "Epoch: 9218 Train Loss: 0.52094 Validation Loss: 0.54899\n",
            "Epoch: 9219 Train Loss: 0.52570 Validation Loss: 0.54876\n",
            "Epoch: 9220 Train Loss: 0.52504 Validation Loss: 0.54932\n",
            "Epoch: 9221 Train Loss: 0.52349 Validation Loss: 0.54849\n",
            "Epoch: 9222 Train Loss: 0.52621 Validation Loss: 0.54894\n",
            "Epoch: 9223 Train Loss: 0.52285 Validation Loss: 0.54838\n",
            "Epoch: 9224 Train Loss: 0.52480 Validation Loss: 0.54854\n",
            "Epoch: 9225 Train Loss: 0.52480 Validation Loss: 0.54846\n",
            "Epoch: 9226 Train Loss: 0.52540 Validation Loss: 0.54845\n",
            "Epoch: 9227 Train Loss: 0.52527 Validation Loss: 0.54884\n",
            "Epoch: 9228 Train Loss: 0.52183 Validation Loss: 0.54819\n",
            "Epoch: 9229 Train Loss: 0.52205 Validation Loss: 0.54837\n",
            "Epoch: 9230 Train Loss: 0.52046 Validation Loss: 0.54799\n",
            "Epoch: 9231 Train Loss: 0.52705 Validation Loss: 0.55007\n",
            "Epoch: 9232 Train Loss: 0.52590 Validation Loss: 0.54922\n",
            "Epoch: 9233 Train Loss: 0.52502 Validation Loss: 0.54813\n",
            "Epoch: 9234 Train Loss: 0.52002 Validation Loss: 0.54809\n",
            "Epoch: 9235 Train Loss: 0.52368 Validation Loss: 0.54885\n",
            "Epoch: 9236 Train Loss: 0.52097 Validation Loss: 0.54884\n",
            "Epoch: 9237 Train Loss: 0.52270 Validation Loss: 0.54888\n",
            "Epoch: 9238 Train Loss: 0.52397 Validation Loss: 0.54841\n",
            "Epoch: 9239 Train Loss: 0.52281 Validation Loss: 0.54871\n",
            "Epoch: 9240 Train Loss: 0.52282 Validation Loss: 0.54875\n",
            "Epoch: 9241 Train Loss: 0.52287 Validation Loss: 0.54853\n",
            "Epoch: 9242 Train Loss: 0.52647 Validation Loss: 0.54949\n",
            "Epoch: 9243 Train Loss: 0.52461 Validation Loss: 0.54819\n",
            "Epoch: 9244 Train Loss: 0.52300 Validation Loss: 0.54802\n",
            "Epoch: 9245 Train Loss: 0.52186 Validation Loss: 0.54849\n",
            "Epoch: 9246 Train Loss: 0.52180 Validation Loss: 0.54902\n",
            "Epoch: 9247 Train Loss: 0.52082 Validation Loss: 0.54850\n",
            "Epoch: 9248 Train Loss: 0.52189 Validation Loss: 0.54898\n",
            "Epoch: 9249 Train Loss: 0.52319 Validation Loss: 0.54873\n",
            "Epoch: 9250 Train Loss: 0.52326 Validation Loss: 0.54798\n",
            "Epoch: 9251 Train Loss: 0.52267 Validation Loss: 0.54869\n",
            "Epoch: 9252 Train Loss: 0.52026 Validation Loss: 0.54862\n",
            "Epoch: 9253 Train Loss: 0.52546 Validation Loss: 0.55008\n",
            "Epoch: 9254 Train Loss: 0.52454 Validation Loss: 0.54812\n",
            "Epoch: 9255 Train Loss: 0.52084 Validation Loss: 0.54864\n",
            "Epoch: 9256 Train Loss: 0.51990 Validation Loss: 0.54919\n",
            "Epoch: 9257 Train Loss: 0.52224 Validation Loss: 0.54913\n",
            "Epoch: 9258 Train Loss: 0.52039 Validation Loss: 0.54787\n",
            "Epoch: 9259 Train Loss: 0.52594 Validation Loss: 0.54944\n",
            "Epoch: 9260 Train Loss: 0.52326 Validation Loss: 0.54846\n",
            "Epoch: 9261 Train Loss: 0.52401 Validation Loss: 0.54874\n",
            "Epoch: 9262 Train Loss: 0.52488 Validation Loss: 0.54853\n",
            "Epoch: 9263 Train Loss: 0.52521 Validation Loss: 0.54836\n",
            "Epoch: 9264 Train Loss: 0.52354 Validation Loss: 0.54893\n",
            "Epoch: 9265 Train Loss: 0.52687 Validation Loss: 0.54829\n",
            "Epoch: 9266 Train Loss: 0.52490 Validation Loss: 0.54819\n",
            "Epoch: 9267 Train Loss: 0.52259 Validation Loss: 0.55001\n",
            "Epoch: 9268 Train Loss: 0.52275 Validation Loss: 0.54858\n",
            "Epoch: 9269 Train Loss: 0.52377 Validation Loss: 0.54869\n",
            "Epoch: 9270 Train Loss: 0.52579 Validation Loss: 0.54858\n",
            "Epoch: 9271 Train Loss: 0.52081 Validation Loss: 0.54818\n",
            "Epoch: 9272 Train Loss: 0.52696 Validation Loss: 0.54901\n",
            "Epoch: 9273 Train Loss: 0.52078 Validation Loss: 0.54831\n",
            "Epoch: 9274 Train Loss: 0.52244 Validation Loss: 0.54835\n",
            "Epoch: 9275 Train Loss: 0.52539 Validation Loss: 0.54974\n",
            "Epoch: 9276 Train Loss: 0.52692 Validation Loss: 0.54833\n",
            "Epoch: 9277 Train Loss: 0.52330 Validation Loss: 0.54789\n",
            "Epoch: 9278 Train Loss: 0.52180 Validation Loss: 0.54902\n",
            "Epoch: 9279 Train Loss: 0.52126 Validation Loss: 0.54915\n",
            "Epoch: 9280 Train Loss: 0.52488 Validation Loss: 0.54841\n",
            "Epoch: 9281 Train Loss: 0.52389 Validation Loss: 0.54865\n",
            "Epoch: 9282 Train Loss: 0.52492 Validation Loss: 0.54850\n",
            "Epoch: 9283 Train Loss: 0.52286 Validation Loss: 0.54829\n",
            "Epoch: 9284 Train Loss: 0.52090 Validation Loss: 0.54876\n",
            "Epoch: 9285 Train Loss: 0.52090 Validation Loss: 0.54959\n",
            "Epoch: 9286 Train Loss: 0.52221 Validation Loss: 0.54839\n",
            "Epoch: 9287 Train Loss: 0.52242 Validation Loss: 0.54912\n",
            "Epoch: 9288 Train Loss: 0.52239 Validation Loss: 0.54809\n",
            "Epoch: 9289 Train Loss: 0.52636 Validation Loss: 0.54911\n",
            "Epoch: 9290 Train Loss: 0.52521 Validation Loss: 0.54804\n",
            "Epoch: 9291 Train Loss: 0.52398 Validation Loss: 0.54819\n",
            "Epoch: 9292 Train Loss: 0.52619 Validation Loss: 0.54962\n",
            "Epoch: 9293 Train Loss: 0.52378 Validation Loss: 0.54841\n",
            "Epoch: 9294 Train Loss: 0.52441 Validation Loss: 0.54854\n",
            "Epoch: 9295 Train Loss: 0.52284 Validation Loss: 0.54869\n",
            "Epoch: 9296 Train Loss: 0.52392 Validation Loss: 0.54895\n",
            "Epoch: 9297 Train Loss: 0.52542 Validation Loss: 0.54904\n",
            "Epoch: 9298 Train Loss: 0.52366 Validation Loss: 0.54795\n",
            "Epoch: 9299 Train Loss: 0.52186 Validation Loss: 0.54833\n",
            "Epoch: 9300 Train Loss: 0.52381 Validation Loss: 0.54847\n",
            "Epoch: 9301 Train Loss: 0.52525 Validation Loss: 0.54926\n",
            "Epoch: 9302 Train Loss: 0.52400 Validation Loss: 0.54808\n",
            "Epoch: 9303 Train Loss: 0.52285 Validation Loss: 0.54816\n",
            "Epoch: 9304 Train Loss: 0.52447 Validation Loss: 0.54832\n",
            "Epoch: 9305 Train Loss: 0.52282 Validation Loss: 0.54934\n",
            "Epoch: 9306 Train Loss: 0.52371 Validation Loss: 0.54868\n",
            "Epoch: 9307 Train Loss: 0.52200 Validation Loss: 0.54809\n",
            "Epoch: 9308 Train Loss: 0.52385 Validation Loss: 0.54870\n",
            "Epoch: 9309 Train Loss: 0.52506 Validation Loss: 0.54885\n",
            "Epoch: 9310 Train Loss: 0.52202 Validation Loss: 0.54823\n",
            "Epoch: 9311 Train Loss: 0.52513 Validation Loss: 0.54924\n",
            "Epoch: 9312 Train Loss: 0.52197 Validation Loss: 0.54821\n",
            "Epoch: 9313 Train Loss: 0.52370 Validation Loss: 0.54865\n",
            "Epoch: 9314 Train Loss: 0.52285 Validation Loss: 0.54904\n",
            "Epoch: 9315 Train Loss: 0.52086 Validation Loss: 0.54859\n",
            "Epoch: 9316 Train Loss: 0.52218 Validation Loss: 0.54842\n",
            "Epoch: 9317 Train Loss: 0.52214 Validation Loss: 0.54946\n",
            "Epoch: 9318 Train Loss: 0.52382 Validation Loss: 0.54848\n",
            "Epoch: 9319 Train Loss: 0.52435 Validation Loss: 0.54880\n",
            "Epoch: 9320 Train Loss: 0.52211 Validation Loss: 0.54820\n",
            "Epoch: 9321 Train Loss: 0.52376 Validation Loss: 0.54880\n",
            "Epoch: 9322 Train Loss: 0.52482 Validation Loss: 0.54909\n",
            "Epoch: 9323 Train Loss: 0.52317 Validation Loss: 0.54912\n",
            "Epoch: 9324 Train Loss: 0.52460 Validation Loss: 0.54795\n",
            "Epoch: 9325 Train Loss: 0.52394 Validation Loss: 0.54901\n",
            "Epoch: 9326 Train Loss: 0.52382 Validation Loss: 0.54893\n",
            "Epoch: 9327 Train Loss: 0.52481 Validation Loss: 0.54860\n",
            "Epoch: 9328 Train Loss: 0.52281 Validation Loss: 0.54814\n",
            "Epoch: 9329 Train Loss: 0.52588 Validation Loss: 0.54843\n",
            "Epoch: 9330 Train Loss: 0.52186 Validation Loss: 0.54877\n",
            "Epoch: 9331 Train Loss: 0.52083 Validation Loss: 0.54839\n",
            "Epoch: 9332 Train Loss: 0.52382 Validation Loss: 0.54867\n",
            "Epoch: 9333 Train Loss: 0.52193 Validation Loss: 0.54846\n",
            "Epoch: 9334 Train Loss: 0.52301 Validation Loss: 0.54906\n",
            "Epoch: 9335 Train Loss: 0.52482 Validation Loss: 0.54899\n",
            "Epoch: 9336 Train Loss: 0.52574 Validation Loss: 0.54864\n",
            "Epoch: 9337 Train Loss: 0.52602 Validation Loss: 0.54822\n",
            "Epoch: 9338 Train Loss: 0.52309 Validation Loss: 0.54827\n",
            "Epoch: 9339 Train Loss: 0.52403 Validation Loss: 0.54949\n",
            "Epoch: 9340 Train Loss: 0.52895 Validation Loss: 0.54885\n",
            "Epoch: 9341 Train Loss: 0.52387 Validation Loss: 0.54802\n",
            "Epoch: 9342 Train Loss: 0.52721 Validation Loss: 0.54894\n",
            "Epoch: 9343 Train Loss: 0.52683 Validation Loss: 0.54832\n",
            "Epoch: 9344 Train Loss: 0.52018 Validation Loss: 0.54809\n",
            "Epoch: 9345 Train Loss: 0.52124 Validation Loss: 0.54943\n",
            "Epoch: 9346 Train Loss: 0.52211 Validation Loss: 0.54863\n",
            "Epoch: 9347 Train Loss: 0.52197 Validation Loss: 0.54864\n",
            "Epoch: 9348 Train Loss: 0.52481 Validation Loss: 0.54912\n",
            "Epoch: 9349 Train Loss: 0.52183 Validation Loss: 0.54880\n",
            "Epoch: 9350 Train Loss: 0.52617 Validation Loss: 0.54960\n",
            "Epoch: 9351 Train Loss: 0.51977 Validation Loss: 0.54841\n",
            "Epoch: 9352 Train Loss: 0.52086 Validation Loss: 0.54825\n",
            "Epoch: 9353 Train Loss: 0.52380 Validation Loss: 0.54879\n",
            "Epoch: 9354 Train Loss: 0.52379 Validation Loss: 0.54865\n",
            "Epoch: 9355 Train Loss: 0.52288 Validation Loss: 0.54887\n",
            "Epoch: 9356 Train Loss: 0.52194 Validation Loss: 0.54828\n",
            "Epoch: 9357 Train Loss: 0.52518 Validation Loss: 0.54856\n",
            "Epoch: 9358 Train Loss: 0.52479 Validation Loss: 0.54867\n",
            "Epoch: 9359 Train Loss: 0.52073 Validation Loss: 0.54845\n",
            "Epoch: 9360 Train Loss: 0.52455 Validation Loss: 0.54811\n",
            "Epoch: 9361 Train Loss: 0.52827 Validation Loss: 0.54952\n",
            "Epoch: 9362 Train Loss: 0.52293 Validation Loss: 0.54844\n",
            "Epoch: 9363 Train Loss: 0.52406 Validation Loss: 0.54825\n",
            "Epoch: 9364 Train Loss: 0.52542 Validation Loss: 0.54944\n",
            "Epoch: 9365 Train Loss: 0.52280 Validation Loss: 0.54844\n",
            "Epoch: 9366 Train Loss: 0.52527 Validation Loss: 0.54813\n",
            "Epoch: 9367 Train Loss: 0.52281 Validation Loss: 0.54831\n",
            "Epoch: 9368 Train Loss: 0.52127 Validation Loss: 0.54886\n",
            "Epoch: 9369 Train Loss: 0.52813 Validation Loss: 0.54908\n",
            "Epoch: 9370 Train Loss: 0.52236 Validation Loss: 0.54806\n",
            "Epoch: 9371 Train Loss: 0.52301 Validation Loss: 0.54815\n",
            "Epoch: 9372 Train Loss: 0.52098 Validation Loss: 0.54864\n",
            "Epoch: 9373 Train Loss: 0.52076 Validation Loss: 0.54889\n",
            "Epoch: 9374 Train Loss: 0.52399 Validation Loss: 0.54927\n",
            "Epoch: 9375 Train Loss: 0.52277 Validation Loss: 0.54841\n",
            "Epoch: 9376 Train Loss: 0.52080 Validation Loss: 0.54836\n",
            "Epoch: 9377 Train Loss: 0.52192 Validation Loss: 0.54853\n",
            "Epoch: 9378 Train Loss: 0.52411 Validation Loss: 0.54933\n",
            "Epoch: 9379 Train Loss: 0.52123 Validation Loss: 0.54840\n",
            "Epoch: 9380 Train Loss: 0.52383 Validation Loss: 0.54882\n",
            "Epoch: 9381 Train Loss: 0.52580 Validation Loss: 0.54889\n",
            "Epoch: 9382 Train Loss: 0.52287 Validation Loss: 0.54813\n",
            "Epoch: 9383 Train Loss: 0.52293 Validation Loss: 0.54870\n",
            "Epoch: 9384 Train Loss: 0.52194 Validation Loss: 0.54837\n",
            "Epoch: 9385 Train Loss: 0.52274 Validation Loss: 0.54869\n",
            "Epoch: 9386 Train Loss: 0.52393 Validation Loss: 0.54886\n",
            "Epoch: 9387 Train Loss: 0.52616 Validation Loss: 0.54832\n",
            "Epoch: 9388 Train Loss: 0.52073 Validation Loss: 0.54860\n",
            "Epoch: 9389 Train Loss: 0.52279 Validation Loss: 0.54898\n",
            "Epoch: 9390 Train Loss: 0.52291 Validation Loss: 0.54864\n",
            "Epoch: 9391 Train Loss: 0.52498 Validation Loss: 0.54859\n",
            "Epoch: 9392 Train Loss: 0.52180 Validation Loss: 0.54870\n",
            "Epoch: 9393 Train Loss: 0.52389 Validation Loss: 0.54859\n",
            "Epoch: 9394 Train Loss: 0.52511 Validation Loss: 0.54890\n",
            "Epoch: 9395 Train Loss: 0.52323 Validation Loss: 0.54797\n",
            "Epoch: 9396 Train Loss: 0.52169 Validation Loss: 0.54887\n",
            "Epoch: 9397 Train Loss: 0.52109 Validation Loss: 0.54858\n",
            "Epoch: 9398 Train Loss: 0.52328 Validation Loss: 0.54946\n",
            "Epoch: 9399 Train Loss: 0.52206 Validation Loss: 0.54837\n",
            "Epoch: 9400 Train Loss: 0.52685 Validation Loss: 0.54901\n",
            "Epoch: 9401 Train Loss: 0.52276 Validation Loss: 0.54859\n",
            "Epoch: 9402 Train Loss: 0.52091 Validation Loss: 0.54839\n",
            "Epoch: 9403 Train Loss: 0.52678 Validation Loss: 0.54872\n",
            "Epoch: 9404 Train Loss: 0.52280 Validation Loss: 0.54828\n",
            "Epoch: 9405 Train Loss: 0.52323 Validation Loss: 0.54909\n",
            "Epoch: 9406 Train Loss: 0.52174 Validation Loss: 0.54899\n",
            "Epoch: 9407 Train Loss: 0.52189 Validation Loss: 0.54858\n",
            "Epoch: 9408 Train Loss: 0.52224 Validation Loss: 0.54894\n",
            "Epoch: 9409 Train Loss: 0.52313 Validation Loss: 0.54822\n",
            "Epoch: 9410 Train Loss: 0.52379 Validation Loss: 0.54898\n",
            "Epoch: 9411 Train Loss: 0.51908 Validation Loss: 0.54839\n",
            "Epoch: 9412 Train Loss: 0.52171 Validation Loss: 0.54881\n",
            "Epoch: 9413 Train Loss: 0.52194 Validation Loss: 0.54918\n",
            "Epoch: 9414 Train Loss: 0.52323 Validation Loss: 0.54827\n",
            "Epoch: 9415 Train Loss: 0.52376 Validation Loss: 0.54918\n",
            "Epoch: 9416 Train Loss: 0.52197 Validation Loss: 0.54843\n",
            "Epoch: 9417 Train Loss: 0.52282 Validation Loss: 0.54894\n",
            "Epoch: 9418 Train Loss: 0.52511 Validation Loss: 0.54895\n",
            "Epoch: 9419 Train Loss: 0.52502 Validation Loss: 0.54810\n",
            "Epoch: 9420 Train Loss: 0.52428 Validation Loss: 0.54870\n",
            "Epoch: 9421 Train Loss: 0.52115 Validation Loss: 0.54919\n",
            "Epoch: 9422 Train Loss: 0.52395 Validation Loss: 0.54888\n",
            "Epoch: 9423 Train Loss: 0.52209 Validation Loss: 0.54801\n",
            "Epoch: 9424 Train Loss: 0.52198 Validation Loss: 0.54870\n",
            "Epoch: 9425 Train Loss: 0.52383 Validation Loss: 0.54878\n",
            "Epoch: 9426 Train Loss: 0.52519 Validation Loss: 0.54843\n",
            "Epoch: 9427 Train Loss: 0.52463 Validation Loss: 0.54924\n",
            "Epoch: 9428 Train Loss: 0.52195 Validation Loss: 0.54814\n",
            "Epoch: 9429 Train Loss: 0.52113 Validation Loss: 0.54822\n",
            "Epoch: 9430 Train Loss: 0.52232 Validation Loss: 0.54953\n",
            "Epoch: 9431 Train Loss: 0.52191 Validation Loss: 0.54862\n",
            "Epoch: 9432 Train Loss: 0.52387 Validation Loss: 0.54895\n",
            "Epoch: 9433 Train Loss: 0.52245 Validation Loss: 0.54946\n",
            "Epoch: 9434 Train Loss: 0.52096 Validation Loss: 0.54814\n",
            "Epoch: 9435 Train Loss: 0.52917 Validation Loss: 0.54860\n",
            "Epoch: 9436 Train Loss: 0.51991 Validation Loss: 0.54879\n",
            "Epoch: 9437 Train Loss: 0.52291 Validation Loss: 0.54907\n",
            "Epoch: 9438 Train Loss: 0.52185 Validation Loss: 0.54869\n",
            "Epoch: 9439 Train Loss: 0.52387 Validation Loss: 0.54846\n",
            "Epoch: 9440 Train Loss: 0.52299 Validation Loss: 0.54874\n",
            "Epoch: 9441 Train Loss: 0.52398 Validation Loss: 0.54865\n",
            "Epoch: 9442 Train Loss: 0.52390 Validation Loss: 0.54835\n",
            "Epoch: 9443 Train Loss: 0.52404 Validation Loss: 0.54880\n",
            "Epoch: 9444 Train Loss: 0.52500 Validation Loss: 0.54836\n",
            "Epoch: 9445 Train Loss: 0.52196 Validation Loss: 0.54818\n",
            "Epoch: 9446 Train Loss: 0.52394 Validation Loss: 0.54964\n",
            "Epoch: 9447 Train Loss: 0.52484 Validation Loss: 0.54879\n",
            "Epoch: 9448 Train Loss: 0.52415 Validation Loss: 0.54793\n",
            "Epoch: 9449 Train Loss: 0.52408 Validation Loss: 0.54872\n",
            "Epoch: 9450 Train Loss: 0.52149 Validation Loss: 0.54806\n",
            "Epoch: 9451 Train Loss: 0.52567 Validation Loss: 0.54906\n",
            "Epoch: 9452 Train Loss: 0.52034 Validation Loss: 0.54827\n",
            "Epoch: 9453 Train Loss: 0.52329 Validation Loss: 0.54913\n",
            "Epoch: 9454 Train Loss: 0.52299 Validation Loss: 0.54862\n",
            "Epoch: 9455 Train Loss: 0.52091 Validation Loss: 0.54830\n",
            "Epoch: 9456 Train Loss: 0.52183 Validation Loss: 0.54911\n",
            "Epoch: 9457 Train Loss: 0.52326 Validation Loss: 0.54939\n",
            "Epoch: 9458 Train Loss: 0.52384 Validation Loss: 0.54838\n",
            "Epoch: 9459 Train Loss: 0.52378 Validation Loss: 0.54867\n",
            "Epoch: 9460 Train Loss: 0.52208 Validation Loss: 0.54830\n",
            "Epoch: 9461 Train Loss: 0.52103 Validation Loss: 0.54924\n",
            "Epoch: 9462 Train Loss: 0.51915 Validation Loss: 0.54825\n",
            "Epoch: 9463 Train Loss: 0.52202 Validation Loss: 0.54872\n",
            "Epoch: 9464 Train Loss: 0.52500 Validation Loss: 0.54955\n",
            "Epoch: 9465 Train Loss: 0.52412 Validation Loss: 0.54882\n",
            "Epoch: 9466 Train Loss: 0.52491 Validation Loss: 0.54781\n",
            "Epoch: 9467 Train Loss: 0.52067 Validation Loss: 0.54855\n",
            "Epoch: 9468 Train Loss: 0.52197 Validation Loss: 0.54898\n",
            "Epoch: 9469 Train Loss: 0.52282 Validation Loss: 0.54921\n",
            "Epoch: 9470 Train Loss: 0.52386 Validation Loss: 0.54917\n",
            "Epoch: 9471 Train Loss: 0.52271 Validation Loss: 0.54852\n",
            "Epoch: 9472 Train Loss: 0.52494 Validation Loss: 0.54865\n",
            "Epoch: 9473 Train Loss: 0.52474 Validation Loss: 0.54816\n",
            "Epoch: 9474 Train Loss: 0.52689 Validation Loss: 0.54860\n",
            "Epoch: 9475 Train Loss: 0.52387 Validation Loss: 0.54812\n",
            "Epoch: 9476 Train Loss: 0.52227 Validation Loss: 0.54922\n",
            "Epoch: 9477 Train Loss: 0.52408 Validation Loss: 0.54838\n",
            "Epoch: 9478 Train Loss: 0.52501 Validation Loss: 0.54870\n",
            "Epoch: 9479 Train Loss: 0.52410 Validation Loss: 0.54886\n",
            "Epoch: 9480 Train Loss: 0.52110 Validation Loss: 0.54859\n",
            "Epoch: 9481 Train Loss: 0.52275 Validation Loss: 0.54858\n",
            "Epoch: 9482 Train Loss: 0.52275 Validation Loss: 0.54840\n",
            "Epoch: 9483 Train Loss: 0.52296 Validation Loss: 0.54881\n",
            "Epoch: 9484 Train Loss: 0.52388 Validation Loss: 0.54856\n",
            "Epoch: 9485 Train Loss: 0.52488 Validation Loss: 0.54877\n",
            "Epoch: 9486 Train Loss: 0.52313 Validation Loss: 0.54870\n",
            "Epoch: 9487 Train Loss: 0.52289 Validation Loss: 0.54865\n",
            "Epoch: 9488 Train Loss: 0.52308 Validation Loss: 0.54904\n",
            "Epoch: 9489 Train Loss: 0.52286 Validation Loss: 0.54845\n",
            "Epoch: 9490 Train Loss: 0.52506 Validation Loss: 0.54867\n",
            "Epoch: 9491 Train Loss: 0.52497 Validation Loss: 0.54824\n",
            "Epoch: 9492 Train Loss: 0.51928 Validation Loss: 0.54775\n",
            "Epoch: 9493 Train Loss: 0.52226 Validation Loss: 0.54929\n",
            "Epoch: 9494 Train Loss: 0.52488 Validation Loss: 0.54859\n",
            "Epoch: 9495 Train Loss: 0.52357 Validation Loss: 0.54997\n",
            "Epoch: 9496 Train Loss: 0.52229 Validation Loss: 0.54801\n",
            "Epoch: 9497 Train Loss: 0.52201 Validation Loss: 0.54864\n",
            "Epoch: 9498 Train Loss: 0.52298 Validation Loss: 0.54885\n",
            "Epoch: 9499 Train Loss: 0.52305 Validation Loss: 0.54817\n",
            "Epoch: 9500 Train Loss: 0.52183 Validation Loss: 0.54883\n",
            "Epoch: 9501 Train Loss: 0.52395 Validation Loss: 0.54858\n",
            "Epoch: 9502 Train Loss: 0.52187 Validation Loss: 0.54882\n",
            "Epoch: 9503 Train Loss: 0.52210 Validation Loss: 0.54809\n",
            "Epoch: 9504 Train Loss: 0.52477 Validation Loss: 0.54850\n",
            "Epoch: 9505 Train Loss: 0.52390 Validation Loss: 0.54916\n",
            "Epoch: 9506 Train Loss: 0.52393 Validation Loss: 0.54891\n",
            "Epoch: 9507 Train Loss: 0.52522 Validation Loss: 0.54797\n",
            "Epoch: 9508 Train Loss: 0.51982 Validation Loss: 0.54868\n",
            "Epoch: 9509 Train Loss: 0.52304 Validation Loss: 0.54874\n",
            "Epoch: 9510 Train Loss: 0.52179 Validation Loss: 0.54874\n",
            "Epoch: 9511 Train Loss: 0.52387 Validation Loss: 0.54864\n",
            "Epoch: 9512 Train Loss: 0.52183 Validation Loss: 0.54915\n",
            "Epoch: 9513 Train Loss: 0.52509 Validation Loss: 0.54873\n",
            "Epoch: 9514 Train Loss: 0.52391 Validation Loss: 0.54860\n",
            "Epoch: 9515 Train Loss: 0.52611 Validation Loss: 0.54853\n",
            "Epoch: 9516 Train Loss: 0.52377 Validation Loss: 0.54820\n",
            "Epoch: 9517 Train Loss: 0.52454 Validation Loss: 0.54801\n",
            "Epoch: 9518 Train Loss: 0.52417 Validation Loss: 0.54951\n",
            "Epoch: 9519 Train Loss: 0.52348 Validation Loss: 0.54800\n",
            "Epoch: 9520 Train Loss: 0.52372 Validation Loss: 0.54867\n",
            "Epoch: 9521 Train Loss: 0.52300 Validation Loss: 0.54920\n",
            "Epoch: 9522 Train Loss: 0.52536 Validation Loss: 0.54918\n",
            "Epoch: 9523 Train Loss: 0.52472 Validation Loss: 0.54837\n",
            "Epoch: 9524 Train Loss: 0.52283 Validation Loss: 0.54824\n",
            "Epoch: 9525 Train Loss: 0.52385 Validation Loss: 0.54831\n",
            "Epoch: 9526 Train Loss: 0.52480 Validation Loss: 0.54903\n",
            "Epoch: 9527 Train Loss: 0.52195 Validation Loss: 0.54846\n",
            "Epoch: 9528 Train Loss: 0.52508 Validation Loss: 0.54893\n",
            "Epoch: 9529 Train Loss: 0.52675 Validation Loss: 0.54846\n",
            "Epoch: 9530 Train Loss: 0.52292 Validation Loss: 0.54800\n",
            "Epoch: 9531 Train Loss: 0.52222 Validation Loss: 0.54858\n",
            "Epoch: 9532 Train Loss: 0.52003 Validation Loss: 0.54822\n",
            "Epoch: 9533 Train Loss: 0.51983 Validation Loss: 0.54917\n",
            "Epoch: 9534 Train Loss: 0.52185 Validation Loss: 0.54874\n",
            "Epoch: 9535 Train Loss: 0.52189 Validation Loss: 0.54910\n",
            "Epoch: 9536 Train Loss: 0.52282 Validation Loss: 0.54842\n",
            "Epoch: 9537 Train Loss: 0.51941 Validation Loss: 0.54813\n",
            "Epoch: 9538 Train Loss: 0.52006 Validation Loss: 0.54927\n",
            "Epoch: 9539 Train Loss: 0.52731 Validation Loss: 0.54963\n",
            "Epoch: 9540 Train Loss: 0.52226 Validation Loss: 0.54800\n",
            "Epoch: 9541 Train Loss: 0.52642 Validation Loss: 0.54936\n",
            "Epoch: 9542 Train Loss: 0.52508 Validation Loss: 0.54856\n",
            "Epoch: 9543 Train Loss: 0.52337 Validation Loss: 0.54860\n",
            "Epoch: 9544 Train Loss: 0.52519 Validation Loss: 0.54849\n",
            "Epoch: 9545 Train Loss: 0.52700 Validation Loss: 0.54918\n",
            "Epoch: 9546 Train Loss: 0.52129 Validation Loss: 0.54807\n",
            "Epoch: 9547 Train Loss: 0.52387 Validation Loss: 0.54892\n",
            "Epoch: 9548 Train Loss: 0.52292 Validation Loss: 0.54897\n",
            "Epoch: 9549 Train Loss: 0.52223 Validation Loss: 0.54857\n",
            "Epoch: 9550 Train Loss: 0.52192 Validation Loss: 0.54943\n",
            "Epoch: 9551 Train Loss: 0.52502 Validation Loss: 0.54914\n",
            "Epoch: 9552 Train Loss: 0.52167 Validation Loss: 0.54839\n",
            "Epoch: 9553 Train Loss: 0.52095 Validation Loss: 0.54842\n",
            "Epoch: 9554 Train Loss: 0.51990 Validation Loss: 0.54849\n",
            "Epoch: 9555 Train Loss: 0.52284 Validation Loss: 0.54919\n",
            "Epoch: 9556 Train Loss: 0.52182 Validation Loss: 0.54852\n",
            "Epoch: 9557 Train Loss: 0.52600 Validation Loss: 0.54899\n",
            "Epoch: 9558 Train Loss: 0.52234 Validation Loss: 0.54805\n",
            "Epoch: 9559 Train Loss: 0.52558 Validation Loss: 0.54927\n",
            "Epoch: 9560 Train Loss: 0.52195 Validation Loss: 0.54800\n",
            "Epoch: 9561 Train Loss: 0.52676 Validation Loss: 0.54845\n",
            "Epoch: 9562 Train Loss: 0.52388 Validation Loss: 0.54871\n",
            "Epoch: 9563 Train Loss: 0.52332 Validation Loss: 0.54850\n",
            "Epoch: 9564 Train Loss: 0.52181 Validation Loss: 0.54885\n",
            "Epoch: 9565 Train Loss: 0.52290 Validation Loss: 0.54880\n",
            "Epoch: 9566 Train Loss: 0.52492 Validation Loss: 0.54908\n",
            "Epoch: 9567 Train Loss: 0.52176 Validation Loss: 0.54852\n",
            "Epoch: 9568 Train Loss: 0.52387 Validation Loss: 0.54867\n",
            "Epoch: 9569 Train Loss: 0.52140 Validation Loss: 0.54809\n",
            "Epoch: 9570 Train Loss: 0.52379 Validation Loss: 0.54899\n",
            "Epoch: 9571 Train Loss: 0.52299 Validation Loss: 0.54931\n",
            "Epoch: 9572 Train Loss: 0.52432 Validation Loss: 0.54905\n",
            "Epoch: 9573 Train Loss: 0.52268 Validation Loss: 0.54796\n",
            "Epoch: 9574 Train Loss: 0.52199 Validation Loss: 0.54857\n",
            "Epoch: 9575 Train Loss: 0.52261 Validation Loss: 0.54834\n",
            "Epoch: 9576 Train Loss: 0.52291 Validation Loss: 0.54860\n",
            "Epoch: 9577 Train Loss: 0.52479 Validation Loss: 0.54842\n",
            "Epoch: 9578 Train Loss: 0.52189 Validation Loss: 0.54846\n",
            "Epoch: 9579 Train Loss: 0.52281 Validation Loss: 0.54844\n",
            "Epoch: 9580 Train Loss: 0.52287 Validation Loss: 0.54834\n",
            "Epoch: 9581 Train Loss: 0.52435 Validation Loss: 0.54914\n",
            "Epoch: 9582 Train Loss: 0.52299 Validation Loss: 0.54845\n",
            "Epoch: 9583 Train Loss: 0.52097 Validation Loss: 0.54820\n",
            "Epoch: 9584 Train Loss: 0.52326 Validation Loss: 0.54849\n",
            "Epoch: 9585 Train Loss: 0.52340 Validation Loss: 0.54835\n",
            "Epoch: 9586 Train Loss: 0.52293 Validation Loss: 0.54962\n",
            "Epoch: 9587 Train Loss: 0.52336 Validation Loss: 0.54854\n",
            "Epoch: 9588 Train Loss: 0.52219 Validation Loss: 0.54877\n",
            "Epoch: 9589 Train Loss: 0.52280 Validation Loss: 0.54886\n",
            "Epoch: 9590 Train Loss: 0.52407 Validation Loss: 0.54896\n",
            "Epoch: 9591 Train Loss: 0.52185 Validation Loss: 0.54834\n",
            "Epoch: 9592 Train Loss: 0.52177 Validation Loss: 0.54874\n",
            "Epoch: 9593 Train Loss: 0.52715 Validation Loss: 0.54919\n",
            "Epoch: 9594 Train Loss: 0.52471 Validation Loss: 0.54825\n",
            "Epoch: 9595 Train Loss: 0.52303 Validation Loss: 0.54846\n",
            "Epoch: 9596 Train Loss: 0.52203 Validation Loss: 0.54813\n",
            "Epoch: 9597 Train Loss: 0.52403 Validation Loss: 0.54939\n",
            "Epoch: 9598 Train Loss: 0.52314 Validation Loss: 0.54837\n",
            "Epoch: 9599 Train Loss: 0.52612 Validation Loss: 0.54917\n",
            "Epoch: 9600 Train Loss: 0.52077 Validation Loss: 0.54870\n",
            "Epoch: 9601 Train Loss: 0.52481 Validation Loss: 0.54830\n",
            "Epoch: 9602 Train Loss: 0.52289 Validation Loss: 0.54881\n",
            "Epoch: 9603 Train Loss: 0.52172 Validation Loss: 0.54848\n",
            "Epoch: 9604 Train Loss: 0.52226 Validation Loss: 0.54905\n",
            "Epoch: 9605 Train Loss: 0.52481 Validation Loss: 0.54848\n",
            "Epoch: 9606 Train Loss: 0.52291 Validation Loss: 0.54823\n",
            "Epoch: 9607 Train Loss: 0.52484 Validation Loss: 0.54866\n",
            "Epoch: 9608 Train Loss: 0.52245 Validation Loss: 0.54965\n",
            "Epoch: 9609 Train Loss: 0.52492 Validation Loss: 0.54843\n",
            "Epoch: 9610 Train Loss: 0.52197 Validation Loss: 0.54833\n",
            "Epoch: 9611 Train Loss: 0.52277 Validation Loss: 0.54847\n",
            "Epoch: 9612 Train Loss: 0.52183 Validation Loss: 0.54847\n",
            "Epoch: 9613 Train Loss: 0.52301 Validation Loss: 0.54848\n",
            "Epoch: 9614 Train Loss: 0.52103 Validation Loss: 0.54876\n",
            "Epoch: 9615 Train Loss: 0.52511 Validation Loss: 0.54871\n",
            "Epoch: 9616 Train Loss: 0.52293 Validation Loss: 0.54833\n",
            "Epoch: 9617 Train Loss: 0.52560 Validation Loss: 0.54924\n",
            "Epoch: 9618 Train Loss: 0.52188 Validation Loss: 0.54825\n",
            "Epoch: 9619 Train Loss: 0.52393 Validation Loss: 0.54849\n",
            "Epoch: 9620 Train Loss: 0.52284 Validation Loss: 0.54839\n",
            "Epoch: 9621 Train Loss: 0.52307 Validation Loss: 0.54860\n",
            "Epoch: 9622 Train Loss: 0.52088 Validation Loss: 0.54948\n",
            "Epoch: 9623 Train Loss: 0.52297 Validation Loss: 0.54851\n",
            "Epoch: 9624 Train Loss: 0.52321 Validation Loss: 0.54872\n",
            "Epoch: 9625 Train Loss: 0.52425 Validation Loss: 0.54864\n",
            "Epoch: 9626 Train Loss: 0.52273 Validation Loss: 0.54896\n",
            "Epoch: 9627 Train Loss: 0.52865 Validation Loss: 0.54957\n",
            "Epoch: 9628 Train Loss: 0.52270 Validation Loss: 0.54808\n",
            "Epoch: 9629 Train Loss: 0.52120 Validation Loss: 0.54795\n",
            "Epoch: 9630 Train Loss: 0.52088 Validation Loss: 0.54853\n",
            "Epoch: 9631 Train Loss: 0.52304 Validation Loss: 0.54928\n",
            "Epoch: 9632 Train Loss: 0.52628 Validation Loss: 0.54944\n",
            "Epoch: 9633 Train Loss: 0.52522 Validation Loss: 0.54799\n",
            "Epoch: 9634 Train Loss: 0.52390 Validation Loss: 0.54862\n",
            "Epoch: 9635 Train Loss: 0.52179 Validation Loss: 0.54832\n",
            "Epoch: 9636 Train Loss: 0.52617 Validation Loss: 0.54930\n",
            "Epoch: 9637 Train Loss: 0.52298 Validation Loss: 0.54899\n",
            "Epoch: 9638 Train Loss: 0.52498 Validation Loss: 0.54839\n",
            "Epoch: 9639 Train Loss: 0.52284 Validation Loss: 0.54872\n",
            "Epoch: 9640 Train Loss: 0.52441 Validation Loss: 0.54812\n",
            "Epoch: 9641 Train Loss: 0.52515 Validation Loss: 0.54928\n",
            "Epoch: 9642 Train Loss: 0.52322 Validation Loss: 0.54805\n",
            "Epoch: 9643 Train Loss: 0.52194 Validation Loss: 0.54854\n",
            "Epoch: 9644 Train Loss: 0.52520 Validation Loss: 0.54847\n",
            "Epoch: 9645 Train Loss: 0.52036 Validation Loss: 0.54801\n",
            "Epoch: 9646 Train Loss: 0.52464 Validation Loss: 0.54964\n",
            "Epoch: 9647 Train Loss: 0.52182 Validation Loss: 0.54839\n",
            "Epoch: 9648 Train Loss: 0.52272 Validation Loss: 0.54851\n",
            "Epoch: 9649 Train Loss: 0.52233 Validation Loss: 0.54804\n",
            "Epoch: 9650 Train Loss: 0.52174 Validation Loss: 0.54841\n",
            "Epoch: 9651 Train Loss: 0.52532 Validation Loss: 0.54917\n",
            "Epoch: 9652 Train Loss: 0.52402 Validation Loss: 0.54871\n",
            "Epoch: 9653 Train Loss: 0.52288 Validation Loss: 0.54876\n",
            "Epoch: 9654 Train Loss: 0.52182 Validation Loss: 0.54863\n",
            "Epoch: 9655 Train Loss: 0.52714 Validation Loss: 0.54881\n",
            "Epoch: 9656 Train Loss: 0.52582 Validation Loss: 0.54844\n",
            "Epoch: 9657 Train Loss: 0.52544 Validation Loss: 0.54835\n",
            "Epoch: 9658 Train Loss: 0.52287 Validation Loss: 0.54856\n",
            "Epoch: 9659 Train Loss: 0.52131 Validation Loss: 0.54827\n",
            "Epoch: 9660 Train Loss: 0.52590 Validation Loss: 0.54873\n",
            "Epoch: 9661 Train Loss: 0.52580 Validation Loss: 0.54859\n",
            "Epoch: 9662 Train Loss: 0.52316 Validation Loss: 0.54806\n",
            "Epoch: 9663 Train Loss: 0.52185 Validation Loss: 0.54856\n",
            "Epoch: 9664 Train Loss: 0.52393 Validation Loss: 0.54934\n",
            "Epoch: 9665 Train Loss: 0.52486 Validation Loss: 0.54898\n",
            "Epoch: 9666 Train Loss: 0.52609 Validation Loss: 0.54811\n",
            "Epoch: 9667 Train Loss: 0.52437 Validation Loss: 0.54895\n",
            "Epoch: 9668 Train Loss: 0.52159 Validation Loss: 0.54817\n",
            "Epoch: 9669 Train Loss: 0.52397 Validation Loss: 0.54943\n",
            "Epoch: 9670 Train Loss: 0.52101 Validation Loss: 0.54876\n",
            "Epoch: 9671 Train Loss: 0.52406 Validation Loss: 0.54870\n",
            "Epoch: 9672 Train Loss: 0.52607 Validation Loss: 0.54918\n",
            "Epoch: 9673 Train Loss: 0.52399 Validation Loss: 0.54838\n",
            "Epoch: 9674 Train Loss: 0.52086 Validation Loss: 0.54830\n",
            "Epoch: 9675 Train Loss: 0.52291 Validation Loss: 0.54875\n",
            "Epoch: 9676 Train Loss: 0.52185 Validation Loss: 0.54877\n",
            "Epoch: 9677 Train Loss: 0.52189 Validation Loss: 0.54826\n",
            "Epoch: 9678 Train Loss: 0.52387 Validation Loss: 0.54860\n",
            "Epoch: 9679 Train Loss: 0.52519 Validation Loss: 0.54941\n",
            "Epoch: 9680 Train Loss: 0.52525 Validation Loss: 0.54832\n",
            "Epoch: 9681 Train Loss: 0.52133 Validation Loss: 0.54887\n",
            "Epoch: 9682 Train Loss: 0.52395 Validation Loss: 0.54853\n",
            "Epoch: 9683 Train Loss: 0.52081 Validation Loss: 0.54849\n",
            "Epoch: 9684 Train Loss: 0.52180 Validation Loss: 0.54863\n",
            "Epoch: 9685 Train Loss: 0.52403 Validation Loss: 0.54880\n",
            "Epoch: 9686 Train Loss: 0.52498 Validation Loss: 0.54931\n",
            "Epoch: 9687 Train Loss: 0.52490 Validation Loss: 0.54913\n",
            "Epoch: 9688 Train Loss: 0.52372 Validation Loss: 0.54829\n",
            "Epoch: 9689 Train Loss: 0.52400 Validation Loss: 0.54874\n",
            "Epoch: 9690 Train Loss: 0.52078 Validation Loss: 0.54827\n",
            "Epoch: 9691 Train Loss: 0.52019 Validation Loss: 0.54821\n",
            "Epoch: 9692 Train Loss: 0.52188 Validation Loss: 0.54914\n",
            "Epoch: 9693 Train Loss: 0.52586 Validation Loss: 0.54885\n",
            "Epoch: 9694 Train Loss: 0.52390 Validation Loss: 0.54885\n",
            "Epoch: 9695 Train Loss: 0.52193 Validation Loss: 0.54843\n",
            "Epoch: 9696 Train Loss: 0.52491 Validation Loss: 0.54880\n",
            "Epoch: 9697 Train Loss: 0.52501 Validation Loss: 0.54891\n",
            "Epoch: 9698 Train Loss: 0.52288 Validation Loss: 0.54851\n",
            "Epoch: 9699 Train Loss: 0.52091 Validation Loss: 0.54877\n",
            "Epoch: 9700 Train Loss: 0.52005 Validation Loss: 0.54856\n",
            "Epoch: 9701 Train Loss: 0.52330 Validation Loss: 0.54845\n",
            "Epoch: 9702 Train Loss: 0.52217 Validation Loss: 0.54892\n",
            "Epoch: 9703 Train Loss: 0.52179 Validation Loss: 0.54897\n",
            "Epoch: 9704 Train Loss: 0.52623 Validation Loss: 0.54961\n",
            "Epoch: 9705 Train Loss: 0.52145 Validation Loss: 0.54846\n",
            "Epoch: 9706 Train Loss: 0.52479 Validation Loss: 0.54884\n",
            "Epoch: 9707 Train Loss: 0.52184 Validation Loss: 0.54865\n",
            "Epoch: 9708 Train Loss: 0.52422 Validation Loss: 0.54947\n",
            "Epoch: 9709 Train Loss: 0.52390 Validation Loss: 0.54896\n",
            "Epoch: 9710 Train Loss: 0.52326 Validation Loss: 0.54874\n",
            "Epoch: 9711 Train Loss: 0.52593 Validation Loss: 0.54821\n",
            "Epoch: 9712 Train Loss: 0.52099 Validation Loss: 0.54837\n",
            "Epoch: 9713 Train Loss: 0.52187 Validation Loss: 0.54866\n",
            "Epoch: 9714 Train Loss: 0.52301 Validation Loss: 0.54892\n",
            "Epoch: 9715 Train Loss: 0.52345 Validation Loss: 0.54962\n",
            "Epoch: 9716 Train Loss: 0.52216 Validation Loss: 0.54817\n",
            "Epoch: 9717 Train Loss: 0.51969 Validation Loss: 0.54873\n",
            "Epoch: 9718 Train Loss: 0.52399 Validation Loss: 0.54933\n",
            "Epoch: 9719 Train Loss: 0.52412 Validation Loss: 0.54852\n",
            "Epoch: 9720 Train Loss: 0.52280 Validation Loss: 0.54855\n",
            "Epoch: 9721 Train Loss: 0.52409 Validation Loss: 0.54916\n",
            "Epoch: 9722 Train Loss: 0.52493 Validation Loss: 0.54831\n",
            "Epoch: 9723 Train Loss: 0.52387 Validation Loss: 0.54878\n",
            "Epoch: 9724 Train Loss: 0.52217 Validation Loss: 0.54842\n",
            "Epoch: 9725 Train Loss: 0.52317 Validation Loss: 0.54923\n",
            "Epoch: 9726 Train Loss: 0.52487 Validation Loss: 0.54853\n",
            "Epoch: 9727 Train Loss: 0.52585 Validation Loss: 0.54845\n",
            "Epoch: 9728 Train Loss: 0.52613 Validation Loss: 0.54851\n",
            "Epoch: 9729 Train Loss: 0.52056 Validation Loss: 0.54792\n",
            "Epoch: 9730 Train Loss: 0.52511 Validation Loss: 0.54972\n",
            "Epoch: 9731 Train Loss: 0.52600 Validation Loss: 0.54851\n",
            "Epoch: 9732 Train Loss: 0.52294 Validation Loss: 0.54813\n",
            "Epoch: 9733 Train Loss: 0.52446 Validation Loss: 0.54932\n",
            "Epoch: 9734 Train Loss: 0.52150 Validation Loss: 0.54827\n",
            "Epoch: 9735 Train Loss: 0.51975 Validation Loss: 0.54901\n",
            "Epoch: 9736 Train Loss: 0.52226 Validation Loss: 0.54861\n",
            "Epoch: 9737 Train Loss: 0.52299 Validation Loss: 0.54868\n",
            "Epoch: 9738 Train Loss: 0.52197 Validation Loss: 0.54919\n",
            "Epoch: 9739 Train Loss: 0.52328 Validation Loss: 0.54812\n",
            "Epoch: 9740 Train Loss: 0.52619 Validation Loss: 0.54925\n",
            "Epoch: 9741 Train Loss: 0.52389 Validation Loss: 0.54842\n",
            "Epoch: 9742 Train Loss: 0.52285 Validation Loss: 0.54848\n",
            "Epoch: 9743 Train Loss: 0.52174 Validation Loss: 0.54869\n",
            "Epoch: 9744 Train Loss: 0.52478 Validation Loss: 0.54899\n",
            "Epoch: 9745 Train Loss: 0.52136 Validation Loss: 0.54803\n",
            "Epoch: 9746 Train Loss: 0.52468 Validation Loss: 0.54887\n",
            "Epoch: 9747 Train Loss: 0.52816 Validation Loss: 0.54956\n",
            "Epoch: 9748 Train Loss: 0.52325 Validation Loss: 0.54772\n",
            "Epoch: 9749 Train Loss: 0.52201 Validation Loss: 0.54947\n",
            "Epoch: 9750 Train Loss: 0.52280 Validation Loss: 0.54867\n",
            "Epoch: 9751 Train Loss: 0.52386 Validation Loss: 0.54862\n",
            "Epoch: 9752 Train Loss: 0.52294 Validation Loss: 0.54853\n",
            "Epoch: 9753 Train Loss: 0.52476 Validation Loss: 0.54888\n",
            "Epoch: 9754 Train Loss: 0.52000 Validation Loss: 0.54837\n",
            "Epoch: 9755 Train Loss: 0.52327 Validation Loss: 0.54900\n",
            "Epoch: 9756 Train Loss: 0.52084 Validation Loss: 0.54880\n",
            "Epoch: 9757 Train Loss: 0.52109 Validation Loss: 0.54824\n",
            "Epoch: 9758 Train Loss: 0.52295 Validation Loss: 0.54853\n",
            "Epoch: 9759 Train Loss: 0.52396 Validation Loss: 0.54901\n",
            "Epoch: 9760 Train Loss: 0.52515 Validation Loss: 0.54862\n",
            "Epoch: 9761 Train Loss: 0.52584 Validation Loss: 0.54884\n",
            "Epoch: 9762 Train Loss: 0.52301 Validation Loss: 0.54822\n",
            "Epoch: 9763 Train Loss: 0.52209 Validation Loss: 0.54902\n",
            "Epoch: 9764 Train Loss: 0.52505 Validation Loss: 0.54854\n",
            "Epoch: 9765 Train Loss: 0.52363 Validation Loss: 0.54814\n",
            "Epoch: 9766 Train Loss: 0.52632 Validation Loss: 0.54934\n",
            "Epoch: 9767 Train Loss: 0.52174 Validation Loss: 0.54838\n",
            "Epoch: 9768 Train Loss: 0.52480 Validation Loss: 0.54838\n",
            "Epoch: 9769 Train Loss: 0.52823 Validation Loss: 0.54927\n",
            "Epoch: 9770 Train Loss: 0.52276 Validation Loss: 0.54844\n",
            "Epoch: 9771 Train Loss: 0.52209 Validation Loss: 0.54842\n",
            "Epoch: 9772 Train Loss: 0.52380 Validation Loss: 0.54817\n",
            "Epoch: 9773 Train Loss: 0.52627 Validation Loss: 0.54902\n",
            "Epoch: 9774 Train Loss: 0.52195 Validation Loss: 0.54821\n",
            "Epoch: 9775 Train Loss: 0.52100 Validation Loss: 0.54813\n",
            "Epoch: 9776 Train Loss: 0.51965 Validation Loss: 0.54914\n",
            "Epoch: 9777 Train Loss: 0.52288 Validation Loss: 0.54935\n",
            "Epoch: 9778 Train Loss: 0.52616 Validation Loss: 0.54938\n",
            "Epoch: 9779 Train Loss: 0.52157 Validation Loss: 0.54820\n",
            "Epoch: 9780 Train Loss: 0.52191 Validation Loss: 0.54814\n",
            "Epoch: 9781 Train Loss: 0.52398 Validation Loss: 0.54886\n",
            "Epoch: 9782 Train Loss: 0.52334 Validation Loss: 0.54847\n",
            "Epoch: 9783 Train Loss: 0.52493 Validation Loss: 0.54854\n",
            "Epoch: 9784 Train Loss: 0.52093 Validation Loss: 0.54872\n",
            "Epoch: 9785 Train Loss: 0.52276 Validation Loss: 0.54878\n",
            "Epoch: 9786 Train Loss: 0.52416 Validation Loss: 0.54927\n",
            "Epoch: 9787 Train Loss: 0.52638 Validation Loss: 0.54817\n",
            "Epoch: 9788 Train Loss: 0.52283 Validation Loss: 0.54842\n",
            "Epoch: 9789 Train Loss: 0.52506 Validation Loss: 0.54894\n",
            "Epoch: 9790 Train Loss: 0.52373 Validation Loss: 0.54833\n",
            "Epoch: 9791 Train Loss: 0.52390 Validation Loss: 0.54841\n",
            "Epoch: 9792 Train Loss: 0.52372 Validation Loss: 0.54858\n",
            "Epoch: 9793 Train Loss: 0.52392 Validation Loss: 0.54833\n",
            "Epoch: 9794 Train Loss: 0.52288 Validation Loss: 0.54913\n",
            "Epoch: 9795 Train Loss: 0.52275 Validation Loss: 0.54859\n",
            "Epoch: 9796 Train Loss: 0.52092 Validation Loss: 0.54852\n",
            "Epoch: 9797 Train Loss: 0.52386 Validation Loss: 0.54862\n",
            "Epoch: 9798 Train Loss: 0.52222 Validation Loss: 0.54874\n",
            "Epoch: 9799 Train Loss: 0.52277 Validation Loss: 0.54875\n",
            "Epoch: 9800 Train Loss: 0.52286 Validation Loss: 0.54821\n",
            "Epoch: 9801 Train Loss: 0.52079 Validation Loss: 0.54857\n",
            "Epoch: 9802 Train Loss: 0.52179 Validation Loss: 0.54903\n",
            "Epoch: 9803 Train Loss: 0.52379 Validation Loss: 0.54868\n",
            "Epoch: 9804 Train Loss: 0.52520 Validation Loss: 0.54835\n",
            "Epoch: 9805 Train Loss: 0.52200 Validation Loss: 0.54845\n",
            "Epoch: 9806 Train Loss: 0.52402 Validation Loss: 0.54953\n",
            "Epoch: 9807 Train Loss: 0.52305 Validation Loss: 0.54831\n",
            "Epoch: 9808 Train Loss: 0.52687 Validation Loss: 0.54845\n",
            "Epoch: 9809 Train Loss: 0.52423 Validation Loss: 0.54937\n",
            "Epoch: 9810 Train Loss: 0.52570 Validation Loss: 0.54852\n",
            "Epoch: 9811 Train Loss: 0.52282 Validation Loss: 0.54795\n",
            "Epoch: 9812 Train Loss: 0.52407 Validation Loss: 0.54883\n",
            "Epoch: 9813 Train Loss: 0.52193 Validation Loss: 0.54856\n",
            "Epoch: 9814 Train Loss: 0.52259 Validation Loss: 0.54864\n",
            "Epoch: 9815 Train Loss: 0.52190 Validation Loss: 0.54955\n",
            "Epoch: 9816 Train Loss: 0.52387 Validation Loss: 0.54913\n",
            "Epoch: 9817 Train Loss: 0.52149 Validation Loss: 0.54788\n",
            "Epoch: 9818 Train Loss: 0.52357 Validation Loss: 0.54952\n",
            "Epoch: 9819 Train Loss: 0.52490 Validation Loss: 0.54890\n",
            "Epoch: 9820 Train Loss: 0.52078 Validation Loss: 0.54842\n",
            "Epoch: 9821 Train Loss: 0.52379 Validation Loss: 0.54859\n",
            "Epoch: 9822 Train Loss: 0.52484 Validation Loss: 0.54851\n",
            "Epoch: 9823 Train Loss: 0.52586 Validation Loss: 0.54851\n",
            "Epoch: 9824 Train Loss: 0.52170 Validation Loss: 0.54800\n",
            "Epoch: 9825 Train Loss: 0.52262 Validation Loss: 0.54888\n",
            "Epoch: 9826 Train Loss: 0.52106 Validation Loss: 0.54936\n",
            "Epoch: 9827 Train Loss: 0.52512 Validation Loss: 0.54916\n",
            "Epoch: 9828 Train Loss: 0.52164 Validation Loss: 0.54788\n",
            "Epoch: 9829 Train Loss: 0.52032 Validation Loss: 0.54860\n",
            "Epoch: 9830 Train Loss: 0.52333 Validation Loss: 0.54999\n",
            "Epoch: 9831 Train Loss: 0.52459 Validation Loss: 0.54843\n",
            "Epoch: 9832 Train Loss: 0.52403 Validation Loss: 0.54798\n",
            "Epoch: 9833 Train Loss: 0.52284 Validation Loss: 0.54871\n",
            "Epoch: 9834 Train Loss: 0.52180 Validation Loss: 0.54845\n",
            "Epoch: 9835 Train Loss: 0.52278 Validation Loss: 0.54935\n",
            "Epoch: 9836 Train Loss: 0.52488 Validation Loss: 0.54926\n",
            "Epoch: 9837 Train Loss: 0.52283 Validation Loss: 0.54860\n",
            "Epoch: 9838 Train Loss: 0.52088 Validation Loss: 0.54827\n",
            "Epoch: 9839 Train Loss: 0.52658 Validation Loss: 0.54847\n",
            "Epoch: 9840 Train Loss: 0.52204 Validation Loss: 0.54852\n",
            "Epoch: 9841 Train Loss: 0.52746 Validation Loss: 0.54944\n",
            "Epoch: 9842 Train Loss: 0.52554 Validation Loss: 0.54820\n",
            "Epoch: 9843 Train Loss: 0.52478 Validation Loss: 0.54816\n",
            "Epoch: 9844 Train Loss: 0.52220 Validation Loss: 0.54818\n",
            "Epoch: 9845 Train Loss: 0.52372 Validation Loss: 0.54881\n",
            "Epoch: 9846 Train Loss: 0.52382 Validation Loss: 0.54892\n",
            "Epoch: 9847 Train Loss: 0.52141 Validation Loss: 0.54812\n",
            "Epoch: 9848 Train Loss: 0.52102 Validation Loss: 0.54900\n",
            "Epoch: 9849 Train Loss: 0.52306 Validation Loss: 0.54854\n",
            "Epoch: 9850 Train Loss: 0.52042 Validation Loss: 0.54863\n",
            "Epoch: 9851 Train Loss: 0.52293 Validation Loss: 0.54872\n",
            "Epoch: 9852 Train Loss: 0.52301 Validation Loss: 0.54884\n",
            "Epoch: 9853 Train Loss: 0.52617 Validation Loss: 0.54898\n",
            "Epoch: 9854 Train Loss: 0.52182 Validation Loss: 0.54825\n",
            "Epoch: 9855 Train Loss: 0.52277 Validation Loss: 0.54859\n",
            "Epoch: 9856 Train Loss: 0.52587 Validation Loss: 0.54876\n",
            "Epoch: 9857 Train Loss: 0.52797 Validation Loss: 0.54890\n",
            "Epoch: 9858 Train Loss: 0.52106 Validation Loss: 0.54794\n",
            "Epoch: 9859 Train Loss: 0.52216 Validation Loss: 0.54886\n",
            "Epoch: 9860 Train Loss: 0.52324 Validation Loss: 0.54814\n",
            "Epoch: 9861 Train Loss: 0.52319 Validation Loss: 0.54949\n",
            "Epoch: 9862 Train Loss: 0.52342 Validation Loss: 0.54827\n",
            "Epoch: 9863 Train Loss: 0.51962 Validation Loss: 0.54844\n",
            "Epoch: 9864 Train Loss: 0.52208 Validation Loss: 0.54853\n",
            "Epoch: 9865 Train Loss: 0.52294 Validation Loss: 0.54926\n",
            "Epoch: 9866 Train Loss: 0.52078 Validation Loss: 0.54838\n",
            "Epoch: 9867 Train Loss: 0.52091 Validation Loss: 0.54909\n",
            "Epoch: 9868 Train Loss: 0.52657 Validation Loss: 0.54916\n",
            "Epoch: 9869 Train Loss: 0.52449 Validation Loss: 0.54804\n",
            "Epoch: 9870 Train Loss: 0.52125 Validation Loss: 0.54830\n",
            "Epoch: 9871 Train Loss: 0.52098 Validation Loss: 0.54831\n",
            "Epoch: 9872 Train Loss: 0.52235 Validation Loss: 0.54897\n",
            "Epoch: 9873 Train Loss: 0.52397 Validation Loss: 0.54871\n",
            "Epoch: 9874 Train Loss: 0.52078 Validation Loss: 0.54886\n",
            "Epoch: 9875 Train Loss: 0.52577 Validation Loss: 0.54871\n",
            "Epoch: 9876 Train Loss: 0.52314 Validation Loss: 0.54876\n",
            "Epoch: 9877 Train Loss: 0.52591 Validation Loss: 0.54832\n",
            "Epoch: 9878 Train Loss: 0.52281 Validation Loss: 0.54819\n",
            "Epoch: 9879 Train Loss: 0.52380 Validation Loss: 0.54867\n",
            "Epoch: 9880 Train Loss: 0.52090 Validation Loss: 0.54869\n",
            "Epoch: 9881 Train Loss: 0.52201 Validation Loss: 0.54823\n",
            "Epoch: 9882 Train Loss: 0.52328 Validation Loss: 0.54849\n",
            "Epoch: 9883 Train Loss: 0.52626 Validation Loss: 0.54888\n",
            "Epoch: 9884 Train Loss: 0.52433 Validation Loss: 0.54934\n",
            "Epoch: 9885 Train Loss: 0.52103 Validation Loss: 0.54813\n",
            "Epoch: 9886 Train Loss: 0.52460 Validation Loss: 0.54808\n",
            "Epoch: 9887 Train Loss: 0.52270 Validation Loss: 0.54914\n",
            "Epoch: 9888 Train Loss: 0.52181 Validation Loss: 0.54935\n",
            "Epoch: 9889 Train Loss: 0.52106 Validation Loss: 0.54852\n",
            "Epoch: 9890 Train Loss: 0.52183 Validation Loss: 0.54895\n",
            "Epoch: 9891 Train Loss: 0.52086 Validation Loss: 0.54867\n",
            "Epoch: 9892 Train Loss: 0.52195 Validation Loss: 0.54866\n",
            "Epoch: 9893 Train Loss: 0.52585 Validation Loss: 0.54878\n",
            "Epoch: 9894 Train Loss: 0.52615 Validation Loss: 0.54923\n",
            "Epoch: 9895 Train Loss: 0.52069 Validation Loss: 0.54814\n",
            "Epoch: 9896 Train Loss: 0.52010 Validation Loss: 0.54820\n",
            "Epoch: 9897 Train Loss: 0.52296 Validation Loss: 0.54885\n",
            "Epoch: 9898 Train Loss: 0.52217 Validation Loss: 0.54848\n",
            "Epoch: 9899 Train Loss: 0.52270 Validation Loss: 0.54909\n",
            "Epoch: 9900 Train Loss: 0.52594 Validation Loss: 0.54890\n",
            "Epoch: 9901 Train Loss: 0.52378 Validation Loss: 0.54810\n",
            "Epoch: 9902 Train Loss: 0.52204 Validation Loss: 0.54865\n",
            "Epoch: 9903 Train Loss: 0.52599 Validation Loss: 0.54920\n",
            "Epoch: 9904 Train Loss: 0.52300 Validation Loss: 0.54875\n",
            "Epoch: 9905 Train Loss: 0.52326 Validation Loss: 0.54808\n",
            "Epoch: 9906 Train Loss: 0.52179 Validation Loss: 0.54897\n",
            "Epoch: 9907 Train Loss: 0.52486 Validation Loss: 0.54917\n",
            "Epoch: 9908 Train Loss: 0.52292 Validation Loss: 0.54846\n",
            "Epoch: 9909 Train Loss: 0.52511 Validation Loss: 0.54887\n",
            "Epoch: 9910 Train Loss: 0.52509 Validation Loss: 0.54870\n",
            "Epoch: 9911 Train Loss: 0.52482 Validation Loss: 0.54861\n",
            "Epoch: 9912 Train Loss: 0.52297 Validation Loss: 0.54811\n",
            "Epoch: 9913 Train Loss: 0.52012 Validation Loss: 0.54841\n",
            "Epoch: 9914 Train Loss: 0.52279 Validation Loss: 0.54908\n",
            "Epoch: 9915 Train Loss: 0.52181 Validation Loss: 0.54897\n",
            "Epoch: 9916 Train Loss: 0.52290 Validation Loss: 0.54899\n",
            "Epoch: 9917 Train Loss: 0.52524 Validation Loss: 0.54805\n",
            "Epoch: 9918 Train Loss: 0.52198 Validation Loss: 0.54888\n",
            "Epoch: 9919 Train Loss: 0.52645 Validation Loss: 0.54941\n",
            "Epoch: 9920 Train Loss: 0.52274 Validation Loss: 0.54812\n",
            "Epoch: 9921 Train Loss: 0.52320 Validation Loss: 0.54843\n",
            "Epoch: 9922 Train Loss: 0.52003 Validation Loss: 0.54882\n",
            "Epoch: 9923 Train Loss: 0.52231 Validation Loss: 0.54824\n",
            "Epoch: 9924 Train Loss: 0.52583 Validation Loss: 0.54870\n",
            "Epoch: 9925 Train Loss: 0.52390 Validation Loss: 0.54873\n",
            "Epoch: 9926 Train Loss: 0.52271 Validation Loss: 0.54802\n",
            "Epoch: 9927 Train Loss: 0.52292 Validation Loss: 0.54961\n",
            "Epoch: 9928 Train Loss: 0.52780 Validation Loss: 0.54996\n",
            "Epoch: 9929 Train Loss: 0.52070 Validation Loss: 0.54776\n",
            "Epoch: 9930 Train Loss: 0.52174 Validation Loss: 0.54835\n",
            "Epoch: 9931 Train Loss: 0.52311 Validation Loss: 0.54960\n",
            "Epoch: 9932 Train Loss: 0.52394 Validation Loss: 0.54880\n",
            "Epoch: 9933 Train Loss: 0.52491 Validation Loss: 0.54846\n",
            "Epoch: 9934 Train Loss: 0.52473 Validation Loss: 0.54833\n",
            "Epoch: 9935 Train Loss: 0.52087 Validation Loss: 0.54863\n",
            "Epoch: 9936 Train Loss: 0.52289 Validation Loss: 0.54873\n",
            "Epoch: 9937 Train Loss: 0.52612 Validation Loss: 0.54906\n",
            "Epoch: 9938 Train Loss: 0.52175 Validation Loss: 0.54817\n",
            "Epoch: 9939 Train Loss: 0.52705 Validation Loss: 0.54868\n",
            "Epoch: 9940 Train Loss: 0.52325 Validation Loss: 0.54803\n",
            "Epoch: 9941 Train Loss: 0.52509 Validation Loss: 0.54948\n",
            "Epoch: 9942 Train Loss: 0.52290 Validation Loss: 0.54784\n",
            "Epoch: 9943 Train Loss: 0.52075 Validation Loss: 0.54840\n",
            "Epoch: 9944 Train Loss: 0.52314 Validation Loss: 0.54967\n",
            "Epoch: 9945 Train Loss: 0.52384 Validation Loss: 0.54870\n",
            "Epoch: 9946 Train Loss: 0.52300 Validation Loss: 0.54867\n",
            "Epoch: 9947 Train Loss: 0.52388 Validation Loss: 0.54815\n",
            "Epoch: 9948 Train Loss: 0.52385 Validation Loss: 0.54870\n",
            "Epoch: 9949 Train Loss: 0.52286 Validation Loss: 0.54850\n",
            "Epoch: 9950 Train Loss: 0.52274 Validation Loss: 0.54867\n",
            "Epoch: 9951 Train Loss: 0.52521 Validation Loss: 0.54939\n",
            "Epoch: 9952 Train Loss: 0.52277 Validation Loss: 0.54866\n",
            "Epoch: 9953 Train Loss: 0.51965 Validation Loss: 0.54810\n",
            "Epoch: 9954 Train Loss: 0.52571 Validation Loss: 0.54945\n",
            "Epoch: 9955 Train Loss: 0.52300 Validation Loss: 0.54837\n",
            "Epoch: 9956 Train Loss: 0.52705 Validation Loss: 0.54980\n",
            "Epoch: 9957 Train Loss: 0.52296 Validation Loss: 0.54890\n",
            "Epoch: 9958 Train Loss: 0.52275 Validation Loss: 0.54852\n",
            "Epoch: 9959 Train Loss: 0.52101 Validation Loss: 0.54808\n",
            "Epoch: 9960 Train Loss: 0.52167 Validation Loss: 0.54890\n",
            "Epoch: 9961 Train Loss: 0.52370 Validation Loss: 0.54980\n",
            "Epoch: 9962 Train Loss: 0.52316 Validation Loss: 0.54806\n",
            "Epoch: 9963 Train Loss: 0.52196 Validation Loss: 0.54871\n",
            "Epoch: 9964 Train Loss: 0.52391 Validation Loss: 0.54862\n",
            "Epoch: 9965 Train Loss: 0.51973 Validation Loss: 0.54906\n",
            "Epoch: 9966 Train Loss: 0.52475 Validation Loss: 0.54890\n",
            "Epoch: 9967 Train Loss: 0.52491 Validation Loss: 0.54895\n",
            "Epoch: 9968 Train Loss: 0.52385 Validation Loss: 0.54826\n",
            "Epoch: 9969 Train Loss: 0.52529 Validation Loss: 0.54854\n",
            "Epoch: 9970 Train Loss: 0.52486 Validation Loss: 0.54880\n",
            "Epoch: 9971 Train Loss: 0.52092 Validation Loss: 0.54828\n",
            "Epoch: 9972 Train Loss: 0.52383 Validation Loss: 0.54862\n",
            "Epoch: 9973 Train Loss: 0.52238 Validation Loss: 0.54868\n",
            "Epoch: 9974 Train Loss: 0.52281 Validation Loss: 0.54929\n",
            "Epoch: 9975 Train Loss: 0.52527 Validation Loss: 0.54930\n",
            "Epoch: 9976 Train Loss: 0.52221 Validation Loss: 0.54782\n",
            "Epoch: 9977 Train Loss: 0.52080 Validation Loss: 0.54858\n",
            "Epoch: 9978 Train Loss: 0.52272 Validation Loss: 0.54908\n",
            "Epoch: 9979 Train Loss: 0.52411 Validation Loss: 0.54830\n",
            "Epoch: 9980 Train Loss: 0.52314 Validation Loss: 0.54848\n",
            "Epoch: 9981 Train Loss: 0.52481 Validation Loss: 0.54881\n",
            "Epoch: 9982 Train Loss: 0.52520 Validation Loss: 0.54828\n",
            "Epoch: 9983 Train Loss: 0.52318 Validation Loss: 0.54834\n",
            "Epoch: 9984 Train Loss: 0.52300 Validation Loss: 0.54921\n",
            "Epoch: 9985 Train Loss: 0.52010 Validation Loss: 0.54853\n",
            "Epoch: 9986 Train Loss: 0.52396 Validation Loss: 0.54892\n",
            "Epoch: 9987 Train Loss: 0.52407 Validation Loss: 0.54827\n",
            "Epoch: 9988 Train Loss: 0.52185 Validation Loss: 0.54854\n",
            "Epoch: 9989 Train Loss: 0.52788 Validation Loss: 0.54987\n",
            "Epoch: 9990 Train Loss: 0.52309 Validation Loss: 0.54795\n",
            "Epoch: 9991 Train Loss: 0.52598 Validation Loss: 0.54868\n",
            "Epoch: 9992 Train Loss: 0.51982 Validation Loss: 0.54828\n",
            "Epoch: 9993 Train Loss: 0.52002 Validation Loss: 0.54828\n",
            "Epoch: 9994 Train Loss: 0.52502 Validation Loss: 0.55026\n",
            "Epoch: 9995 Train Loss: 0.52232 Validation Loss: 0.54845\n",
            "Epoch: 9996 Train Loss: 0.52277 Validation Loss: 0.54874\n",
            "Epoch: 9997 Train Loss: 0.52123 Validation Loss: 0.54813\n",
            "Epoch: 9998 Train Loss: 0.52532 Validation Loss: 0.55001\n",
            "Epoch: 9999 Train Loss: 0.52153 Validation Loss: 0.54808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVB7NZJMsjY3"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/DataCamp/model10000.pth')\r\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tv4bALkohEpB",
        "outputId": "b41bffc1-c398-4b37-ede7-c85cc4bda9fc"
      },
      "source": [
        "global_steps_list=[i for i in range(0,epochs)]\r\n",
        "train_loss_list, valid_loss_list=fit.train_losss, fit.val_losss\r\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\r\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\r\n",
        "plt.xlabel('Global Steps')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show() "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW_BFRls1BCZ"
      },
      "source": [
        "# Accuracy Score. \r\n",
        "def accuracy(out, yb):\r\n",
        "    preds = torch.argmax(out, dim=1) # classe prédite: tensor de taille bs. \r\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDenoQAqjc16",
        "outputId": "9e4343bc-03a4-435b-811d-654e74dcc473"
      },
      "source": [
        "def validation_accuracy(model, dl):\r\n",
        "  acc = [accuracy(model(xb.to(device)), yb.to(device)) for xb, yb in dl]\r\n",
        "  acc = sum(acc) / len(acc)\r\n",
        "  print('Accuracy on validation dataset', acc)\r\n",
        "validation_accuracy(model, valid_dl)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on validation dataset tensor(0.7631, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKqQhdlVk2mU"
      },
      "source": [
        "x_test_h5=h5py.File('/content/drive/My Drive/DataCamp/X_test_new.h5', 'r')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byu_oaS8mEY8"
      },
      "source": [
        "x_test=x_test_h5['features']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr2yGrGozdlo"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa-RfFmhl_f-"
      },
      "source": [
        "import torch\r\n",
        "x_test=torch.tensor(x_test).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaGC2xTVxr5k"
      },
      "source": [
        "model = deer()\r\n",
        "model.load_state_dict(torch.load( '/content/drive/My Drive/DataCamp/model10000.pth'))\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4cj9JJ7l6RR"
      },
      "source": [
        "x_test = x_test.view(946,40*7,250,2)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4KNo97Hrlp8",
        "outputId": "65d4a159-3d32-49fa-8d59-ce9eacc0ef05"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([946, 280, 250, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1-5eCSbkw-4"
      },
      "source": [
        "pred=model(x_test)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPR0cbgSGM0m",
        "outputId": "d2e0aabe-b154-4c91-b18b-8661de4121ae"
      },
      "source": [
        "\r\n",
        "#test_ds = TensorDataset(x_test)\r\n",
        "test_dl = DataLoader(x_test, batch_size=bs, shuffle=True) # we usually shuffle the training data.\r\n",
        "\r\n",
        "print(\"number of testing samples\", len(test_dl))\r\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of testing samples 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQcgnrHIGDeb"
      },
      "source": [
        "predictions = []\r\n",
        "for batch in test_dl:\r\n",
        "    batch=batch.to(device)      \r\n",
        "    p= model(batch)\r\n",
        "    predictions.append(torch.argmax(p, dim=1).tolist())"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIacMkQDVFri",
        "outputId": "c12a8fca-9d37-4acf-ac3c-d9c2d57634a5"
      },
      "source": [
        "len(predictions[-2]),type(p),len(p),p.shape"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, torch.Tensor, 18, torch.Size([18, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFDiuXqWt-_"
      },
      "source": [
        "prediction=torch.tensor(predictions[:-1]).to(device)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIfxm6Ew6G2D"
      },
      "source": [
        "prediction=prediction.view(32*29)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdSjMM36UTG"
      },
      "source": [
        "prediction_f=prediction.tolist()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVCL5TkB6a42"
      },
      "source": [
        "prediction_f+=predictions[-1]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7REknuM6iFt",
        "outputId": "36360c91-6645-42b5-acaf-98af5541e2ed"
      },
      "source": [
        "len(prediction_f)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxV8PPNC6lei",
        "outputId": "309ce09e-cdd4-452f-cd20-be6e019860b0"
      },
      "source": [
        "prediction_f.index(1)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "538"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnwzbzz671m"
      },
      "source": [
        "d= {'label': prediction_f}\r\n",
        "df_t=pd.DataFrame(d)\r\n",
        "df_t = df_t.reset_index()\r\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "mM3gNzD-7F3N",
        "outputId": "5897ae08-23f5-4daa-8b5a-eb6eccdaff1f"
      },
      "source": [
        "df_t.head()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  label\n",
              "0      0      0\n",
              "1      1      0\n",
              "2      2      0\n",
              "3      3      0\n",
              "4      4      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87JIPLNH8mqR"
      },
      "source": [
        "df_t.to_csv('/content/drive/My Drive/DataCamp/df_t1.csv', index = False)\r\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA8sjzX9zdMf"
      },
      "source": [
        "### **TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw88i4i5wdKC"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense\r\n",
        "from tensorflow.python.keras.optimizers import Adam\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpkc_kB7weB5"
      },
      "source": [
        "yt_tf=tf.convert_to_tensor(yt.cpu())\r\n",
        "dd_tf=tf.convert_to_tensor(dd.cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0xOYjasylyu",
        "outputId": "5abf07a0-b254-4716-9c54-5c1b0d88e519"
      },
      "source": [
        "dd_tf=tf.reshape(dd_tf,(37840,250,2,7))\r\n",
        "dd_tf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([37840, 250, 2, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "87M_1qt0uAqF",
        "outputId": "7e916cef-9aa2-4396-e8ff-5e6a6813e9f7"
      },
      "source": [
        "input_shape=dd_tf.shape\r\n",
        "num_classes=2\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=input_shape))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(37840, activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "#Compile\r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "#Train and Test The Model\r\n",
        "model.fit(dd_tf, yt_tf, batch_size=4, epochs=10, verbose=1, validation_data=(dd_tf, yt_tf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-790b8cba0a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m37840\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2598\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1651\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     return op(\n\u001b[0;32m-> 1044\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       result = gen_random_ops.random_uniform(\n\u001b[0;32m--> 302\u001b[0;31m           shape, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[1;32m    303\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mminval_is_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaxval_is_one\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[301509120,37840] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeicHzUnZoRj"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc30LtwhZ3EN"
      },
      "source": [
        "x_train=h5py.File('/content/drive/My Drive/DataCamp/X_train_new.h5', 'r')\r\n",
        "y_train_src=pd.read_csv('/content/drive/My Drive/DataCamp/y_train_AvCsavx.csv',index_col=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP3qku5DcJOy"
      },
      "source": [
        "y_train=y_train_src['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufJjKDc2ZyII"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\r\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(128, activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "#Compile\r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adam(), metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "#Train and Test The Model\r\n",
        "model.fit(x_train, y_train, batch_size=4, epochs=10, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}